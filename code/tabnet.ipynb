{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fXGv-XrjbZaq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXGv-XrjbZaq",
        "outputId": "8b7f4690-7256-43b9-a468-905296b479f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch_tabnet\n",
            "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pytorch_tabnet) (1.26.4)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pytorch_tabnet) (1.6.1)\n",
            "Requirement already satisfied: scipy>1.4 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pytorch_tabnet) (1.13.1)\n",
            "Collecting torch>=1.3 (from pytorch_tabnet)\n",
            "  Downloading torch-2.8.0-cp39-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: tqdm>=4.36 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pytorch_tabnet) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from scikit_learn>0.21->pytorch_tabnet) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from scikit_learn>0.21->pytorch_tabnet) (3.6.0)\n",
            "Collecting filelock (from torch>=1.3->pytorch_tabnet)\n",
            "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from torch>=1.3->pytorch_tabnet) (4.14.1)\n",
            "Collecting sympy>=1.13.3 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from torch>=1.3->pytorch_tabnet) (3.2.1)\n",
            "Collecting jinja2 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch>=1.3->pytorch_tabnet)\n",
            "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.3->pytorch_tabnet)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.3->pytorch_tabnet)\n",
            "  Downloading MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
            "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
            "Downloading torch-2.8.0-cp39-none-macosx_11_0_arm64.whl (73.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
            "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
            "Installing collected packages: mpmath, sympy, MarkupSafe, fsspec, filelock, jinja2, torch, pytorch_tabnet\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [pytorch_tabnet]m [torch]\n",
            "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 filelock-3.19.1 fsspec-2025.7.0 jinja2-3.1.6 mpmath-1.3.0 pytorch_tabnet-4.1.0 sympy-1.14.0 torch-2.8.0\n",
            "Collecting tableone\n",
            "  Downloading tableone-0.9.5-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: jinja2>=3.1.4 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from tableone) (3.1.6)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from tableone) (1.26.4)\n",
            "Requirement already satisfied: openpyxl>=3.1.2 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from tableone) (3.1.5)\n",
            "Requirement already satisfied: pandas>=2.0.3 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from tableone) (2.3.1)\n",
            "Requirement already satisfied: scipy>=1.10.1 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from tableone) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.14.1 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from tableone) (0.14.5)\n",
            "Collecting tabulate>=0.9.0 (from tableone)\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from jinja2>=3.1.4->tableone) (3.0.2)\n",
            "Requirement already satisfied: et-xmlfile in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from openpyxl>=3.1.2->tableone) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pandas>=2.0.3->tableone) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pandas>=2.0.3->tableone) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pandas>=2.0.3->tableone) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->tableone) (1.17.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from statsmodels>=0.14.1->tableone) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from statsmodels>=0.14.1->tableone) (25.0)\n",
            "Downloading tableone-0.9.5-py3-none-any.whl (43 kB)\n",
            "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: tabulate, tableone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [tableone]\n",
            "\u001b[1A\u001b[2KSuccessfully installed tableone-0.9.5 tabulate-0.9.0\n",
            "Requirement already satisfied: catboost in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from catboost) (3.9.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from catboost) (2.3.1)\n",
            "Requirement already satisfied: scipy in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from catboost) (6.3.0)\n",
            "Requirement already satisfied: six in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from matplotlib->catboost) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from matplotlib->catboost) (3.2.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from matplotlib->catboost) (6.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->catboost) (3.21.0)\n",
            "Requirement already satisfied: narwhals>=1.15.1 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from plotly->catboost) (2.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_tabnet\n",
        "!pip install tableone\n",
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c4f3f67",
      "metadata": {
        "id": "1c4f3f67"
      },
      "source": [
        "## DataLoad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "2ad1b1cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "2ad1b1cb",
        "outputId": "f571a779-9ab6-4998-f1c2-7b6cf45f7c88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3103 entries, 0 to 3102\n",
            "Data columns (total 28 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   날짜             3103 non-null   object \n",
            " 1   요일             3103 non-null   object \n",
            " 2   공휴일            3103 non-null   int64  \n",
            " 3   목욕장업           3103 non-null   int64  \n",
            " 4   세탁업            3103 non-null   int64  \n",
            " 5   수영장업           3103 non-null   int64  \n",
            " 6   종합체육시설업        3103 non-null   int64  \n",
            " 7   체력단련장업         3103 non-null   int64  \n",
            " 8   하천             3103 non-null   float64\n",
            " 9   생활인구           3103 non-null   float64\n",
            " 10  불쾌지수(DI)       3103 non-null   float64\n",
            " 11  불쾌지수등급         3103 non-null   object \n",
            " 12  일_일강수량(mm)     3103 non-null   float64\n",
            " 13  일_최저기온(°C)     3103 non-null   float64\n",
            " 14  일_평균기온(°C)     3103 non-null   float64\n",
            " 15  일_최고기온(°C)     3103 non-null   float64\n",
            " 16  일_평균풍속(m/s)    3103 non-null   float64\n",
            " 17  일_최대순간풍속(m/s)  3103 non-null   float64\n",
            " 18  최저습도(%)        3103 non-null   float64\n",
            " 19  평균습도(%)        3103 non-null   float64\n",
            " 20  최고습도(%)        3103 non-null   float64\n",
            " 21  습도표준편차         3103 non-null   float64\n",
            " 22  1처리장           3103 non-null   float64\n",
            " 23  2처리장           3103 non-null   float64\n",
            " 24  정화조            3103 non-null   float64\n",
            " 25  중계펌프장          3103 non-null   int64  \n",
            " 26  합계             3103 non-null   float64\n",
            " 27  계절             3103 non-null   object \n",
            "dtypes: float64(17), int64(7), object(4)\n",
            "memory usage: 678.9+ KB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "nanji = pd.read_csv('../data/processed/center_season/nanji/난지_merged.csv', encoding='utf-8-sig')\n",
        "\n",
        "nanji.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "3bda3faa",
      "metadata": {},
      "outputs": [],
      "source": [
        "nanji['날짜'] = pd.to_datetime(nanji['날짜'])\n",
        "nanji = nanji.sort_values('날짜').reset_index(drop=True)\n",
        "# 난지 6월 데이터 없음 -> 다 0으로 되어 있으니까 제거 \n",
        "nanji = nanji[nanji[\"날짜\"] < \"2025-06-01\"] \n",
        "\n",
        "nanji['월'] = nanji['날짜'].dt.month #월\n",
        "\n",
        "def add_cyclical_features(df, date_col='날짜'):\n",
        "    d = df[date_col].dt\n",
        "\n",
        "    # 요일\n",
        "    dow = d.weekday\n",
        "    df['요일_sin'] = np.sin(2*np.pi*dow/7)\n",
        "    df['요일_cos'] = np.cos(2*np.pi*dow/7)\n",
        "\n",
        "    # 연중일\n",
        "    doy = d.dayofyear\n",
        "    df['연중일_sin'] = np.sin(2*np.pi*doy/365.25)\n",
        "    df['연중일_cos'] = np.cos(2*np.pi*doy/365.25)\n",
        "    return df\n",
        "nanji = add_cyclical_features(nanji)\n",
        "\n",
        "nanji['요일_숫자'] = nanji['날짜'].dt.weekday # 0:월요일, 1:화요일, ... , 6:일요일 -> 요일 숫자로\n",
        "\n",
        "# 계절, 불쾌지수등급 숫자로\n",
        "season_map = {'봄': 0, '여름': 1, '가을': 2, '겨울': 3}\n",
        "discomfort_map = {'쾌적': 0, '약간 불쾌': 1, '불쾌': 2, '매우 불쾌': 3, '극심한 불쾌': 4}\n",
        "nanji['계절'] = nanji['계절'].map(season_map).astype('Int64')\n",
        "nanji['불쾌지수등급'] = nanji['불쾌지수등급'].map(discomfort_map).astype('Int64')\n",
        "\n",
        "# 타겟\n",
        "nanji['합계_1일후'] = nanji['합계'].shift(-1)\n",
        "nanji['합계_2일후'] = nanji['합계'].shift(-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "6ecc0de3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3073 entries, 0 to 3072\n",
            "Data columns (total 36 columns):\n",
            " #   Column         Non-Null Count  Dtype         \n",
            "---  ------         --------------  -----         \n",
            " 0   날짜             3073 non-null   datetime64[ns]\n",
            " 1   요일             3073 non-null   object        \n",
            " 2   공휴일            3073 non-null   int64         \n",
            " 3   목욕장업           3073 non-null   int64         \n",
            " 4   세탁업            3073 non-null   int64         \n",
            " 5   수영장업           3073 non-null   int64         \n",
            " 6   종합체육시설업        3073 non-null   int64         \n",
            " 7   체력단련장업         3073 non-null   int64         \n",
            " 8   하천             3073 non-null   float64       \n",
            " 9   생활인구           3073 non-null   float64       \n",
            " 10  불쾌지수(DI)       3073 non-null   float64       \n",
            " 11  불쾌지수등급         3073 non-null   Int64         \n",
            " 12  일_일강수량(mm)     3073 non-null   float64       \n",
            " 13  일_최저기온(°C)     3073 non-null   float64       \n",
            " 14  일_평균기온(°C)     3073 non-null   float64       \n",
            " 15  일_최고기온(°C)     3073 non-null   float64       \n",
            " 16  일_평균풍속(m/s)    3073 non-null   float64       \n",
            " 17  일_최대순간풍속(m/s)  3073 non-null   float64       \n",
            " 18  최저습도(%)        3073 non-null   float64       \n",
            " 19  평균습도(%)        3073 non-null   float64       \n",
            " 20  최고습도(%)        3073 non-null   float64       \n",
            " 21  습도표준편차         3073 non-null   float64       \n",
            " 22  1처리장           3073 non-null   float64       \n",
            " 23  2처리장           3073 non-null   float64       \n",
            " 24  정화조            3073 non-null   float64       \n",
            " 25  중계펌프장          3073 non-null   int64         \n",
            " 26  합계             3073 non-null   float64       \n",
            " 27  계절             3073 non-null   Int64         \n",
            " 28  월              3073 non-null   int32         \n",
            " 29  요일_sin         3073 non-null   float64       \n",
            " 30  요일_cos         3073 non-null   float64       \n",
            " 31  연중일_sin        3073 non-null   float64       \n",
            " 32  연중일_cos        3073 non-null   float64       \n",
            " 33  요일_숫자          3073 non-null   int32         \n",
            " 34  합계_1일후         3072 non-null   float64       \n",
            " 35  합계_2일후         3071 non-null   float64       \n",
            "dtypes: Int64(2), datetime64[ns](1), float64(23), int32(2), int64(7), object(1)\n",
            "memory usage: 870.3+ KB\n"
          ]
        }
      ],
      "source": [
        "nanji.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "3a0a4217",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3071, 30)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nanji = nanji.drop(['날짜', '요일','1처리장', '2처리장', '정화조', '중계펌프장'], axis=1)\n",
        "nanji = nanji.dropna()\n",
        "nanji.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "20dff761",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3071 entries, 0 to 3070\n",
            "Data columns (total 30 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   공휴일            3071 non-null   int64  \n",
            " 1   목욕장업           3071 non-null   int64  \n",
            " 2   세탁업            3071 non-null   int64  \n",
            " 3   수영장업           3071 non-null   int64  \n",
            " 4   종합체육시설업        3071 non-null   int64  \n",
            " 5   체력단련장업         3071 non-null   int64  \n",
            " 6   하천             3071 non-null   float64\n",
            " 7   생활인구           3071 non-null   float64\n",
            " 8   불쾌지수(DI)       3071 non-null   float64\n",
            " 9   불쾌지수등급         3071 non-null   Int64  \n",
            " 10  일_일강수량(mm)     3071 non-null   float64\n",
            " 11  일_최저기온(°C)     3071 non-null   float64\n",
            " 12  일_평균기온(°C)     3071 non-null   float64\n",
            " 13  일_최고기온(°C)     3071 non-null   float64\n",
            " 14  일_평균풍속(m/s)    3071 non-null   float64\n",
            " 15  일_최대순간풍속(m/s)  3071 non-null   float64\n",
            " 16  최저습도(%)        3071 non-null   float64\n",
            " 17  평균습도(%)        3071 non-null   float64\n",
            " 18  최고습도(%)        3071 non-null   float64\n",
            " 19  습도표준편차         3071 non-null   float64\n",
            " 20  합계             3071 non-null   float64\n",
            " 21  계절             3071 non-null   Int64  \n",
            " 22  월              3071 non-null   int32  \n",
            " 23  요일_sin         3071 non-null   float64\n",
            " 24  요일_cos         3071 non-null   float64\n",
            " 25  연중일_sin        3071 non-null   float64\n",
            " 26  연중일_cos        3071 non-null   float64\n",
            " 27  요일_숫자          3071 non-null   int32  \n",
            " 28  합계_1일후         3071 non-null   float64\n",
            " 29  합계_2일후         3071 non-null   float64\n",
            "dtypes: Int64(2), float64(20), int32(2), int64(6)\n",
            "memory usage: 725.8 KB\n"
          ]
        }
      ],
      "source": [
        "nanji.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "dabc5e39",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2763, 27) (308, 27) (2763,) (308,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = nanji.drop(['합계','합계_1일후', '합계_2일후'], axis=1)\n",
        "y = nanji['합계']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.1, random_state=42)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "a1d193ef",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'from sklearn.metrics import mean_squared_error, r2_score\\nfrom pytorch_tabnet.tab_model import TabNetRegressor\\nimport torch\\n\\n# 6. TabNetRegressor\\ntabnet = TabNetRegressor(\\n    n_d=10, n_a=10, n_steps=5,\\n    gamma=1.5, n_independent=2, n_shared=2,\\n    optimizer_fn=torch.optim.Adam,\\n    optimizer_params=dict(lr=2e-2),\\n    scheduler_params={\"step_size\":20, \"gamma\":0.9},\\n    scheduler_fn=torch.optim.lr_scheduler.StepLR,\\n    mask_type=\\'entmax\\',\\n    verbose=1\\n)\\n\\n# numpy 변환\\nX_train_np = X_train_scaled.astype(np.float32)\\nX_test_np  = X_test_scaled.astype(np.float32)\\ny_train_np = y_train.to_numpy().reshape(-1, 1).astype(np.float32)  \\ny_test_np  = y_test.to_numpy().reshape(-1, 1).astype(np.float32)   \\n\\ntabnet.fit(\\n    X_train=X_train_np, y_train=y_train_np,\\n    eval_set=[(X_test_np, y_test_np)],\\n    eval_metric=[\"rmse\"],\\n    max_epochs=100,\\n    patience=5,\\n    batch_size=32,\\n    virtual_batch_size=16,\\n    num_workers=0,\\n    drop_last=False\\n)\\n\\ny_pred_tabnet = tabnet.predict(X_test_np).squeeze()\\nresults = {}\\nresults[\"TabNet\"] = {\\n    \"model\": tabnet,\\n    \"rmse\": np.sqrt(mean_squared_error(y_test_np, y_pred_tabnet)),\\n    \"r2\": r2_score(y_test_np, y_pred_tabnet),\\n    \"y_pred\": y_pred_tabnet\\n}\\n\\n# 7. 결과 출력\\nfor name, res in results.items():\\n    print(f\"{name}: RMSE={res[\\'rmse\\']:.2f}, R²={res[\\'r2\\']:.4f}\")\\n'"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''from sklearn.metrics import mean_squared_error, r2_score\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "import torch\n",
        "\n",
        "# 6. TabNetRegressor\n",
        "tabnet = TabNetRegressor(\n",
        "    n_d=10, n_a=10, n_steps=5,\n",
        "    gamma=1.5, n_independent=2, n_shared=2,\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=2e-2),\n",
        "    scheduler_params={\"step_size\":20, \"gamma\":0.9},\n",
        "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "    mask_type='entmax',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# numpy 변환\n",
        "X_train_np = X_train_scaled.astype(np.float32)\n",
        "X_test_np  = X_test_scaled.astype(np.float32)\n",
        "y_train_np = y_train.to_numpy().reshape(-1, 1).astype(np.float32)  \n",
        "y_test_np  = y_test.to_numpy().reshape(-1, 1).astype(np.float32)   \n",
        "\n",
        "tabnet.fit(\n",
        "    X_train=X_train_np, y_train=y_train_np,\n",
        "    eval_set=[(X_test_np, y_test_np)],\n",
        "    eval_metric=[\"rmse\"],\n",
        "    max_epochs=100,\n",
        "    patience=5,\n",
        "    batch_size=32,\n",
        "    virtual_batch_size=16,\n",
        "    num_workers=0,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "y_pred_tabnet = tabnet.predict(X_test_np).squeeze()\n",
        "results = {}\n",
        "results[\"TabNet\"] = {\n",
        "    \"model\": tabnet,\n",
        "    \"rmse\": np.sqrt(mean_squared_error(y_test_np, y_pred_tabnet)),\n",
        "    \"r2\": r2_score(y_test_np, y_pred_tabnet),\n",
        "    \"y_pred\": y_pred_tabnet\n",
        "}\n",
        "\n",
        "# 7. 결과 출력\n",
        "for name, res in results.items():\n",
        "    print(f\"{name}: RMSE={res['rmse']:.2f}, R²={res['r2']:.4f}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de58c042",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SPLIT] train=(2763, 28), test=(308, 28)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 4.75102 | val_0_rmse: 1.84898 |  0:00:00s\n",
            "epoch 1  | loss: 3.30284 | val_0_rmse: 1.20244 |  0:00:00s\n",
            "epoch 2  | loss: 2.27439 | val_0_rmse: 1.01814 |  0:00:00s\n",
            "epoch 3  | loss: 1.7867  | val_0_rmse: 1.00781 |  0:00:00s\n",
            "epoch 4  | loss: 1.60676 | val_0_rmse: 0.99834 |  0:00:00s\n",
            "epoch 5  | loss: 1.53675 | val_0_rmse: 1.01613 |  0:00:01s\n",
            "epoch 6  | loss: 1.31314 | val_0_rmse: 1.05899 |  0:00:01s\n",
            "epoch 7  | loss: 1.17958 | val_0_rmse: 1.06617 |  0:00:01s\n",
            "epoch 8  | loss: 1.05648 | val_0_rmse: 1.03472 |  0:00:01s\n",
            "epoch 9  | loss: 1.04824 | val_0_rmse: 1.05552 |  0:00:01s\n",
            "epoch 10 | loss: 0.94672 | val_0_rmse: 1.06754 |  0:00:02s\n",
            "epoch 11 | loss: 0.84219 | val_0_rmse: 0.96109 |  0:00:02s\n",
            "epoch 12 | loss: 0.86821 | val_0_rmse: 0.87624 |  0:00:02s\n",
            "epoch 13 | loss: 0.81331 | val_0_rmse: 0.87541 |  0:00:02s\n",
            "epoch 14 | loss: 0.76561 | val_0_rmse: 0.88799 |  0:00:02s\n",
            "epoch 15 | loss: 0.7455  | val_0_rmse: 0.86846 |  0:00:02s\n",
            "epoch 16 | loss: 0.68841 | val_0_rmse: 0.82915 |  0:00:03s\n",
            "epoch 17 | loss: 0.6742  | val_0_rmse: 0.83316 |  0:00:03s\n",
            "epoch 18 | loss: 0.65581 | val_0_rmse: 0.81455 |  0:00:03s\n",
            "epoch 19 | loss: 0.65367 | val_0_rmse: 0.77177 |  0:00:03s\n",
            "epoch 20 | loss: 0.64941 | val_0_rmse: 0.73611 |  0:00:03s\n",
            "epoch 21 | loss: 0.60203 | val_0_rmse: 0.71807 |  0:00:04s\n",
            "epoch 22 | loss: 0.58375 | val_0_rmse: 0.7625  |  0:00:04s\n",
            "epoch 23 | loss: 0.58326 | val_0_rmse: 0.73938 |  0:00:04s\n",
            "epoch 24 | loss: 0.55675 | val_0_rmse: 0.7685  |  0:00:04s\n",
            "epoch 25 | loss: 0.55589 | val_0_rmse: 0.79346 |  0:00:04s\n",
            "epoch 26 | loss: 0.5115  | val_0_rmse: 0.77632 |  0:00:04s\n",
            "epoch 27 | loss: 0.5272  | val_0_rmse: 0.78231 |  0:00:05s\n",
            "epoch 28 | loss: 0.50222 | val_0_rmse: 0.78475 |  0:00:05s\n",
            "epoch 29 | loss: 0.52963 | val_0_rmse: 0.75193 |  0:00:05s\n",
            "epoch 30 | loss: 0.52155 | val_0_rmse: 0.70368 |  0:00:05s\n",
            "epoch 31 | loss: 0.51417 | val_0_rmse: 0.70686 |  0:00:05s\n",
            "epoch 32 | loss: 0.48431 | val_0_rmse: 0.75136 |  0:00:06s\n",
            "epoch 33 | loss: 0.47849 | val_0_rmse: 0.80516 |  0:00:06s\n",
            "epoch 34 | loss: 0.46554 | val_0_rmse: 0.79097 |  0:00:06s\n",
            "epoch 35 | loss: 0.4924  | val_0_rmse: 0.81865 |  0:00:06s\n",
            "epoch 36 | loss: 0.48791 | val_0_rmse: 0.8129  |  0:00:06s\n",
            "epoch 37 | loss: 0.49101 | val_0_rmse: 0.75379 |  0:00:06s\n",
            "epoch 38 | loss: 0.45629 | val_0_rmse: 0.70097 |  0:00:07s\n",
            "epoch 39 | loss: 0.44167 | val_0_rmse: 0.83021 |  0:00:07s\n",
            "epoch 40 | loss: 0.44149 | val_0_rmse: 0.73541 |  0:00:07s\n",
            "epoch 41 | loss: 0.42889 | val_0_rmse: 0.66744 |  0:00:07s\n",
            "epoch 42 | loss: 0.44368 | val_0_rmse: 0.67836 |  0:00:07s\n",
            "epoch 43 | loss: 0.42186 | val_0_rmse: 0.67992 |  0:00:08s\n",
            "epoch 44 | loss: 0.43541 | val_0_rmse: 0.70995 |  0:00:08s\n",
            "epoch 45 | loss: 0.40737 | val_0_rmse: 0.67131 |  0:00:08s\n",
            "epoch 46 | loss: 0.40445 | val_0_rmse: 0.63738 |  0:00:08s\n",
            "epoch 47 | loss: 0.41957 | val_0_rmse: 0.65466 |  0:00:08s\n",
            "epoch 48 | loss: 0.42495 | val_0_rmse: 0.62279 |  0:00:08s\n",
            "epoch 49 | loss: 0.39615 | val_0_rmse: 0.64846 |  0:00:09s\n",
            "epoch 50 | loss: 0.41777 | val_0_rmse: 0.68816 |  0:00:09s\n",
            "epoch 51 | loss: 0.40832 | val_0_rmse: 0.64437 |  0:00:09s\n",
            "epoch 52 | loss: 0.40189 | val_0_rmse: 0.64713 |  0:00:09s\n",
            "epoch 53 | loss: 0.39145 | val_0_rmse: 0.71794 |  0:00:09s\n",
            "epoch 54 | loss: 0.39027 | val_0_rmse: 0.66375 |  0:00:10s\n",
            "epoch 55 | loss: 0.37963 | val_0_rmse: 0.65593 |  0:00:10s\n",
            "epoch 56 | loss: 0.37246 | val_0_rmse: 0.67132 |  0:00:10s\n",
            "epoch 57 | loss: 0.37824 | val_0_rmse: 0.66435 |  0:00:10s\n",
            "epoch 58 | loss: 0.3669  | val_0_rmse: 0.66626 |  0:00:10s\n",
            "epoch 59 | loss: 0.37619 | val_0_rmse: 0.63439 |  0:00:10s\n",
            "epoch 60 | loss: 0.35111 | val_0_rmse: 0.62397 |  0:00:11s\n",
            "epoch 61 | loss: 0.36709 | val_0_rmse: 0.6543  |  0:00:11s\n",
            "epoch 62 | loss: 0.37283 | val_0_rmse: 0.66346 |  0:00:11s\n",
            "epoch 63 | loss: 0.37647 | val_0_rmse: 0.67123 |  0:00:11s\n",
            "epoch 64 | loss: 0.36222 | val_0_rmse: 0.68633 |  0:00:11s\n",
            "epoch 65 | loss: 0.34466 | val_0_rmse: 0.6691  |  0:00:12s\n",
            "epoch 66 | loss: 0.35136 | val_0_rmse: 0.64914 |  0:00:12s\n",
            "epoch 67 | loss: 0.35514 | val_0_rmse: 0.646   |  0:00:12s\n",
            "epoch 68 | loss: 0.35126 | val_0_rmse: 0.6617  |  0:00:12s\n",
            "epoch 69 | loss: 0.33898 | val_0_rmse: 0.63302 |  0:00:12s\n",
            "epoch 70 | loss: 0.33162 | val_0_rmse: 0.64277 |  0:00:13s\n",
            "epoch 71 | loss: 0.343   | val_0_rmse: 0.6534  |  0:00:13s\n",
            "epoch 72 | loss: 0.34327 | val_0_rmse: 0.63658 |  0:00:13s\n",
            "epoch 73 | loss: 0.33397 | val_0_rmse: 0.63423 |  0:00:13s\n",
            "epoch 74 | loss: 0.33065 | val_0_rmse: 0.60492 |  0:00:13s\n",
            "epoch 75 | loss: 0.34049 | val_0_rmse: 0.59718 |  0:00:13s\n",
            "epoch 76 | loss: 0.32695 | val_0_rmse: 0.63143 |  0:00:14s\n",
            "epoch 77 | loss: 0.32035 | val_0_rmse: 0.61228 |  0:00:14s\n",
            "epoch 78 | loss: 0.33102 | val_0_rmse: 0.60803 |  0:00:14s\n",
            "epoch 79 | loss: 0.33017 | val_0_rmse: 0.60924 |  0:00:14s\n",
            "epoch 80 | loss: 0.32386 | val_0_rmse: 0.60649 |  0:00:14s\n",
            "epoch 81 | loss: 0.31686 | val_0_rmse: 0.61894 |  0:00:15s\n",
            "epoch 82 | loss: 0.3161  | val_0_rmse: 0.65058 |  0:00:15s\n",
            "epoch 83 | loss: 0.32165 | val_0_rmse: 0.62676 |  0:00:15s\n",
            "epoch 84 | loss: 0.31609 | val_0_rmse: 0.65601 |  0:00:15s\n",
            "epoch 85 | loss: 0.31064 | val_0_rmse: 0.6273  |  0:00:15s\n",
            "epoch 86 | loss: 0.31433 | val_0_rmse: 0.60736 |  0:00:16s\n",
            "epoch 87 | loss: 0.32254 | val_0_rmse: 0.60674 |  0:00:16s\n",
            "epoch 88 | loss: 0.31787 | val_0_rmse: 0.59965 |  0:00:16s\n",
            "epoch 89 | loss: 0.29983 | val_0_rmse: 0.60177 |  0:00:16s\n",
            "epoch 90 | loss: 0.29852 | val_0_rmse: 0.6316  |  0:00:16s\n",
            "epoch 91 | loss: 0.29083 | val_0_rmse: 0.60188 |  0:00:16s\n",
            "epoch 92 | loss: 0.30473 | val_0_rmse: 0.60574 |  0:00:17s\n",
            "epoch 93 | loss: 0.31031 | val_0_rmse: 0.64364 |  0:00:17s\n",
            "epoch 94 | loss: 0.2997  | val_0_rmse: 0.60001 |  0:00:17s\n",
            "epoch 95 | loss: 0.30242 | val_0_rmse: 0.59677 |  0:00:17s\n",
            "epoch 96 | loss: 0.28146 | val_0_rmse: 0.62011 |  0:00:17s\n",
            "epoch 97 | loss: 0.29797 | val_0_rmse: 0.62284 |  0:00:18s\n",
            "epoch 98 | loss: 0.29352 | val_0_rmse: 0.5984  |  0:00:18s\n",
            "epoch 99 | loss: 0.27374 | val_0_rmse: 0.59645 |  0:00:18s\n",
            "epoch 100| loss: 0.29477 | val_0_rmse: 0.62035 |  0:00:18s\n",
            "epoch 101| loss: 0.2939  | val_0_rmse: 0.6013  |  0:00:18s\n",
            "epoch 102| loss: 0.27524 | val_0_rmse: 0.58223 |  0:00:18s\n",
            "epoch 103| loss: 0.28669 | val_0_rmse: 0.59303 |  0:00:19s\n",
            "epoch 104| loss: 0.29424 | val_0_rmse: 0.60257 |  0:00:19s\n",
            "epoch 105| loss: 0.28243 | val_0_rmse: 0.59028 |  0:00:19s\n",
            "epoch 106| loss: 0.27953 | val_0_rmse: 0.61652 |  0:00:19s\n",
            "epoch 107| loss: 0.2901  | val_0_rmse: 0.61142 |  0:00:19s\n",
            "epoch 108| loss: 0.29489 | val_0_rmse: 0.61983 |  0:00:20s\n",
            "epoch 109| loss: 0.27273 | val_0_rmse: 0.63601 |  0:00:20s\n",
            "epoch 110| loss: 0.28033 | val_0_rmse: 0.63062 |  0:00:20s\n",
            "epoch 111| loss: 0.27329 | val_0_rmse: 0.64714 |  0:00:20s\n",
            "epoch 112| loss: 0.29214 | val_0_rmse: 0.6301  |  0:00:20s\n",
            "epoch 113| loss: 0.27794 | val_0_rmse: 0.63257 |  0:00:21s\n",
            "epoch 114| loss: 0.27008 | val_0_rmse: 0.65073 |  0:00:21s\n",
            "epoch 115| loss: 0.27246 | val_0_rmse: 0.63008 |  0:00:21s\n",
            "epoch 116| loss: 0.27277 | val_0_rmse: 0.63383 |  0:00:21s\n",
            "epoch 117| loss: 0.25764 | val_0_rmse: 0.62671 |  0:00:21s\n",
            "epoch 118| loss: 0.26367 | val_0_rmse: 0.61721 |  0:00:22s\n",
            "epoch 119| loss: 0.26001 | val_0_rmse: 0.60872 |  0:00:22s\n",
            "epoch 120| loss: 0.25737 | val_0_rmse: 0.65065 |  0:00:22s\n",
            "epoch 121| loss: 0.25371 | val_0_rmse: 0.62119 |  0:00:22s\n",
            "epoch 122| loss: 0.26082 | val_0_rmse: 0.61511 |  0:00:22s\n",
            "epoch 123| loss: 0.25361 | val_0_rmse: 0.62399 |  0:00:23s\n",
            "epoch 124| loss: 0.26098 | val_0_rmse: 0.64531 |  0:00:23s\n",
            "epoch 125| loss: 0.26256 | val_0_rmse: 0.60359 |  0:00:23s\n",
            "epoch 126| loss: 0.26028 | val_0_rmse: 0.6056  |  0:00:23s\n",
            "epoch 127| loss: 0.25769 | val_0_rmse: 0.63937 |  0:00:23s\n",
            "epoch 128| loss: 0.24017 | val_0_rmse: 0.62839 |  0:00:23s\n",
            "epoch 129| loss: 0.25295 | val_0_rmse: 0.62762 |  0:00:24s\n",
            "epoch 130| loss: 0.24716 | val_0_rmse: 0.65526 |  0:00:24s\n",
            "epoch 131| loss: 0.24547 | val_0_rmse: 0.60275 |  0:00:24s\n",
            "epoch 132| loss: 0.25591 | val_0_rmse: 0.59897 |  0:00:24s\n",
            "epoch 133| loss: 0.2453  | val_0_rmse: 0.64011 |  0:00:24s\n",
            "epoch 134| loss: 0.24153 | val_0_rmse: 0.61553 |  0:00:25s\n",
            "epoch 135| loss: 0.25156 | val_0_rmse: 0.64028 |  0:00:25s\n",
            "epoch 136| loss: 0.23433 | val_0_rmse: 0.66224 |  0:00:25s\n",
            "epoch 137| loss: 0.25486 | val_0_rmse: 0.66755 |  0:00:25s\n",
            "epoch 138| loss: 0.24265 | val_0_rmse: 0.68302 |  0:00:25s\n",
            "epoch 139| loss: 0.23424 | val_0_rmse: 0.68238 |  0:00:25s\n",
            "epoch 140| loss: 0.23225 | val_0_rmse: 0.698   |  0:00:26s\n",
            "epoch 141| loss: 0.2395  | val_0_rmse: 0.68765 |  0:00:26s\n",
            "epoch 142| loss: 0.24161 | val_0_rmse: 0.64957 |  0:00:26s\n",
            "epoch 143| loss: 0.23485 | val_0_rmse: 0.64578 |  0:00:26s\n",
            "epoch 144| loss: 0.23716 | val_0_rmse: 0.63926 |  0:00:26s\n",
            "epoch 145| loss: 0.24183 | val_0_rmse: 0.6415  |  0:00:27s\n",
            "epoch 146| loss: 0.22817 | val_0_rmse: 0.64357 |  0:00:27s\n",
            "epoch 147| loss: 0.23413 | val_0_rmse: 0.66759 |  0:00:27s\n",
            "epoch 148| loss: 0.22675 | val_0_rmse: 0.64161 |  0:00:27s\n",
            "epoch 149| loss: 0.24934 | val_0_rmse: 0.62449 |  0:00:27s\n",
            "epoch 150| loss: 0.23404 | val_0_rmse: 0.6326  |  0:00:27s\n",
            "epoch 151| loss: 0.22455 | val_0_rmse: 0.64    |  0:00:28s\n",
            "epoch 152| loss: 0.22275 | val_0_rmse: 0.6342  |  0:00:28s\n",
            "\n",
            "Early stopping occurred at epoch 152 with best_epoch = 102 and best_val_0_rmse = 0.58223\n",
            "✅ TabNet training time: 28.3s\n",
            "[TabNet] MAE=52185.653  RMSE=79353.951  R²=0.3449\n",
            "[Naive-1]   RMSE=76245.847  R²=0.3952\n",
            "[Week-Naive]RMSE=100431.342  R²=-0.0494\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math, time\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "import torch\n",
        "\n",
        "# 1) 데이터 분리 \n",
        "TARGET = '합계_1일후'\n",
        "X = nanji.drop(columns=[c for c in ['합계_1일후','합계_2일후'] if c in nanji.columns]).copy()\n",
        "y = nanji[TARGET].copy()\n",
        "\n",
        "n = len(X)\n",
        "test_n = int(math.ceil(n * 0.10))\n",
        "train_n = n - test_n\n",
        "\n",
        "X_train, X_test = X.iloc[:train_n, :].copy(), X.iloc[train_n:, :].copy()\n",
        "y_train, y_test = y.iloc[:train_n].copy(), y.iloc[train_n:].copy()\n",
        "\n",
        "print(f\"[SPLIT] train={X_train.shape}, test={X_test.shape}\")\n",
        "\n",
        "# 2) 스케일링 (누수 방지: train으로만 fit)\n",
        "x_scaler = StandardScaler()\n",
        "X_train_s = x_scaler.fit_transform(X_train)\n",
        "X_test_s  = x_scaler.transform(X_test)\n",
        "\n",
        "y_scaler = StandardScaler()\n",
        "y_train_s = y_scaler.fit_transform(y_train.values.reshape(-1,1)).astype(np.float32)  # (n,1)\n",
        "y_test_s  = y_scaler.transform(y_test.values.reshape(-1,1)).astype(np.float32)      # (m,1)\n",
        "\n",
        "# TabNet 입력 dtype/shape\n",
        "X_train_np = X_train_s.astype(np.float32)\n",
        "X_test_np  = X_test_s.astype(np.float32)\n",
        "\n",
        "# 3) TabNet 설정\n",
        "tabnet = TabNetRegressor(\n",
        "    n_d=32, n_a=32, n_steps=4,\n",
        "    gamma=1, n_independent=2, n_shared=2,\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=1e-3, weight_decay=1e-5),\n",
        "    scheduler_fn=None,      # 우선 스케줄러 OFF로 수렴 확인\n",
        "    mask_type='entmax',                # 엔트맥스가 일반적으로 안정적\n",
        "    verbose=1,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "t0 = time.time()\n",
        "tabnet.fit(\n",
        "    X_train=X_train_np,\n",
        "    y_train=y_train_s,                    # <-- (n,1) 2D\n",
        "    eval_set=[(X_test_np, y_test_s)],     # <-- (m,1) 2D\n",
        "    eval_metric=[\"rmse\"],                 # 표준화된 y 기준 RMSE\n",
        "    max_epochs=500,\n",
        "    patience=50,\n",
        "    batch_size=512,\n",
        "    virtual_batch_size=128,\n",
        "    num_workers=0,\n",
        "    drop_last=False\n",
        ")\n",
        "print(f\"TabNet training time: {time.time()-t0:.1f}s\")\n",
        "\n",
        "# 4) 예측 & 역변환 평가 \n",
        "y_pred_s = tabnet.predict(X_test_np).squeeze()                 # (m,)\n",
        "y_pred = y_scaler.inverse_transform(y_pred_s.reshape(-1,1)).ravel()\n",
        "\n",
        "y_true = y_test.values\n",
        "mse  = mean_squared_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mse)   # 버전 독립 RMSE\n",
        "mae  = mean_absolute_error(y_true, y_pred)\n",
        "r2   = r2_score(y_true, y_pred)\n",
        "\n",
        "print(f\"[TabNet] MAE={mae:.3f}  RMSE={rmse:.3f}  R²={r2:.4f}\")\n",
        "\n",
        "# 5) 기준선과 비교 (원 스케일)\n",
        "y_test_arr = y_true\n",
        "# 어제값(naive-1): 테스트 내 시프트로 근사 비교 (첫 값 보정)\n",
        "y_na1 = np.roll(y_test_arr, 1); y_na1[0] = y_test_arr[0]\n",
        "# 7일 전(week naive): 주간 패턴 근사\n",
        "y_wk  = np.roll(y_test_arr, 7); y_wk[:7] = y_test_arr[:7]\n",
        "\n",
        "rmse_na1 = np.sqrt(mean_squared_error(y_test_arr, y_na1)); r2_na1 = r2_score(y_test_arr, y_na1)\n",
        "rmse_wk  = np.sqrt(mean_squared_error(y_test_arr, y_wk));  r2_wk  = r2_score(y_test_arr, y_wk)\n",
        "\n",
        "print(f\"[Naive-1]   RMSE={rmse_na1:.3f}  R²={r2_na1:.4f}\")\n",
        "print(f\"[Week-Naive]RMSE={rmse_wk:.3f}  R²={r2_wk:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a7e5ae5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SPLIT] train=(2763, 27), test=(308, 27)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 3.61672 | val_0_rmse: 1.75849 |  0:00:00s\n",
            "epoch 1  | loss: 2.71206 | val_0_rmse: 1.26097 |  0:00:00s\n",
            "epoch 2  | loss: 2.13977 | val_0_rmse: 1.07348 |  0:00:00s\n",
            "epoch 3  | loss: 1.75452 | val_0_rmse: 1.03323 |  0:00:00s\n",
            "epoch 4  | loss: 1.48026 | val_0_rmse: 0.9804  |  0:00:00s\n",
            "epoch 5  | loss: 1.26208 | val_0_rmse: 0.92909 |  0:00:01s\n",
            "epoch 6  | loss: 1.19113 | val_0_rmse: 0.88741 |  0:00:01s\n",
            "epoch 7  | loss: 1.08766 | val_0_rmse: 0.89136 |  0:00:01s\n",
            "epoch 8  | loss: 0.98387 | val_0_rmse: 0.8518  |  0:00:01s\n",
            "epoch 9  | loss: 0.95525 | val_0_rmse: 0.80904 |  0:00:01s\n",
            "epoch 10 | loss: 0.88944 | val_0_rmse: 0.78601 |  0:00:02s\n",
            "epoch 11 | loss: 0.80558 | val_0_rmse: 0.83771 |  0:00:02s\n",
            "epoch 12 | loss: 0.77179 | val_0_rmse: 0.81076 |  0:00:02s\n",
            "epoch 13 | loss: 0.77625 | val_0_rmse: 0.83842 |  0:00:02s\n",
            "epoch 14 | loss: 0.71271 | val_0_rmse: 0.80068 |  0:00:02s\n",
            "epoch 15 | loss: 0.70846 | val_0_rmse: 0.79525 |  0:00:02s\n",
            "epoch 16 | loss: 0.6908  | val_0_rmse: 0.75433 |  0:00:03s\n",
            "epoch 17 | loss: 0.70553 | val_0_rmse: 0.75273 |  0:00:03s\n",
            "epoch 18 | loss: 0.6792  | val_0_rmse: 0.75303 |  0:00:03s\n",
            "epoch 19 | loss: 0.65538 | val_0_rmse: 0.74867 |  0:00:03s\n",
            "epoch 20 | loss: 0.64954 | val_0_rmse: 0.75414 |  0:00:03s\n",
            "epoch 21 | loss: 0.63392 | val_0_rmse: 0.74904 |  0:00:03s\n",
            "epoch 22 | loss: 0.57855 | val_0_rmse: 0.70376 |  0:00:04s\n",
            "epoch 23 | loss: 0.60068 | val_0_rmse: 0.68406 |  0:00:04s\n",
            "epoch 24 | loss: 0.56382 | val_0_rmse: 0.69459 |  0:00:04s\n",
            "epoch 25 | loss: 0.54232 | val_0_rmse: 0.71185 |  0:00:04s\n",
            "epoch 26 | loss: 0.56332 | val_0_rmse: 0.71734 |  0:00:04s\n",
            "epoch 27 | loss: 0.51633 | val_0_rmse: 0.72209 |  0:00:05s\n",
            "epoch 28 | loss: 0.53806 | val_0_rmse: 0.67608 |  0:00:05s\n",
            "epoch 29 | loss: 0.5233  | val_0_rmse: 0.64702 |  0:00:05s\n",
            "epoch 30 | loss: 0.5125  | val_0_rmse: 0.62715 |  0:00:05s\n",
            "epoch 31 | loss: 0.50891 | val_0_rmse: 0.65066 |  0:00:05s\n",
            "epoch 32 | loss: 0.51156 | val_0_rmse: 0.65093 |  0:00:05s\n",
            "epoch 33 | loss: 0.49536 | val_0_rmse: 0.63662 |  0:00:06s\n",
            "epoch 34 | loss: 0.47223 | val_0_rmse: 0.59622 |  0:00:06s\n",
            "epoch 35 | loss: 0.49024 | val_0_rmse: 0.63117 |  0:00:06s\n",
            "epoch 36 | loss: 0.45044 | val_0_rmse: 0.61186 |  0:00:06s\n",
            "epoch 37 | loss: 0.47076 | val_0_rmse: 0.64347 |  0:00:06s\n",
            "epoch 38 | loss: 0.4549  | val_0_rmse: 0.64215 |  0:00:07s\n",
            "epoch 39 | loss: 0.43729 | val_0_rmse: 0.63423 |  0:00:07s\n",
            "epoch 40 | loss: 0.43763 | val_0_rmse: 0.62655 |  0:00:07s\n",
            "epoch 41 | loss: 0.43585 | val_0_rmse: 0.60827 |  0:00:07s\n",
            "epoch 42 | loss: 0.42224 | val_0_rmse: 0.62373 |  0:00:07s\n",
            "epoch 43 | loss: 0.43379 | val_0_rmse: 0.603   |  0:00:07s\n",
            "epoch 44 | loss: 0.41789 | val_0_rmse: 0.58762 |  0:00:08s\n",
            "epoch 45 | loss: 0.40074 | val_0_rmse: 0.5516  |  0:00:08s\n",
            "epoch 46 | loss: 0.4092  | val_0_rmse: 0.57843 |  0:00:08s\n",
            "epoch 47 | loss: 0.39205 | val_0_rmse: 0.61083 |  0:00:08s\n",
            "epoch 48 | loss: 0.40759 | val_0_rmse: 0.59901 |  0:00:08s\n",
            "epoch 49 | loss: 0.38038 | val_0_rmse: 0.5868  |  0:00:09s\n",
            "epoch 50 | loss: 0.36942 | val_0_rmse: 0.59629 |  0:00:09s\n",
            "epoch 51 | loss: 0.37566 | val_0_rmse: 0.61547 |  0:00:09s\n",
            "epoch 52 | loss: 0.37444 | val_0_rmse: 0.59013 |  0:00:09s\n",
            "epoch 53 | loss: 0.37268 | val_0_rmse: 0.58495 |  0:00:09s\n",
            "epoch 54 | loss: 0.37166 | val_0_rmse: 0.60358 |  0:00:09s\n",
            "epoch 55 | loss: 0.36554 | val_0_rmse: 0.5981  |  0:00:10s\n",
            "epoch 56 | loss: 0.36855 | val_0_rmse: 0.55382 |  0:00:10s\n",
            "epoch 57 | loss: 0.35246 | val_0_rmse: 0.52812 |  0:00:10s\n",
            "epoch 58 | loss: 0.33259 | val_0_rmse: 0.56587 |  0:00:10s\n",
            "epoch 59 | loss: 0.34726 | val_0_rmse: 0.55216 |  0:00:10s\n",
            "epoch 60 | loss: 0.33678 | val_0_rmse: 0.53677 |  0:00:11s\n",
            "epoch 61 | loss: 0.34673 | val_0_rmse: 0.5347  |  0:00:11s\n",
            "epoch 62 | loss: 0.34113 | val_0_rmse: 0.54486 |  0:00:11s\n",
            "epoch 63 | loss: 0.34121 | val_0_rmse: 0.5629  |  0:00:11s\n",
            "epoch 64 | loss: 0.34945 | val_0_rmse: 0.5401  |  0:00:11s\n",
            "epoch 65 | loss: 0.33154 | val_0_rmse: 0.52354 |  0:00:11s\n",
            "epoch 66 | loss: 0.34668 | val_0_rmse: 0.56301 |  0:00:12s\n",
            "epoch 67 | loss: 0.32063 | val_0_rmse: 0.54405 |  0:00:12s\n",
            "epoch 68 | loss: 0.31641 | val_0_rmse: 0.5763  |  0:00:12s\n",
            "epoch 69 | loss: 0.34153 | val_0_rmse: 0.5473  |  0:00:12s\n",
            "epoch 70 | loss: 0.33669 | val_0_rmse: 0.53452 |  0:00:12s\n",
            "epoch 71 | loss: 0.31095 | val_0_rmse: 0.54198 |  0:00:13s\n",
            "epoch 72 | loss: 0.32951 | val_0_rmse: 0.55252 |  0:00:13s\n",
            "epoch 73 | loss: 0.34302 | val_0_rmse: 0.53811 |  0:00:13s\n",
            "epoch 74 | loss: 0.31875 | val_0_rmse: 0.5378  |  0:00:13s\n",
            "epoch 75 | loss: 0.32961 | val_0_rmse: 0.54568 |  0:00:13s\n",
            "epoch 76 | loss: 0.29237 | val_0_rmse: 0.54886 |  0:00:13s\n",
            "epoch 77 | loss: 0.34674 | val_0_rmse: 0.55883 |  0:00:14s\n",
            "epoch 78 | loss: 0.3162  | val_0_rmse: 0.58376 |  0:00:14s\n",
            "epoch 79 | loss: 0.30667 | val_0_rmse: 0.55366 |  0:00:14s\n",
            "epoch 80 | loss: 0.31605 | val_0_rmse: 0.54816 |  0:00:14s\n",
            "epoch 81 | loss: 0.30512 | val_0_rmse: 0.55207 |  0:00:14s\n",
            "epoch 82 | loss: 0.29151 | val_0_rmse: 0.55294 |  0:00:14s\n",
            "epoch 83 | loss: 0.28881 | val_0_rmse: 0.56933 |  0:00:15s\n",
            "epoch 84 | loss: 0.2881  | val_0_rmse: 0.56483 |  0:00:15s\n",
            "epoch 85 | loss: 0.30895 | val_0_rmse: 0.55086 |  0:00:15s\n",
            "epoch 86 | loss: 0.30835 | val_0_rmse: 0.56887 |  0:00:15s\n",
            "epoch 87 | loss: 0.28983 | val_0_rmse: 0.55331 |  0:00:15s\n",
            "epoch 88 | loss: 0.27939 | val_0_rmse: 0.53748 |  0:00:16s\n",
            "epoch 89 | loss: 0.29435 | val_0_rmse: 0.53618 |  0:00:16s\n",
            "epoch 90 | loss: 0.26422 | val_0_rmse: 0.52818 |  0:00:16s\n",
            "epoch 91 | loss: 0.30284 | val_0_rmse: 0.53511 |  0:00:16s\n",
            "epoch 92 | loss: 0.29213 | val_0_rmse: 0.53115 |  0:00:16s\n",
            "epoch 93 | loss: 0.27695 | val_0_rmse: 0.52305 |  0:00:16s\n",
            "epoch 94 | loss: 0.28263 | val_0_rmse: 0.51722 |  0:00:17s\n",
            "epoch 95 | loss: 0.28682 | val_0_rmse: 0.55367 |  0:00:17s\n",
            "epoch 96 | loss: 0.27375 | val_0_rmse: 0.52852 |  0:00:17s\n",
            "epoch 97 | loss: 0.26722 | val_0_rmse: 0.48968 |  0:00:17s\n",
            "epoch 98 | loss: 0.27106 | val_0_rmse: 0.51706 |  0:00:17s\n",
            "epoch 99 | loss: 0.26834 | val_0_rmse: 0.52645 |  0:00:18s\n",
            "epoch 100| loss: 0.25035 | val_0_rmse: 0.54431 |  0:00:18s\n",
            "epoch 101| loss: 0.28044 | val_0_rmse: 0.5365  |  0:00:18s\n",
            "epoch 102| loss: 0.26217 | val_0_rmse: 0.53361 |  0:00:18s\n",
            "epoch 103| loss: 0.26102 | val_0_rmse: 0.53631 |  0:00:18s\n",
            "epoch 104| loss: 0.26908 | val_0_rmse: 0.5236  |  0:00:18s\n",
            "epoch 105| loss: 0.25368 | val_0_rmse: 0.52272 |  0:00:19s\n",
            "epoch 106| loss: 0.26503 | val_0_rmse: 0.54259 |  0:00:19s\n",
            "epoch 107| loss: 0.25889 | val_0_rmse: 0.5273  |  0:00:19s\n",
            "epoch 108| loss: 0.25956 | val_0_rmse: 0.53092 |  0:00:19s\n",
            "epoch 109| loss: 0.25969 | val_0_rmse: 0.54415 |  0:00:19s\n",
            "epoch 110| loss: 0.2539  | val_0_rmse: 0.52715 |  0:00:19s\n",
            "epoch 111| loss: 0.25208 | val_0_rmse: 0.50842 |  0:00:20s\n",
            "epoch 112| loss: 0.25045 | val_0_rmse: 0.51008 |  0:00:20s\n",
            "epoch 113| loss: 0.25596 | val_0_rmse: 0.5275  |  0:00:20s\n",
            "epoch 114| loss: 0.25323 | val_0_rmse: 0.5242  |  0:00:20s\n",
            "epoch 115| loss: 0.26584 | val_0_rmse: 0.5273  |  0:00:20s\n",
            "epoch 116| loss: 0.25028 | val_0_rmse: 0.52546 |  0:00:21s\n",
            "epoch 117| loss: 0.24159 | val_0_rmse: 0.54453 |  0:00:21s\n",
            "epoch 118| loss: 0.24119 | val_0_rmse: 0.53201 |  0:00:21s\n",
            "epoch 119| loss: 0.24578 | val_0_rmse: 0.5348  |  0:00:21s\n",
            "epoch 120| loss: 0.24918 | val_0_rmse: 0.54236 |  0:00:21s\n",
            "epoch 121| loss: 0.24867 | val_0_rmse: 0.55503 |  0:00:21s\n",
            "epoch 122| loss: 0.25149 | val_0_rmse: 0.50392 |  0:00:22s\n",
            "epoch 123| loss: 0.23855 | val_0_rmse: 0.49297 |  0:00:22s\n",
            "epoch 124| loss: 0.24715 | val_0_rmse: 0.48312 |  0:00:22s\n",
            "epoch 125| loss: 0.2446  | val_0_rmse: 0.48599 |  0:00:22s\n",
            "epoch 126| loss: 0.22945 | val_0_rmse: 0.50421 |  0:00:22s\n",
            "epoch 127| loss: 0.2328  | val_0_rmse: 0.48721 |  0:00:23s\n",
            "epoch 128| loss: 0.26009 | val_0_rmse: 0.49223 |  0:00:23s\n",
            "epoch 129| loss: 0.24469 | val_0_rmse: 0.52233 |  0:00:23s\n",
            "epoch 130| loss: 0.22803 | val_0_rmse: 0.51073 |  0:00:23s\n",
            "epoch 131| loss: 0.23231 | val_0_rmse: 0.50063 |  0:00:23s\n",
            "epoch 132| loss: 0.22814 | val_0_rmse: 0.52056 |  0:00:24s\n",
            "epoch 133| loss: 0.22575 | val_0_rmse: 0.51546 |  0:00:24s\n",
            "epoch 134| loss: 0.22102 | val_0_rmse: 0.51809 |  0:00:24s\n",
            "epoch 135| loss: 0.23125 | val_0_rmse: 0.51229 |  0:00:24s\n",
            "epoch 136| loss: 0.24407 | val_0_rmse: 0.49694 |  0:00:24s\n",
            "epoch 137| loss: 0.22358 | val_0_rmse: 0.50569 |  0:00:25s\n",
            "epoch 138| loss: 0.22341 | val_0_rmse: 0.51502 |  0:00:25s\n",
            "epoch 139| loss: 0.22089 | val_0_rmse: 0.51181 |  0:00:25s\n",
            "epoch 140| loss: 0.21656 | val_0_rmse: 0.50566 |  0:00:25s\n",
            "epoch 141| loss: 0.21777 | val_0_rmse: 0.51015 |  0:00:25s\n",
            "epoch 142| loss: 0.22429 | val_0_rmse: 0.51262 |  0:00:25s\n",
            "epoch 143| loss: 0.22944 | val_0_rmse: 0.49038 |  0:00:26s\n",
            "epoch 144| loss: 0.21888 | val_0_rmse: 0.4835  |  0:00:26s\n",
            "epoch 145| loss: 0.23667 | val_0_rmse: 0.50996 |  0:00:26s\n",
            "epoch 146| loss: 0.21799 | val_0_rmse: 0.49631 |  0:00:26s\n",
            "epoch 147| loss: 0.20813 | val_0_rmse: 0.49745 |  0:00:26s\n",
            "epoch 148| loss: 0.22272 | val_0_rmse: 0.51754 |  0:00:26s\n",
            "epoch 149| loss: 0.23512 | val_0_rmse: 0.53036 |  0:00:27s\n",
            "epoch 150| loss: 0.22725 | val_0_rmse: 0.51705 |  0:00:27s\n",
            "epoch 151| loss: 0.21823 | val_0_rmse: 0.51815 |  0:00:27s\n",
            "epoch 152| loss: 0.23742 | val_0_rmse: 0.53243 |  0:00:27s\n",
            "epoch 153| loss: 0.21901 | val_0_rmse: 0.50587 |  0:00:27s\n",
            "epoch 154| loss: 0.2284  | val_0_rmse: 0.52641 |  0:00:28s\n",
            "epoch 155| loss: 0.22426 | val_0_rmse: 0.51259 |  0:00:28s\n",
            "epoch 156| loss: 0.2311  | val_0_rmse: 0.50592 |  0:00:28s\n",
            "epoch 157| loss: 0.22028 | val_0_rmse: 0.5036  |  0:00:28s\n",
            "epoch 158| loss: 0.21841 | val_0_rmse: 0.5063  |  0:00:28s\n",
            "epoch 159| loss: 0.21249 | val_0_rmse: 0.51203 |  0:00:29s\n",
            "epoch 160| loss: 0.2079  | val_0_rmse: 0.5073  |  0:00:29s\n",
            "epoch 161| loss: 0.21406 | val_0_rmse: 0.50561 |  0:00:29s\n",
            "epoch 162| loss: 0.21343 | val_0_rmse: 0.51521 |  0:00:29s\n",
            "epoch 163| loss: 0.21657 | val_0_rmse: 0.50423 |  0:00:29s\n",
            "epoch 164| loss: 0.21222 | val_0_rmse: 0.4969  |  0:00:30s\n",
            "epoch 165| loss: 0.20129 | val_0_rmse: 0.51024 |  0:00:30s\n",
            "epoch 166| loss: 0.21326 | val_0_rmse: 0.54526 |  0:00:30s\n",
            "epoch 167| loss: 0.2198  | val_0_rmse: 0.52883 |  0:00:30s\n",
            "epoch 168| loss: 0.21805 | val_0_rmse: 0.55131 |  0:00:30s\n",
            "epoch 169| loss: 0.20907 | val_0_rmse: 0.54172 |  0:00:30s\n",
            "epoch 170| loss: 0.20673 | val_0_rmse: 0.564   |  0:00:31s\n",
            "epoch 171| loss: 0.21519 | val_0_rmse: 0.521   |  0:00:31s\n",
            "epoch 172| loss: 0.20521 | val_0_rmse: 0.51486 |  0:00:31s\n",
            "epoch 173| loss: 0.1912  | val_0_rmse: 0.50991 |  0:00:31s\n",
            "epoch 174| loss: 0.20625 | val_0_rmse: 0.52233 |  0:00:31s\n",
            "\n",
            "Early stopping occurred at epoch 174 with best_epoch = 124 and best_val_0_rmse = 0.48312\n",
            "✅ TabNet training time: 31.9s\n",
            "[TabNet] MAE=45066.492  RMSE=65628.170  R²=0.5974\n",
            "[Naive-1]   RMSE=76499.021  R²=0.4530\n",
            "[Week-Naive]RMSE=103966.023  R²=-0.0103\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math, time\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "import torch\n",
        "\n",
        "# 데이터 분리\n",
        "TARGET = '합계'\n",
        "X = nanji.drop(columns=[c for c in ['합계', '합계_1일후','합계_2일후'] if c in nanji.columns]).copy()\n",
        "y = nanji[TARGET].copy()\n",
        "\n",
        "n = len(X)\n",
        "test_n = int(math.ceil(n * 0.10))\n",
        "train_n = n - test_n\n",
        "\n",
        "X_train, X_test = X.iloc[:train_n, :].copy(), X.iloc[train_n:, :].copy()\n",
        "y_train, y_test = y.iloc[:train_n].copy(), y.iloc[train_n:].copy()\n",
        "\n",
        "print(f\"[SPLIT] train={X_train.shape}, test={X_test.shape}\")\n",
        "\n",
        "# 스케일링\n",
        "x_scaler = StandardScaler()\n",
        "X_train_s = x_scaler.fit_transform(X_train)\n",
        "X_test_s  = x_scaler.transform(X_test)\n",
        "\n",
        "y_scaler = StandardScaler()\n",
        "y_train_s = y_scaler.fit_transform(y_train.values.reshape(-1,1)).astype(np.float32)  # (n,1)\n",
        "y_test_s  = y_scaler.transform(y_test.values.reshape(-1,1)).astype(np.float32)      # (m,1)\n",
        "\n",
        "# TabNet 입력 dtype/shape\n",
        "X_train_np = X_train_s.astype(np.float32)\n",
        "X_test_np  = X_test_s.astype(np.float32)\n",
        "\n",
        "# TabNet 설정\n",
        "tabnet = TabNetRegressor(\n",
        "    n_d=32, n_a=32, n_steps=4,\n",
        "    gamma=1.5, n_independent=2, n_shared=2,\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=1e-3, weight_decay=1e-5),\n",
        "    scheduler_fn=None,                 # 우선 스케줄러 OFF로 수렴 확인\n",
        "    mask_type='entmax',                # 엔트맥스가 일반적으로 안정적\n",
        "    verbose=1,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "t0 = time.time()\n",
        "tabnet.fit(\n",
        "    X_train=X_train_np,\n",
        "    y_train=y_train_s,                    # <-- (n,1) 2D\n",
        "    eval_set=[(X_test_np, y_test_s)],     # <-- (m,1) 2D\n",
        "    eval_metric=[\"rmse\"],                 # 표준화된 y 기준 RMSE\n",
        "    max_epochs=500,\n",
        "    patience=50,\n",
        "    batch_size=512,\n",
        "    virtual_batch_size=128,\n",
        "    num_workers=0,\n",
        "    drop_last=False\n",
        ")\n",
        "print(f\"TabNet training time: {time.time()-t0:.1f}s\")\n",
        "\n",
        "y_pred_s = tabnet.predict(X_test_np).squeeze()                 # (m,)\n",
        "y_pred = y_scaler.inverse_transform(y_pred_s.reshape(-1,1)).ravel()\n",
        "\n",
        "y_true = y_test.values\n",
        "mse  = mean_squared_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mse)   # 버전 독립 RMSE\n",
        "mae  = mean_absolute_error(y_true, y_pred)\n",
        "r2   = r2_score(y_true, y_pred)\n",
        "\n",
        "print(f\"[TabNet] MAE={mae:.3f}  RMSE={rmse:.3f}  R²={r2:.4f}\")\n",
        "\n",
        "y_test_arr = y_true\n",
        "# 어제값(naive-1): 테스트 내 시프트로 근사 비교 (첫 값 보정)\n",
        "y_na1 = np.roll(y_test_arr, 1); y_na1[0] = y_test_arr[0]\n",
        "# 7일 전(week naive): 주간 패턴 근사\n",
        "y_wk  = np.roll(y_test_arr, 7); y_wk[:7] = y_test_arr[:7]\n",
        "\n",
        "rmse_na1 = np.sqrt(mean_squared_error(y_test_arr, y_na1)); r2_na1 = r2_score(y_test_arr, y_na1)\n",
        "rmse_wk  = np.sqrt(mean_squared_error(y_test_arr, y_wk));  r2_wk  = r2_score(y_test_arr, y_wk)\n",
        "\n",
        "print(f\"[Naive-1]   RMSE={rmse_na1:.3f}  R²={r2_na1:.4f}\")\n",
        "print(f\"[Week-Naive]RMSE={rmse_wk:.3f}  R²={r2_wk:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03fb8f2c",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2f1679e",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b11b77f1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "2b91af43",
      "metadata": {
        "id": "2b91af43"
      },
      "source": [
        "## TableOne\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dae257a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dae257a8",
        "outputId": "b8f9bad2-e5ca-45a8-a537-d09afb434fe2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 1.82031 | val_0_rmse: 0.79211 |  0:00:03s\n",
            "epoch 1  | loss: 1.29141 | val_0_rmse: 0.81296 |  0:00:05s\n",
            "epoch 2  | loss: 1.21116 | val_0_rmse: 0.83193 |  0:00:06s\n",
            "epoch 3  | loss: 1.06765 | val_0_rmse: 0.80885 |  0:00:08s\n",
            "epoch 4  | loss: 1.01738 | val_0_rmse: 0.77695 |  0:00:09s\n",
            "epoch 5  | loss: 1.00513 | val_0_rmse: 0.81134 |  0:00:10s\n",
            "epoch 6  | loss: 1.1137  | val_0_rmse: 0.76654 |  0:00:11s\n",
            "epoch 7  | loss: 1.01181 | val_0_rmse: 0.74934 |  0:00:12s\n",
            "epoch 8  | loss: 0.99809 | val_0_rmse: 0.78254 |  0:00:14s\n",
            "epoch 9  | loss: 0.98623 | val_0_rmse: 0.7726  |  0:00:16s\n",
            "epoch 10 | loss: 0.97368 | val_0_rmse: 0.74438 |  0:00:18s\n",
            "epoch 11 | loss: 1.04397 | val_0_rmse: 0.73356 |  0:00:20s\n",
            "epoch 12 | loss: 1.00513 | val_0_rmse: 0.74566 |  0:00:21s\n",
            "epoch 13 | loss: 1.00761 | val_0_rmse: 0.75986 |  0:00:22s\n",
            "epoch 14 | loss: 0.99114 | val_0_rmse: 0.73782 |  0:00:22s\n",
            "epoch 15 | loss: 0.95215 | val_0_rmse: 0.76794 |  0:00:23s\n",
            "epoch 16 | loss: 0.96789 | val_0_rmse: 0.75083 |  0:00:24s\n",
            "\n",
            "Early stopping occurred at epoch 16 with best_epoch = 11 and best_val_0_rmse = 0.73356\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TabNet: RMSE=0.73, R²=0.1064\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# 1. 데이터 생성\n",
        "target = data_clean['price']\n",
        "x_data_clean = data_clean.drop(['price'], axis=1)\n",
        "\n",
        "# 2. train-test 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_data_clean, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. 스케일링\n",
        "x_scaler = StandardScaler()\n",
        "y_scaler = StandardScaler()\n",
        "X_train_scaled = x_scaler.fit_transform(X_train)\n",
        "X_test_scaled = x_scaler.transform(X_test)\n",
        "Y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "Y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "\n",
        "# 6. TabNetRegressor\n",
        "tabnet = TabNetRegressor(\n",
        "    n_d=10, n_a=10, n_steps=5,\n",
        "    gamma=1.5, n_independent=2, n_shared=2,\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=2e-2),\n",
        "    scheduler_params={\"step_size\":10, \"gamma\":0.9},\n",
        "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "    mask_type='entmax',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# numpy 변환\n",
        "X_train_np = X_train_scaled\n",
        "X_test_np = X_test_scaled\n",
        "y_train_np = Y_train_scaled.reshape(-1, 1)\n",
        "y_test_np = Y_test_scaled.reshape(-1, 1)\n",
        "\n",
        "\n",
        "tabnet.fit(\n",
        "    X_train=X_train_np, y_train=y_train_np,\n",
        "    eval_set=[(X_test_np, y_test_np)],\n",
        "    eval_metric=[\"rmse\"],\n",
        "    max_epochs=100,\n",
        "    patience=5,\n",
        "    batch_size=32,\n",
        "    virtual_batch_size=16,\n",
        "    num_workers=0,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "y_pred_tabnet = tabnet.predict(X_test_np).squeeze()\n",
        "results = {}\n",
        "results[\"TabNet\"] = {\n",
        "    \"model\": tabnet,\n",
        "    \"rmse\": np.sqrt(mean_squared_error(y_test_np, y_pred_tabnet)),\n",
        "    \"r2\": r2_score(y_test_np, y_pred_tabnet),\n",
        "    \"y_pred\": y_pred_tabnet\n",
        "}\n",
        "\n",
        "# 7. 결과 출력\n",
        "for name, res in results.items():\n",
        "    print(f\"{name}: RMSE={res['rmse']:.2f}, R²={res['r2']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64628a87",
      "metadata": {
        "id": "64628a87"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "youngwon",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
