{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fXGv-XrjbZaq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXGv-XrjbZaq",
        "outputId": "8b7f4690-7256-43b9-a468-905296b479f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_tabnet in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (4.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pytorch_tabnet) (1.26.4)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pytorch_tabnet) (1.6.1)\n",
            "Requirement already satisfied: scipy>1.4 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pytorch_tabnet) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.3 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pytorch_tabnet) (2.8.0)\n",
            "Requirement already satisfied: tqdm>=4.36 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pytorch_tabnet) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from scikit_learn>0.21->pytorch_tabnet) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from scikit_learn>0.21->pytorch_tabnet) (3.6.0)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from torch>=1.3->pytorch_tabnet) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from torch>=1.3->pytorch_tabnet) (4.14.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from torch>=1.3->pytorch_tabnet) (1.14.0)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from torch>=1.3->pytorch_tabnet) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from torch>=1.3->pytorch_tabnet) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from torch>=1.3->pytorch_tabnet) (2025.7.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from sympy>=1.13.3->torch>=1.3->pytorch_tabnet) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from jinja2->torch>=1.3->pytorch_tabnet) (3.0.2)\n",
            "Requirement already satisfied: tableone in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (0.9.5)\n",
            "Requirement already satisfied: jinja2>=3.1.4 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from tableone) (3.1.6)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from tableone) (1.26.4)\n",
            "Requirement already satisfied: openpyxl>=3.1.2 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from tableone) (3.1.5)\n",
            "Requirement already satisfied: pandas>=2.0.3 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from tableone) (2.3.1)\n",
            "Requirement already satisfied: scipy>=1.10.1 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from tableone) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.14.1 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from tableone) (0.14.5)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from tableone) (0.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from jinja2>=3.1.4->tableone) (3.0.2)\n",
            "Requirement already satisfied: et-xmlfile in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from openpyxl>=3.1.2->tableone) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pandas>=2.0.3->tableone) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pandas>=2.0.3->tableone) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pandas>=2.0.3->tableone) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->tableone) (1.17.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from statsmodels>=0.14.1->tableone) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from statsmodels>=0.14.1->tableone) (25.0)\n",
            "Requirement already satisfied: catboost in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from catboost) (3.9.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from catboost) (2.3.1)\n",
            "Requirement already satisfied: scipy in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from catboost) (6.3.0)\n",
            "Requirement already satisfied: six in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from matplotlib->catboost) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from matplotlib->catboost) (3.2.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from matplotlib->catboost) (6.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->catboost) (3.21.0)\n",
            "Requirement already satisfied: narwhals>=1.15.1 in /opt/anaconda3/envs/youngwon/lib/python3.9/site-packages (from plotly->catboost) (2.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_tabnet\n",
        "!pip install tableone\n",
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c4f3f67",
      "metadata": {
        "id": "1c4f3f67"
      },
      "source": [
        "## DataLoad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2ad1b1cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "2ad1b1cb",
        "outputId": "f571a779-9ab6-4998-f1c2-7b6cf45f7c88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3069 entries, 0 to 3068\n",
            "Data columns (total 44 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   날짜             3069 non-null   object \n",
            " 1   요일             3069 non-null   int64  \n",
            " 2   공휴일            3069 non-null   int64  \n",
            " 3   목욕장업           3069 non-null   int64  \n",
            " 4   세탁업            3069 non-null   int64  \n",
            " 5   수영장업           3069 non-null   int64  \n",
            " 6   종합체육시설업        3069 non-null   int64  \n",
            " 7   체력단련장업         3069 non-null   int64  \n",
            " 8   하천             3069 non-null   float64\n",
            " 9   생활인구           3069 non-null   float64\n",
            " 10  불쾌지수(DI)       3069 non-null   float64\n",
            " 11  불쾌지수등급         3069 non-null   int64  \n",
            " 12  일_일강수량(mm)     3069 non-null   float64\n",
            " 13  일_최저기온(°C)     3069 non-null   float64\n",
            " 14  일_평균기온(°C)     3069 non-null   float64\n",
            " 15  일_최고기온(°C)     3069 non-null   float64\n",
            " 16  일_평균풍속(m/s)    3069 non-null   float64\n",
            " 17  일_최대순간풍속(m/s)  3069 non-null   float64\n",
            " 18  최저습도(%)        3069 non-null   float64\n",
            " 19  평균습도(%)        3069 non-null   float64\n",
            " 20  최고습도(%)        3069 non-null   float64\n",
            " 21  습도표준편차         3069 non-null   float64\n",
            " 22  1처리장           3069 non-null   float64\n",
            " 23  2처리장           3069 non-null   float64\n",
            " 24  정화조            3069 non-null   float64\n",
            " 25  중계펌프장          3069 non-null   int64  \n",
            " 26  합계             3069 non-null   float64\n",
            " 27  계절             3069 non-null   int64  \n",
            " 28  월              3069 non-null   int64  \n",
            " 29  강수량_1일전        3069 non-null   float64\n",
            " 30  강수량_2일전        3069 non-null   float64\n",
            " 31  강수량_1일_누적      3069 non-null   float64\n",
            " 32  강수량_2일_누적      3069 non-null   float64\n",
            " 33  강수량_3일_누적      3069 non-null   float64\n",
            " 34  강수량_5일_누적      3069 non-null   float64\n",
            " 35  강수량_7일_누적      3069 non-null   float64\n",
            " 36  일교차            3069 non-null   float64\n",
            " 37  폭우_여부          3069 non-null   int64  \n",
            " 38  체감온도(°C)       3069 non-null   float64\n",
            " 39  등급             3069 non-null   int64  \n",
            " 40  합계_1일후         3069 non-null   float64\n",
            " 41  합계_2일후         3069 non-null   float64\n",
            " 42  등급_1일후         3069 non-null   int64  \n",
            " 43  등급_2일후         3069 non-null   int64  \n",
            "dtypes: float64(28), int64(15), object(1)\n",
            "memory usage: 1.0+ MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "nanji = pd.read_csv('../data/add_feature/nanji_add_feature.csv', encoding='utf-8-sig')\n",
        "\n",
        "nanji.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6ecc0de3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3069 entries, 0 to 3068\n",
            "Data columns (total 44 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   날짜             3069 non-null   object \n",
            " 1   요일             3069 non-null   int64  \n",
            " 2   공휴일            3069 non-null   int64  \n",
            " 3   목욕장업           3069 non-null   int64  \n",
            " 4   세탁업            3069 non-null   int64  \n",
            " 5   수영장업           3069 non-null   int64  \n",
            " 6   종합체육시설업        3069 non-null   int64  \n",
            " 7   체력단련장업         3069 non-null   int64  \n",
            " 8   하천             3069 non-null   float64\n",
            " 9   생활인구           3069 non-null   float64\n",
            " 10  불쾌지수(DI)       3069 non-null   float64\n",
            " 11  불쾌지수등급         3069 non-null   int64  \n",
            " 12  일_일강수량(mm)     3069 non-null   float64\n",
            " 13  일_최저기온(°C)     3069 non-null   float64\n",
            " 14  일_평균기온(°C)     3069 non-null   float64\n",
            " 15  일_최고기온(°C)     3069 non-null   float64\n",
            " 16  일_평균풍속(m/s)    3069 non-null   float64\n",
            " 17  일_최대순간풍속(m/s)  3069 non-null   float64\n",
            " 18  최저습도(%)        3069 non-null   float64\n",
            " 19  평균습도(%)        3069 non-null   float64\n",
            " 20  최고습도(%)        3069 non-null   float64\n",
            " 21  습도표준편차         3069 non-null   float64\n",
            " 22  1처리장           3069 non-null   float64\n",
            " 23  2처리장           3069 non-null   float64\n",
            " 24  정화조            3069 non-null   float64\n",
            " 25  중계펌프장          3069 non-null   int64  \n",
            " 26  합계             3069 non-null   float64\n",
            " 27  계절             3069 non-null   int64  \n",
            " 28  월              3069 non-null   int64  \n",
            " 29  강수량_1일전        3069 non-null   float64\n",
            " 30  강수량_2일전        3069 non-null   float64\n",
            " 31  강수량_1일_누적      3069 non-null   float64\n",
            " 32  강수량_2일_누적      3069 non-null   float64\n",
            " 33  강수량_3일_누적      3069 non-null   float64\n",
            " 34  강수량_5일_누적      3069 non-null   float64\n",
            " 35  강수량_7일_누적      3069 non-null   float64\n",
            " 36  일교차            3069 non-null   float64\n",
            " 37  폭우_여부          3069 non-null   int64  \n",
            " 38  체감온도(°C)       3069 non-null   float64\n",
            " 39  등급             3069 non-null   int64  \n",
            " 40  합계_1일후         3069 non-null   float64\n",
            " 41  합계_2일후         3069 non-null   float64\n",
            " 42  등급_1일후         3069 non-null   int64  \n",
            " 43  등급_2일후         3069 non-null   int64  \n",
            "dtypes: float64(28), int64(15), object(1)\n",
            "memory usage: 1.0+ MB\n"
          ]
        }
      ],
      "source": [
        "nanji.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3a0a4217",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3069, 38)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nanji = nanji.drop(['날짜', '요일','1처리장', '2처리장', '정화조', '중계펌프장'], axis=1)\n",
        "nanji = nanji.dropna()\n",
        "nanji.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "20dff761",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3069 entries, 0 to 3068\n",
            "Data columns (total 38 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   공휴일            3069 non-null   int64  \n",
            " 1   목욕장업           3069 non-null   int64  \n",
            " 2   세탁업            3069 non-null   int64  \n",
            " 3   수영장업           3069 non-null   int64  \n",
            " 4   종합체육시설업        3069 non-null   int64  \n",
            " 5   체력단련장업         3069 non-null   int64  \n",
            " 6   하천             3069 non-null   float64\n",
            " 7   생활인구           3069 non-null   float64\n",
            " 8   불쾌지수(DI)       3069 non-null   float64\n",
            " 9   불쾌지수등급         3069 non-null   int64  \n",
            " 10  일_일강수량(mm)     3069 non-null   float64\n",
            " 11  일_최저기온(°C)     3069 non-null   float64\n",
            " 12  일_평균기온(°C)     3069 non-null   float64\n",
            " 13  일_최고기온(°C)     3069 non-null   float64\n",
            " 14  일_평균풍속(m/s)    3069 non-null   float64\n",
            " 15  일_최대순간풍속(m/s)  3069 non-null   float64\n",
            " 16  최저습도(%)        3069 non-null   float64\n",
            " 17  평균습도(%)        3069 non-null   float64\n",
            " 18  최고습도(%)        3069 non-null   float64\n",
            " 19  습도표준편차         3069 non-null   float64\n",
            " 20  합계             3069 non-null   float64\n",
            " 21  계절             3069 non-null   int64  \n",
            " 22  월              3069 non-null   int64  \n",
            " 23  강수량_1일전        3069 non-null   float64\n",
            " 24  강수량_2일전        3069 non-null   float64\n",
            " 25  강수량_1일_누적      3069 non-null   float64\n",
            " 26  강수량_2일_누적      3069 non-null   float64\n",
            " 27  강수량_3일_누적      3069 non-null   float64\n",
            " 28  강수량_5일_누적      3069 non-null   float64\n",
            " 29  강수량_7일_누적      3069 non-null   float64\n",
            " 30  일교차            3069 non-null   float64\n",
            " 31  폭우_여부          3069 non-null   int64  \n",
            " 32  체감온도(°C)       3069 non-null   float64\n",
            " 33  등급             3069 non-null   int64  \n",
            " 34  합계_1일후         3069 non-null   float64\n",
            " 35  합계_2일후         3069 non-null   float64\n",
            " 36  등급_1일후         3069 non-null   int64  \n",
            " 37  등급_2일후         3069 non-null   int64  \n",
            "dtypes: float64(25), int64(13)\n",
            "memory usage: 911.2 KB\n"
          ]
        }
      ],
      "source": [
        "nanji.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "dabc5e39",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2762, 35) (307, 35) (2762,) (307,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = nanji.drop(['합계','합계_1일후', '합계_2일후'], axis=1)\n",
        "y = nanji['합계']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.1, random_state=42)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a1d193ef",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'from sklearn.metrics import mean_squared_error, r2_score\\nfrom pytorch_tabnet.tab_model import TabNetRegressor\\nimport torch\\n\\n# 6. TabNetRegressor\\ntabnet = TabNetRegressor(\\n    n_d=10, n_a=10, n_steps=5,\\n    gamma=1.5, n_independent=2, n_shared=2,\\n    optimizer_fn=torch.optim.Adam,\\n    optimizer_params=dict(lr=2e-2),\\n    scheduler_params={\"step_size\":20, \"gamma\":0.9},\\n    scheduler_fn=torch.optim.lr_scheduler.StepLR,\\n    mask_type=\\'entmax\\',\\n    verbose=1\\n)\\n\\n# numpy 변환\\nX_train_np = X_train_scaled.astype(np.float32)\\nX_test_np  = X_test_scaled.astype(np.float32)\\ny_train_np = y_train.to_numpy().reshape(-1, 1).astype(np.float32)  \\ny_test_np  = y_test.to_numpy().reshape(-1, 1).astype(np.float32)   \\n\\ntabnet.fit(\\n    X_train=X_train_np, y_train=y_train_np,\\n    eval_set=[(X_test_np, y_test_np)],\\n    eval_metric=[\"rmse\"],\\n    max_epochs=100,\\n    patience=5,\\n    batch_size=32,\\n    virtual_batch_size=16,\\n    num_workers=0,\\n    drop_last=False\\n)\\n\\ny_pred_tabnet = tabnet.predict(X_test_np).squeeze()\\nresults = {}\\nresults[\"TabNet\"] = {\\n    \"model\": tabnet,\\n    \"rmse\": np.sqrt(mean_squared_error(y_test_np, y_pred_tabnet)),\\n    \"r2\": r2_score(y_test_np, y_pred_tabnet),\\n    \"y_pred\": y_pred_tabnet\\n}\\n\\n# 7. 결과 출력\\nfor name, res in results.items():\\n    print(f\"{name}: RMSE={res[\\'rmse\\']:.2f}, R²={res[\\'r2\\']:.4f}\")\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''from sklearn.metrics import mean_squared_error, r2_score\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "import torch\n",
        "\n",
        "# 6. TabNetRegressor\n",
        "tabnet = TabNetRegressor(\n",
        "    n_d=10, n_a=10, n_steps=5,\n",
        "    gamma=1.5, n_independent=2, n_shared=2,\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=2e-2),\n",
        "    scheduler_params={\"step_size\":20, \"gamma\":0.9},\n",
        "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "    mask_type='entmax',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# numpy 변환\n",
        "X_train_np = X_train_scaled.astype(np.float32)\n",
        "X_test_np  = X_test_scaled.astype(np.float32)\n",
        "y_train_np = y_train.to_numpy().reshape(-1, 1).astype(np.float32)  \n",
        "y_test_np  = y_test.to_numpy().reshape(-1, 1).astype(np.float32)   \n",
        "\n",
        "tabnet.fit(\n",
        "    X_train=X_train_np, y_train=y_train_np,\n",
        "    eval_set=[(X_test_np, y_test_np)],\n",
        "    eval_metric=[\"rmse\"],\n",
        "    max_epochs=100,\n",
        "    patience=5,\n",
        "    batch_size=32,\n",
        "    virtual_batch_size=16,\n",
        "    num_workers=0,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "y_pred_tabnet = tabnet.predict(X_test_np).squeeze()\n",
        "results = {}\n",
        "results[\"TabNet\"] = {\n",
        "    \"model\": tabnet,\n",
        "    \"rmse\": np.sqrt(mean_squared_error(y_test_np, y_pred_tabnet)),\n",
        "    \"r2\": r2_score(y_test_np, y_pred_tabnet),\n",
        "    \"y_pred\": y_pred_tabnet\n",
        "}\n",
        "\n",
        "# 7. 결과 출력\n",
        "for name, res in results.items():\n",
        "    print(f\"{name}: RMSE={res['rmse']:.2f}, R²={res['r2']:.4f}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "de58c042",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SPLIT] train=(2762, 36), test=(307, 36)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 3.72787 | val_0_rmse: 1.71091 |  0:00:00s\n",
            "epoch 1  | loss: 2.87877 | val_0_rmse: 1.13076 |  0:00:00s\n",
            "epoch 2  | loss: 2.7399  | val_0_rmse: 0.9914  |  0:00:00s\n",
            "epoch 3  | loss: 2.16086 | val_0_rmse: 0.91343 |  0:00:00s\n",
            "epoch 4  | loss: 1.98216 | val_0_rmse: 0.8408  |  0:00:01s\n",
            "epoch 5  | loss: 1.8122  | val_0_rmse: 0.78208 |  0:00:01s\n",
            "epoch 6  | loss: 1.6484  | val_0_rmse: 0.7629  |  0:00:01s\n",
            "epoch 7  | loss: 1.63355 | val_0_rmse: 0.77719 |  0:00:01s\n",
            "epoch 8  | loss: 1.36637 | val_0_rmse: 0.77305 |  0:00:01s\n",
            "epoch 9  | loss: 1.36384 | val_0_rmse: 0.75019 |  0:00:01s\n",
            "epoch 10 | loss: 1.22564 | val_0_rmse: 0.84804 |  0:00:02s\n",
            "epoch 11 | loss: 1.32498 | val_0_rmse: 0.82138 |  0:00:02s\n",
            "epoch 12 | loss: 1.15306 | val_0_rmse: 0.80141 |  0:00:02s\n",
            "epoch 13 | loss: 1.03739 | val_0_rmse: 0.79615 |  0:00:02s\n",
            "epoch 14 | loss: 1.17733 | val_0_rmse: 0.80729 |  0:00:03s\n",
            "epoch 15 | loss: 1.05653 | val_0_rmse: 0.76614 |  0:00:03s\n",
            "epoch 16 | loss: 1.01742 | val_0_rmse: 0.69229 |  0:00:03s\n",
            "epoch 17 | loss: 1.01122 | val_0_rmse: 0.65964 |  0:00:03s\n",
            "epoch 18 | loss: 0.91012 | val_0_rmse: 0.68602 |  0:00:03s\n",
            "epoch 19 | loss: 0.97099 | val_0_rmse: 0.64157 |  0:00:03s\n",
            "epoch 20 | loss: 0.9638  | val_0_rmse: 0.67845 |  0:00:04s\n",
            "epoch 21 | loss: 0.84367 | val_0_rmse: 0.66911 |  0:00:04s\n",
            "epoch 22 | loss: 0.79472 | val_0_rmse: 0.67447 |  0:00:04s\n",
            "epoch 23 | loss: 0.86896 | val_0_rmse: 0.69305 |  0:00:04s\n",
            "epoch 24 | loss: 0.79465 | val_0_rmse: 0.64906 |  0:00:04s\n",
            "epoch 25 | loss: 0.79266 | val_0_rmse: 0.61635 |  0:00:05s\n",
            "epoch 26 | loss: 0.83306 | val_0_rmse: 0.59082 |  0:00:05s\n",
            "epoch 27 | loss: 0.7619  | val_0_rmse: 0.60001 |  0:00:05s\n",
            "epoch 28 | loss: 0.69466 | val_0_rmse: 0.5989  |  0:00:05s\n",
            "epoch 29 | loss: 0.67621 | val_0_rmse: 0.60132 |  0:00:05s\n",
            "epoch 30 | loss: 0.69889 | val_0_rmse: 0.58369 |  0:00:05s\n",
            "epoch 31 | loss: 0.69277 | val_0_rmse: 0.55255 |  0:00:06s\n",
            "epoch 32 | loss: 0.70118 | val_0_rmse: 0.52684 |  0:00:06s\n",
            "epoch 33 | loss: 0.66082 | val_0_rmse: 0.51707 |  0:00:06s\n",
            "epoch 34 | loss: 0.60994 | val_0_rmse: 0.49185 |  0:00:06s\n",
            "epoch 35 | loss: 0.64702 | val_0_rmse: 0.49283 |  0:00:06s\n",
            "epoch 36 | loss: 0.60725 | val_0_rmse: 0.50921 |  0:00:07s\n",
            "epoch 37 | loss: 0.6279  | val_0_rmse: 0.48685 |  0:00:07s\n",
            "epoch 38 | loss: 0.5961  | val_0_rmse: 0.4769  |  0:00:07s\n",
            "epoch 39 | loss: 0.54557 | val_0_rmse: 0.47169 |  0:00:07s\n",
            "epoch 40 | loss: 0.57907 | val_0_rmse: 0.47388 |  0:00:07s\n",
            "epoch 41 | loss: 0.54643 | val_0_rmse: 0.45576 |  0:00:07s\n",
            "epoch 42 | loss: 0.52963 | val_0_rmse: 0.45731 |  0:00:08s\n",
            "epoch 43 | loss: 0.58618 | val_0_rmse: 0.44089 |  0:00:08s\n",
            "epoch 44 | loss: 0.53515 | val_0_rmse: 0.45978 |  0:00:08s\n",
            "epoch 45 | loss: 0.52856 | val_0_rmse: 0.49434 |  0:00:08s\n",
            "epoch 46 | loss: 0.50043 | val_0_rmse: 0.43004 |  0:00:08s\n",
            "epoch 47 | loss: 0.48    | val_0_rmse: 0.43516 |  0:00:09s\n",
            "epoch 48 | loss: 0.5008  | val_0_rmse: 0.46708 |  0:00:09s\n",
            "epoch 49 | loss: 0.51364 | val_0_rmse: 0.4588  |  0:00:09s\n",
            "epoch 50 | loss: 0.45722 | val_0_rmse: 0.46266 |  0:00:09s\n",
            "epoch 51 | loss: 0.40854 | val_0_rmse: 0.43947 |  0:00:09s\n",
            "epoch 52 | loss: 0.42782 | val_0_rmse: 0.45363 |  0:00:10s\n",
            "epoch 53 | loss: 0.37483 | val_0_rmse: 0.48476 |  0:00:10s\n",
            "epoch 54 | loss: 0.42798 | val_0_rmse: 0.49064 |  0:00:10s\n",
            "epoch 55 | loss: 0.39984 | val_0_rmse: 0.5517  |  0:00:10s\n",
            "epoch 56 | loss: 0.43213 | val_0_rmse: 0.50676 |  0:00:10s\n",
            "epoch 57 | loss: 0.38804 | val_0_rmse: 0.49144 |  0:00:11s\n",
            "epoch 58 | loss: 0.40082 | val_0_rmse: 0.51882 |  0:00:11s\n",
            "epoch 59 | loss: 0.38615 | val_0_rmse: 0.51618 |  0:00:11s\n",
            "epoch 60 | loss: 0.40169 | val_0_rmse: 0.50256 |  0:00:11s\n",
            "epoch 61 | loss: 0.34844 | val_0_rmse: 0.50719 |  0:00:11s\n",
            "epoch 62 | loss: 0.38239 | val_0_rmse: 0.46242 |  0:00:11s\n",
            "epoch 63 | loss: 0.37306 | val_0_rmse: 0.45228 |  0:00:12s\n",
            "epoch 64 | loss: 0.36161 | val_0_rmse: 0.45015 |  0:00:12s\n",
            "epoch 65 | loss: 0.33526 | val_0_rmse: 0.44328 |  0:00:12s\n",
            "epoch 66 | loss: 0.34651 | val_0_rmse: 0.42443 |  0:00:12s\n",
            "epoch 67 | loss: 0.35972 | val_0_rmse: 0.42758 |  0:00:12s\n",
            "epoch 68 | loss: 0.33623 | val_0_rmse: 0.39701 |  0:00:13s\n",
            "epoch 69 | loss: 0.27811 | val_0_rmse: 0.41429 |  0:00:13s\n",
            "epoch 70 | loss: 0.28467 | val_0_rmse: 0.45389 |  0:00:13s\n",
            "epoch 71 | loss: 0.32796 | val_0_rmse: 0.44647 |  0:00:13s\n",
            "epoch 72 | loss: 0.32011 | val_0_rmse: 0.42671 |  0:00:13s\n",
            "epoch 73 | loss: 0.31458 | val_0_rmse: 0.42752 |  0:00:13s\n",
            "epoch 74 | loss: 0.27997 | val_0_rmse: 0.43809 |  0:00:14s\n",
            "epoch 75 | loss: 0.24078 | val_0_rmse: 0.45001 |  0:00:14s\n",
            "epoch 76 | loss: 0.26288 | val_0_rmse: 0.47185 |  0:00:14s\n",
            "epoch 77 | loss: 0.26118 | val_0_rmse: 0.45471 |  0:00:14s\n",
            "epoch 78 | loss: 0.26096 | val_0_rmse: 0.53344 |  0:00:15s\n",
            "epoch 79 | loss: 0.24721 | val_0_rmse: 0.46774 |  0:00:15s\n",
            "epoch 80 | loss: 0.28347 | val_0_rmse: 0.53088 |  0:00:15s\n",
            "epoch 81 | loss: 0.25784 | val_0_rmse: 0.43584 |  0:00:15s\n",
            "epoch 82 | loss: 0.26807 | val_0_rmse: 0.48396 |  0:00:15s\n",
            "epoch 83 | loss: 0.27831 | val_0_rmse: 0.55709 |  0:00:16s\n",
            "epoch 84 | loss: 0.24719 | val_0_rmse: 0.50777 |  0:00:16s\n",
            "epoch 85 | loss: 0.26217 | val_0_rmse: 0.41132 |  0:00:16s\n",
            "epoch 86 | loss: 0.24297 | val_0_rmse: 0.40665 |  0:00:16s\n",
            "epoch 87 | loss: 0.2394  | val_0_rmse: 0.38821 |  0:00:16s\n",
            "epoch 88 | loss: 0.24636 | val_0_rmse: 0.38468 |  0:00:17s\n",
            "epoch 89 | loss: 0.2323  | val_0_rmse: 0.40091 |  0:00:17s\n",
            "epoch 90 | loss: 0.23169 | val_0_rmse: 0.40348 |  0:00:17s\n",
            "epoch 91 | loss: 0.2375  | val_0_rmse: 0.38982 |  0:00:17s\n",
            "epoch 92 | loss: 0.22742 | val_0_rmse: 0.39138 |  0:00:17s\n",
            "epoch 93 | loss: 0.24168 | val_0_rmse: 0.37037 |  0:00:18s\n",
            "epoch 94 | loss: 0.21473 | val_0_rmse: 0.35556 |  0:00:18s\n",
            "epoch 95 | loss: 0.21003 | val_0_rmse: 0.36576 |  0:00:18s\n",
            "epoch 96 | loss: 0.21996 | val_0_rmse: 0.40215 |  0:00:18s\n",
            "epoch 97 | loss: 0.2094  | val_0_rmse: 0.36967 |  0:00:18s\n",
            "epoch 98 | loss: 0.20478 | val_0_rmse: 0.39194 |  0:00:19s\n",
            "epoch 99 | loss: 0.21617 | val_0_rmse: 0.36698 |  0:00:19s\n",
            "epoch 100| loss: 0.20896 | val_0_rmse: 0.40497 |  0:00:19s\n",
            "epoch 101| loss: 0.20385 | val_0_rmse: 0.35776 |  0:00:19s\n",
            "epoch 102| loss: 0.20509 | val_0_rmse: 0.37679 |  0:00:19s\n",
            "epoch 103| loss: 0.19763 | val_0_rmse: 0.39762 |  0:00:20s\n",
            "epoch 104| loss: 0.18724 | val_0_rmse: 0.38355 |  0:00:20s\n",
            "epoch 105| loss: 0.18233 | val_0_rmse: 0.41198 |  0:00:20s\n",
            "epoch 106| loss: 0.19798 | val_0_rmse: 0.37755 |  0:00:20s\n",
            "epoch 107| loss: 0.1937  | val_0_rmse: 0.38038 |  0:00:20s\n",
            "epoch 108| loss: 0.17799 | val_0_rmse: 0.41463 |  0:00:21s\n",
            "epoch 109| loss: 0.19515 | val_0_rmse: 0.38021 |  0:00:21s\n",
            "epoch 110| loss: 0.17015 | val_0_rmse: 0.38307 |  0:00:21s\n",
            "epoch 111| loss: 0.17754 | val_0_rmse: 0.36951 |  0:00:21s\n",
            "epoch 112| loss: 0.18859 | val_0_rmse: 0.36966 |  0:00:21s\n",
            "epoch 113| loss: 0.18273 | val_0_rmse: 0.36246 |  0:00:21s\n",
            "epoch 114| loss: 0.16972 | val_0_rmse: 0.36518 |  0:00:22s\n",
            "epoch 115| loss: 0.15994 | val_0_rmse: 0.3709  |  0:00:22s\n",
            "epoch 116| loss: 0.17643 | val_0_rmse: 0.38563 |  0:00:22s\n",
            "epoch 117| loss: 0.16782 | val_0_rmse: 0.35137 |  0:00:22s\n",
            "epoch 118| loss: 0.16351 | val_0_rmse: 0.36114 |  0:00:22s\n",
            "epoch 119| loss: 0.16074 | val_0_rmse: 0.34661 |  0:00:23s\n",
            "epoch 120| loss: 0.16856 | val_0_rmse: 0.37108 |  0:00:23s\n",
            "epoch 121| loss: 0.18484 | val_0_rmse: 0.35601 |  0:00:23s\n",
            "epoch 122| loss: 0.14872 | val_0_rmse: 0.34519 |  0:00:23s\n",
            "epoch 123| loss: 0.15845 | val_0_rmse: 0.33887 |  0:00:23s\n",
            "epoch 124| loss: 0.16257 | val_0_rmse: 0.34165 |  0:00:24s\n",
            "epoch 125| loss: 0.15328 | val_0_rmse: 0.34494 |  0:00:24s\n",
            "epoch 126| loss: 0.17171 | val_0_rmse: 0.3306  |  0:00:24s\n",
            "epoch 127| loss: 0.16125 | val_0_rmse: 0.33696 |  0:00:24s\n",
            "epoch 128| loss: 0.15562 | val_0_rmse: 0.33432 |  0:00:24s\n",
            "epoch 129| loss: 0.16548 | val_0_rmse: 0.34177 |  0:00:25s\n",
            "epoch 130| loss: 0.15194 | val_0_rmse: 0.35141 |  0:00:25s\n",
            "epoch 131| loss: 0.14685 | val_0_rmse: 0.3491  |  0:00:25s\n",
            "epoch 132| loss: 0.15083 | val_0_rmse: 0.35076 |  0:00:25s\n",
            "epoch 133| loss: 0.1501  | val_0_rmse: 0.36906 |  0:00:25s\n",
            "epoch 134| loss: 0.15205 | val_0_rmse: 0.33193 |  0:00:25s\n",
            "epoch 135| loss: 0.14396 | val_0_rmse: 0.33541 |  0:00:26s\n",
            "epoch 136| loss: 0.1497  | val_0_rmse: 0.36018 |  0:00:26s\n",
            "epoch 137| loss: 0.15114 | val_0_rmse: 0.3697  |  0:00:26s\n",
            "epoch 138| loss: 0.15255 | val_0_rmse: 0.35097 |  0:00:26s\n",
            "epoch 139| loss: 0.13879 | val_0_rmse: 0.36853 |  0:00:26s\n",
            "epoch 140| loss: 0.13305 | val_0_rmse: 0.36763 |  0:00:27s\n",
            "epoch 141| loss: 0.14525 | val_0_rmse: 0.35041 |  0:00:27s\n",
            "epoch 142| loss: 0.15621 | val_0_rmse: 0.35313 |  0:00:27s\n",
            "epoch 143| loss: 0.15056 | val_0_rmse: 0.39501 |  0:00:27s\n",
            "epoch 144| loss: 0.15605 | val_0_rmse: 0.36683 |  0:00:27s\n",
            "epoch 145| loss: 0.13946 | val_0_rmse: 0.35198 |  0:00:27s\n",
            "epoch 146| loss: 0.14505 | val_0_rmse: 0.36712 |  0:00:28s\n",
            "epoch 147| loss: 0.14077 | val_0_rmse: 0.35597 |  0:00:28s\n",
            "epoch 148| loss: 0.13822 | val_0_rmse: 0.3672  |  0:00:28s\n",
            "epoch 149| loss: 0.13863 | val_0_rmse: 0.37615 |  0:00:28s\n",
            "epoch 150| loss: 0.13761 | val_0_rmse: 0.37256 |  0:00:28s\n",
            "epoch 151| loss: 0.13996 | val_0_rmse: 0.37476 |  0:00:29s\n",
            "epoch 152| loss: 0.13797 | val_0_rmse: 0.3634  |  0:00:29s\n",
            "epoch 153| loss: 0.14699 | val_0_rmse: 0.37112 |  0:00:29s\n",
            "epoch 154| loss: 0.13712 | val_0_rmse: 0.35949 |  0:00:29s\n",
            "epoch 155| loss: 0.13668 | val_0_rmse: 0.38181 |  0:00:29s\n",
            "epoch 156| loss: 0.13334 | val_0_rmse: 0.36294 |  0:00:29s\n",
            "epoch 157| loss: 0.12713 | val_0_rmse: 0.35202 |  0:00:30s\n",
            "epoch 158| loss: 0.13569 | val_0_rmse: 0.3745  |  0:00:30s\n",
            "epoch 159| loss: 0.14053 | val_0_rmse: 0.40685 |  0:00:30s\n",
            "epoch 160| loss: 0.12738 | val_0_rmse: 0.36699 |  0:00:30s\n",
            "epoch 161| loss: 0.13329 | val_0_rmse: 0.36272 |  0:00:30s\n",
            "epoch 162| loss: 0.13304 | val_0_rmse: 0.35601 |  0:00:31s\n",
            "epoch 163| loss: 0.12151 | val_0_rmse: 0.35226 |  0:00:31s\n",
            "epoch 164| loss: 0.1245  | val_0_rmse: 0.36639 |  0:00:31s\n",
            "epoch 165| loss: 0.12579 | val_0_rmse: 0.36333 |  0:00:31s\n",
            "epoch 166| loss: 0.12534 | val_0_rmse: 0.34551 |  0:00:31s\n",
            "epoch 167| loss: 0.12959 | val_0_rmse: 0.36106 |  0:00:32s\n",
            "epoch 168| loss: 0.12795 | val_0_rmse: 0.38293 |  0:00:32s\n",
            "epoch 169| loss: 0.12158 | val_0_rmse: 0.37231 |  0:00:32s\n",
            "epoch 170| loss: 0.13532 | val_0_rmse: 0.37026 |  0:00:32s\n",
            "epoch 171| loss: 0.12369 | val_0_rmse: 0.37054 |  0:00:32s\n",
            "epoch 172| loss: 0.1242  | val_0_rmse: 0.36078 |  0:00:33s\n",
            "epoch 173| loss: 0.12269 | val_0_rmse: 0.36286 |  0:00:33s\n",
            "epoch 174| loss: 0.12767 | val_0_rmse: 0.36831 |  0:00:33s\n",
            "epoch 175| loss: 0.11329 | val_0_rmse: 0.37324 |  0:00:33s\n",
            "epoch 176| loss: 0.12153 | val_0_rmse: 0.37409 |  0:00:33s\n",
            "\n",
            "Early stopping occurred at epoch 176 with best_epoch = 126 and best_val_0_rmse = 0.3306\n",
            "TabNet training time: 33.9s\n",
            "[TabNet] MAE=28354.568  RMSE=45160.241  R²=0.7713\n",
            "[Naive-1]   RMSE=76368.137  R²=0.3460\n",
            "[Week-Naive]RMSE=98098.966  R²=-0.0791\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math, time\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "import torch\n",
        "\n",
        "# 1) 데이터 분리 \n",
        "TARGET = '합계_1일후'\n",
        "X = nanji.drop(columns=[c for c in ['합계_1일후','합계_2일후'] if c in nanji.columns]).copy()\n",
        "y = nanji[TARGET].copy()\n",
        "\n",
        "n = len(X)\n",
        "test_n = int(math.ceil(n * 0.10))\n",
        "train_n = n - test_n\n",
        "\n",
        "X_train, X_test = X.iloc[:train_n, :].copy(), X.iloc[train_n:, :].copy()\n",
        "y_train, y_test = y.iloc[:train_n].copy(), y.iloc[train_n:].copy()\n",
        "\n",
        "print(f\"[SPLIT] train={X_train.shape}, test={X_test.shape}\")\n",
        "\n",
        "# 2) 스케일링 (누수 방지: train으로만 fit)\n",
        "x_scaler = StandardScaler()\n",
        "X_train_s = x_scaler.fit_transform(X_train)\n",
        "X_test_s  = x_scaler.transform(X_test)\n",
        "\n",
        "y_scaler = StandardScaler()\n",
        "y_train_s = y_scaler.fit_transform(y_train.values.reshape(-1,1)).astype(np.float32)  # (n,1)\n",
        "y_test_s  = y_scaler.transform(y_test.values.reshape(-1,1)).astype(np.float32)      # (m,1)\n",
        "\n",
        "# TabNet 입력 dtype/shape\n",
        "X_train_np = X_train_s.astype(np.float32)\n",
        "X_test_np  = X_test_s.astype(np.float32)\n",
        "\n",
        "# 3) TabNet 설정\n",
        "tabnet = TabNetRegressor(\n",
        "    n_d=32, n_a=32, n_steps=4,\n",
        "    gamma=1, n_independent=2, n_shared=2,\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=1e-3, weight_decay=1e-5),\n",
        "    scheduler_fn=None,      # 우선 스케줄러 OFF로 수렴 확인\n",
        "    mask_type='entmax',                # 엔트맥스가 일반적으로 안정적\n",
        "    verbose=1,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "t0 = time.time()\n",
        "tabnet.fit(\n",
        "    X_train=X_train_np,\n",
        "    y_train=y_train_s,                    # <-- (n,1) 2D\n",
        "    eval_set=[(X_test_np, y_test_s)],     # <-- (m,1) 2D\n",
        "    eval_metric=[\"rmse\"],                 # 표준화된 y 기준 RMSE\n",
        "    max_epochs=500,\n",
        "    patience=50,\n",
        "    batch_size=512,\n",
        "    virtual_batch_size=128,\n",
        "    num_workers=0,\n",
        "    drop_last=False\n",
        ")\n",
        "print(f\"TabNet training time: {time.time()-t0:.1f}s\")\n",
        "\n",
        "# 4) 예측 & 역변환 평가 \n",
        "y_pred_s = tabnet.predict(X_test_np).squeeze()                 # (m,)\n",
        "y_pred = y_scaler.inverse_transform(y_pred_s.reshape(-1,1)).ravel()\n",
        "\n",
        "y_true = y_test.values\n",
        "mse  = mean_squared_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mse)   # 버전 독립 RMSE\n",
        "mae  = mean_absolute_error(y_true, y_pred)\n",
        "r2   = r2_score(y_true, y_pred)\n",
        "\n",
        "print(f\"[TabNet] MAE={mae:.3f}  RMSE={rmse:.3f}  R²={r2:.4f}\")\n",
        "\n",
        "# 5) 기준선과 비교 (원 스케일)\n",
        "y_test_arr = y_true\n",
        "# 어제값(naive-1): 테스트 내 시프트로 근사 비교 (첫 값 보정)\n",
        "y_na1 = np.roll(y_test_arr, 1); y_na1[0] = y_test_arr[0]\n",
        "# 7일 전(week naive): 주간 패턴 근사\n",
        "y_wk  = np.roll(y_test_arr, 7); y_wk[:7] = y_test_arr[:7]\n",
        "\n",
        "rmse_na1 = np.sqrt(mean_squared_error(y_test_arr, y_na1)); r2_na1 = r2_score(y_test_arr, y_na1)\n",
        "rmse_wk  = np.sqrt(mean_squared_error(y_test_arr, y_wk));  r2_wk  = r2_score(y_test_arr, y_wk)\n",
        "\n",
        "print(f\"[Naive-1]   RMSE={rmse_na1:.3f}  R²={r2_na1:.4f}\")\n",
        "print(f\"[Week-Naive]RMSE={rmse_wk:.3f}  R²={r2_wk:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6a7e5ae5",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SPLIT] train=(2762, 35), test=(307, 35)\n",
            "epoch 0  | loss: 27.2071 | val_0_rmse: 3.29285 |  0:00:00s\n",
            "epoch 1  | loss: 20.58113| val_0_rmse: 2.32534 |  0:00:00s\n",
            "epoch 2  | loss: 16.00532| val_0_rmse: 1.99438 |  0:00:00s\n",
            "epoch 3  | loss: 13.25639| val_0_rmse: 1.81177 |  0:00:00s\n",
            "epoch 4  | loss: 10.63526| val_0_rmse: 1.79225 |  0:00:01s\n",
            "epoch 5  | loss: 8.2686  | val_0_rmse: 1.79257 |  0:00:01s\n",
            "epoch 6  | loss: 5.85719 | val_0_rmse: 1.67333 |  0:00:01s\n",
            "epoch 7  | loss: 4.92261 | val_0_rmse: 1.59967 |  0:00:01s\n",
            "epoch 8  | loss: 4.16316 | val_0_rmse: 1.49481 |  0:00:01s\n",
            "epoch 9  | loss: 3.15378 | val_0_rmse: 1.33846 |  0:00:01s\n",
            "epoch 10 | loss: 2.57029 | val_0_rmse: 1.15788 |  0:00:02s\n",
            "epoch 11 | loss: 2.08844 | val_0_rmse: 1.07138 |  0:00:02s\n",
            "epoch 12 | loss: 1.78875 | val_0_rmse: 1.06977 |  0:00:02s\n",
            "epoch 13 | loss: 1.7443  | val_0_rmse: 1.03803 |  0:00:02s\n",
            "epoch 14 | loss: 1.58353 | val_0_rmse: 0.95351 |  0:00:02s\n",
            "epoch 15 | loss: 1.31013 | val_0_rmse: 1.00816 |  0:00:03s\n",
            "epoch 16 | loss: 1.38992 | val_0_rmse: 0.89794 |  0:00:03s\n",
            "epoch 17 | loss: 1.28873 | val_0_rmse: 0.85731 |  0:00:03s\n",
            "epoch 18 | loss: 1.28524 | val_0_rmse: 0.80351 |  0:00:03s\n",
            "epoch 19 | loss: 1.19065 | val_0_rmse: 0.80097 |  0:00:03s\n",
            "epoch 20 | loss: 1.10833 | val_0_rmse: 0.84696 |  0:00:04s\n",
            "epoch 21 | loss: 1.20957 | val_0_rmse: 0.87903 |  0:00:04s\n",
            "epoch 22 | loss: 1.05506 | val_0_rmse: 0.88442 |  0:00:04s\n",
            "epoch 23 | loss: 0.98013 | val_0_rmse: 0.89705 |  0:00:04s\n",
            "epoch 24 | loss: 0.91244 | val_0_rmse: 0.90025 |  0:00:04s\n",
            "epoch 25 | loss: 0.92899 | val_0_rmse: 0.90355 |  0:00:05s\n",
            "epoch 26 | loss: 0.96616 | val_0_rmse: 0.87231 |  0:00:05s\n",
            "epoch 27 | loss: 0.90486 | val_0_rmse: 0.77697 |  0:00:05s\n",
            "epoch 28 | loss: 0.88222 | val_0_rmse: 0.758   |  0:00:05s\n",
            "epoch 29 | loss: 0.81717 | val_0_rmse: 0.74334 |  0:00:05s\n",
            "epoch 30 | loss: 0.75066 | val_0_rmse: 0.75065 |  0:00:05s\n",
            "epoch 31 | loss: 0.74038 | val_0_rmse: 0.73853 |  0:00:06s\n",
            "epoch 32 | loss: 0.74608 | val_0_rmse: 0.74797 |  0:00:06s\n",
            "epoch 33 | loss: 0.73992 | val_0_rmse: 0.70035 |  0:00:06s\n",
            "epoch 34 | loss: 0.69411 | val_0_rmse: 0.68644 |  0:00:06s\n",
            "epoch 35 | loss: 0.72891 | val_0_rmse: 0.69684 |  0:00:06s\n",
            "epoch 36 | loss: 0.65868 | val_0_rmse: 0.70058 |  0:00:07s\n",
            "epoch 37 | loss: 0.63458 | val_0_rmse: 0.6886  |  0:00:07s\n",
            "epoch 38 | loss: 0.63356 | val_0_rmse: 0.69444 |  0:00:07s\n",
            "epoch 39 | loss: 0.57935 | val_0_rmse: 0.72047 |  0:00:07s\n",
            "epoch 40 | loss: 0.52627 | val_0_rmse: 0.70646 |  0:00:07s\n",
            "epoch 41 | loss: 0.56186 | val_0_rmse: 0.73022 |  0:00:08s\n",
            "epoch 42 | loss: 0.54478 | val_0_rmse: 0.71505 |  0:00:08s\n",
            "epoch 43 | loss: 0.45696 | val_0_rmse: 0.64423 |  0:00:08s\n",
            "epoch 44 | loss: 0.47716 | val_0_rmse: 0.64925 |  0:00:08s\n",
            "epoch 45 | loss: 0.48134 | val_0_rmse: 0.59469 |  0:00:08s\n",
            "epoch 46 | loss: 0.52446 | val_0_rmse: 0.61337 |  0:00:08s\n",
            "epoch 47 | loss: 0.49048 | val_0_rmse: 0.56357 |  0:00:09s\n",
            "epoch 48 | loss: 0.49787 | val_0_rmse: 0.60291 |  0:00:09s\n",
            "epoch 49 | loss: 0.45014 | val_0_rmse: 0.56854 |  0:00:09s\n",
            "epoch 50 | loss: 0.44282 | val_0_rmse: 0.57018 |  0:00:09s\n",
            "epoch 51 | loss: 0.40141 | val_0_rmse: 0.51698 |  0:00:09s\n",
            "epoch 52 | loss: 0.40299 | val_0_rmse: 0.52661 |  0:00:10s\n",
            "epoch 53 | loss: 0.44628 | val_0_rmse: 0.50949 |  0:00:10s\n",
            "epoch 54 | loss: 0.41464 | val_0_rmse: 0.51953 |  0:00:10s\n",
            "epoch 55 | loss: 0.42357 | val_0_rmse: 0.5057  |  0:00:10s\n",
            "epoch 56 | loss: 0.42449 | val_0_rmse: 0.49394 |  0:00:10s\n",
            "epoch 57 | loss: 0.40032 | val_0_rmse: 0.49836 |  0:00:10s\n",
            "epoch 58 | loss: 0.38674 | val_0_rmse: 0.50709 |  0:00:11s\n",
            "epoch 59 | loss: 0.3667  | val_0_rmse: 0.5212  |  0:00:11s\n",
            "epoch 60 | loss: 0.41209 | val_0_rmse: 0.46685 |  0:00:11s\n",
            "epoch 61 | loss: 0.3812  | val_0_rmse: 0.48429 |  0:00:11s\n",
            "epoch 62 | loss: 0.34052 | val_0_rmse: 0.489   |  0:00:11s\n",
            "epoch 63 | loss: 0.34626 | val_0_rmse: 0.45738 |  0:00:12s\n",
            "epoch 64 | loss: 0.37827 | val_0_rmse: 0.44254 |  0:00:12s\n",
            "epoch 65 | loss: 0.36083 | val_0_rmse: 0.45003 |  0:00:12s\n",
            "epoch 66 | loss: 0.35698 | val_0_rmse: 0.43344 |  0:00:12s\n",
            "epoch 67 | loss: 0.32699 | val_0_rmse: 0.45081 |  0:00:12s\n",
            "epoch 68 | loss: 0.35669 | val_0_rmse: 0.45971 |  0:00:12s\n",
            "epoch 69 | loss: 0.3347  | val_0_rmse: 0.46174 |  0:00:13s\n",
            "epoch 70 | loss: 0.30275 | val_0_rmse: 0.4698  |  0:00:13s\n",
            "epoch 71 | loss: 0.30818 | val_0_rmse: 0.4527  |  0:00:13s\n",
            "epoch 72 | loss: 0.30524 | val_0_rmse: 0.42532 |  0:00:13s\n",
            "epoch 73 | loss: 0.31739 | val_0_rmse: 0.41881 |  0:00:13s\n",
            "epoch 74 | loss: 0.28979 | val_0_rmse: 0.41186 |  0:00:14s\n",
            "epoch 75 | loss: 0.29675 | val_0_rmse: 0.40932 |  0:00:14s\n",
            "epoch 76 | loss: 0.32519 | val_0_rmse: 0.41522 |  0:00:14s\n",
            "epoch 77 | loss: 0.28813 | val_0_rmse: 0.39842 |  0:00:14s\n",
            "epoch 78 | loss: 0.28813 | val_0_rmse: 0.40157 |  0:00:14s\n",
            "epoch 79 | loss: 0.28964 | val_0_rmse: 0.3882  |  0:00:14s\n",
            "epoch 80 | loss: 0.29063 | val_0_rmse: 0.396   |  0:00:15s\n",
            "epoch 81 | loss: 0.299   | val_0_rmse: 0.39638 |  0:00:15s\n",
            "epoch 82 | loss: 0.27045 | val_0_rmse: 0.40492 |  0:00:15s\n",
            "epoch 83 | loss: 0.27204 | val_0_rmse: 0.39689 |  0:00:15s\n",
            "epoch 84 | loss: 0.26758 | val_0_rmse: 0.40612 |  0:00:15s\n",
            "epoch 85 | loss: 0.28372 | val_0_rmse: 0.38217 |  0:00:16s\n",
            "epoch 86 | loss: 0.26963 | val_0_rmse: 0.37876 |  0:00:16s\n",
            "epoch 87 | loss: 0.26208 | val_0_rmse: 0.38405 |  0:00:16s\n",
            "epoch 88 | loss: 0.27926 | val_0_rmse: 0.37122 |  0:00:16s\n",
            "epoch 89 | loss: 0.24914 | val_0_rmse: 0.36494 |  0:00:16s\n",
            "epoch 90 | loss: 0.25755 | val_0_rmse: 0.3557  |  0:00:17s\n",
            "epoch 91 | loss: 0.25423 | val_0_rmse: 0.36131 |  0:00:17s\n",
            "epoch 92 | loss: 0.25615 | val_0_rmse: 0.35111 |  0:00:17s\n",
            "epoch 93 | loss: 0.24562 | val_0_rmse: 0.37238 |  0:00:17s\n",
            "epoch 94 | loss: 0.22945 | val_0_rmse: 0.37812 |  0:00:17s\n",
            "epoch 95 | loss: 0.24462 | val_0_rmse: 0.37516 |  0:00:17s\n",
            "epoch 96 | loss: 0.2439  | val_0_rmse: 0.38121 |  0:00:18s\n",
            "epoch 97 | loss: 0.24537 | val_0_rmse: 0.39938 |  0:00:18s\n",
            "epoch 98 | loss: 0.21834 | val_0_rmse: 0.40671 |  0:00:18s\n",
            "epoch 99 | loss: 0.23267 | val_0_rmse: 0.38679 |  0:00:18s\n",
            "epoch 100| loss: 0.24021 | val_0_rmse: 0.38638 |  0:00:18s\n",
            "epoch 101| loss: 0.22457 | val_0_rmse: 0.38792 |  0:00:19s\n",
            "epoch 102| loss: 0.23129 | val_0_rmse: 0.36931 |  0:00:19s\n",
            "epoch 103| loss: 0.22927 | val_0_rmse: 0.36541 |  0:00:19s\n",
            "epoch 104| loss: 0.25236 | val_0_rmse: 0.36499 |  0:00:19s\n",
            "epoch 105| loss: 0.24394 | val_0_rmse: 0.3591  |  0:00:19s\n",
            "epoch 106| loss: 0.23261 | val_0_rmse: 0.35843 |  0:00:19s\n",
            "epoch 107| loss: 0.249   | val_0_rmse: 0.36291 |  0:00:20s\n",
            "epoch 108| loss: 0.22545 | val_0_rmse: 0.34026 |  0:00:20s\n",
            "epoch 109| loss: 0.24628 | val_0_rmse: 0.32709 |  0:00:20s\n",
            "epoch 110| loss: 0.22501 | val_0_rmse: 0.33073 |  0:00:20s\n",
            "epoch 111| loss: 0.2283  | val_0_rmse: 0.34196 |  0:00:20s\n",
            "epoch 112| loss: 0.21634 | val_0_rmse: 0.35945 |  0:00:21s\n",
            "epoch 113| loss: 0.22714 | val_0_rmse: 0.36124 |  0:00:21s\n",
            "epoch 114| loss: 0.22774 | val_0_rmse: 0.3848  |  0:00:21s\n",
            "epoch 115| loss: 0.23319 | val_0_rmse: 0.35223 |  0:00:21s\n",
            "epoch 116| loss: 0.21265 | val_0_rmse: 0.33856 |  0:00:21s\n",
            "epoch 117| loss: 0.21096 | val_0_rmse: 0.34662 |  0:00:22s\n",
            "epoch 118| loss: 0.19971 | val_0_rmse: 0.35995 |  0:00:22s\n",
            "epoch 119| loss: 0.21051 | val_0_rmse: 0.36788 |  0:00:22s\n",
            "epoch 120| loss: 0.20732 | val_0_rmse: 0.36052 |  0:00:22s\n",
            "epoch 121| loss: 0.21239 | val_0_rmse: 0.35217 |  0:00:22s\n",
            "epoch 122| loss: 0.20521 | val_0_rmse: 0.35007 |  0:00:23s\n",
            "epoch 123| loss: 0.18612 | val_0_rmse: 0.3486  |  0:00:23s\n",
            "epoch 124| loss: 0.20724 | val_0_rmse: 0.3497  |  0:00:23s\n",
            "epoch 125| loss: 0.19492 | val_0_rmse: 0.35349 |  0:00:23s\n",
            "epoch 126| loss: 0.18534 | val_0_rmse: 0.36135 |  0:00:23s\n",
            "epoch 127| loss: 0.18913 | val_0_rmse: 0.37152 |  0:00:24s\n",
            "epoch 128| loss: 0.17918 | val_0_rmse: 0.35186 |  0:00:24s\n",
            "epoch 129| loss: 0.17928 | val_0_rmse: 0.32717 |  0:00:24s\n",
            "epoch 130| loss: 0.19644 | val_0_rmse: 0.33513 |  0:00:24s\n",
            "epoch 131| loss: 0.19037 | val_0_rmse: 0.31905 |  0:00:24s\n",
            "epoch 132| loss: 0.19047 | val_0_rmse: 0.3206  |  0:00:25s\n",
            "epoch 133| loss: 0.2049  | val_0_rmse: 0.33716 |  0:00:25s\n",
            "epoch 134| loss: 0.17951 | val_0_rmse: 0.34446 |  0:00:25s\n",
            "epoch 135| loss: 0.18056 | val_0_rmse: 0.36694 |  0:00:25s\n",
            "epoch 136| loss: 0.19428 | val_0_rmse: 0.37112 |  0:00:25s\n",
            "epoch 137| loss: 0.17451 | val_0_rmse: 0.35514 |  0:00:26s\n",
            "epoch 138| loss: 0.18011 | val_0_rmse: 0.33586 |  0:00:26s\n",
            "epoch 139| loss: 0.17827 | val_0_rmse: 0.34434 |  0:00:26s\n",
            "epoch 140| loss: 0.20313 | val_0_rmse: 0.35765 |  0:00:26s\n",
            "epoch 141| loss: 0.17883 | val_0_rmse: 0.35018 |  0:00:26s\n",
            "epoch 142| loss: 0.18027 | val_0_rmse: 0.34417 |  0:00:27s\n",
            "epoch 143| loss: 0.18809 | val_0_rmse: 0.33868 |  0:00:27s\n",
            "epoch 144| loss: 0.1691  | val_0_rmse: 0.32201 |  0:00:27s\n",
            "epoch 145| loss: 0.17408 | val_0_rmse: 0.32372 |  0:00:27s\n",
            "epoch 146| loss: 0.1823  | val_0_rmse: 0.30356 |  0:00:27s\n",
            "epoch 147| loss: 0.16819 | val_0_rmse: 0.31931 |  0:00:28s\n",
            "epoch 148| loss: 0.17689 | val_0_rmse: 0.30514 |  0:00:28s\n",
            "epoch 149| loss: 0.16579 | val_0_rmse: 0.31184 |  0:00:28s\n",
            "epoch 150| loss: 0.18307 | val_0_rmse: 0.30457 |  0:00:28s\n",
            "epoch 151| loss: 0.17308 | val_0_rmse: 0.30806 |  0:00:28s\n",
            "epoch 152| loss: 0.15434 | val_0_rmse: 0.30348 |  0:00:29s\n",
            "epoch 153| loss: 0.17361 | val_0_rmse: 0.31283 |  0:00:29s\n",
            "epoch 154| loss: 0.17164 | val_0_rmse: 0.30461 |  0:00:29s\n",
            "epoch 155| loss: 0.1677  | val_0_rmse: 0.31066 |  0:00:29s\n",
            "epoch 156| loss: 0.16773 | val_0_rmse: 0.30332 |  0:00:29s\n",
            "epoch 157| loss: 0.18441 | val_0_rmse: 0.30675 |  0:00:30s\n",
            "epoch 158| loss: 0.17414 | val_0_rmse: 0.32467 |  0:00:30s\n",
            "epoch 159| loss: 0.17133 | val_0_rmse: 0.33664 |  0:00:30s\n",
            "epoch 160| loss: 0.16858 | val_0_rmse: 0.32057 |  0:00:30s\n",
            "epoch 161| loss: 0.17202 | val_0_rmse: 0.31641 |  0:00:31s\n",
            "epoch 162| loss: 0.16879 | val_0_rmse: 0.2882  |  0:00:31s\n",
            "epoch 163| loss: 0.15575 | val_0_rmse: 0.30088 |  0:00:31s\n",
            "epoch 164| loss: 0.18232 | val_0_rmse: 0.31464 |  0:00:31s\n",
            "epoch 165| loss: 0.16543 | val_0_rmse: 0.30436 |  0:00:31s\n",
            "epoch 166| loss: 0.17455 | val_0_rmse: 0.30645 |  0:00:32s\n",
            "epoch 167| loss: 0.16697 | val_0_rmse: 0.33771 |  0:00:32s\n",
            "epoch 168| loss: 0.16142 | val_0_rmse: 0.34354 |  0:00:32s\n",
            "epoch 169| loss: 0.18372 | val_0_rmse: 0.33923 |  0:00:32s\n",
            "epoch 170| loss: 0.17854 | val_0_rmse: 0.33266 |  0:00:32s\n",
            "epoch 171| loss: 0.16222 | val_0_rmse: 0.3292  |  0:00:33s\n",
            "epoch 172| loss: 0.16028 | val_0_rmse: 0.34642 |  0:00:33s\n",
            "epoch 173| loss: 0.16002 | val_0_rmse: 0.311   |  0:00:33s\n",
            "epoch 174| loss: 0.17303 | val_0_rmse: 0.2833  |  0:00:33s\n",
            "epoch 175| loss: 0.13787 | val_0_rmse: 0.28349 |  0:00:33s\n",
            "epoch 176| loss: 0.15249 | val_0_rmse: 0.27443 |  0:00:34s\n",
            "epoch 177| loss: 0.17134 | val_0_rmse: 0.27693 |  0:00:34s\n",
            "epoch 178| loss: 0.15147 | val_0_rmse: 0.28527 |  0:00:34s\n",
            "epoch 179| loss: 0.15131 | val_0_rmse: 0.28102 |  0:00:34s\n",
            "epoch 180| loss: 0.15179 | val_0_rmse: 0.28237 |  0:00:34s\n",
            "epoch 181| loss: 0.15749 | val_0_rmse: 0.29073 |  0:00:35s\n",
            "epoch 182| loss: 0.16432 | val_0_rmse: 0.29943 |  0:00:35s\n",
            "epoch 183| loss: 0.16168 | val_0_rmse: 0.30662 |  0:00:35s\n",
            "epoch 184| loss: 0.16242 | val_0_rmse: 0.3031  |  0:00:35s\n",
            "epoch 185| loss: 0.15727 | val_0_rmse: 0.29945 |  0:00:35s\n",
            "epoch 186| loss: 0.16349 | val_0_rmse: 0.28551 |  0:00:36s\n",
            "epoch 187| loss: 0.14763 | val_0_rmse: 0.2816  |  0:00:36s\n",
            "epoch 188| loss: 0.15384 | val_0_rmse: 0.29236 |  0:00:36s\n",
            "epoch 189| loss: 0.14833 | val_0_rmse: 0.28481 |  0:00:36s\n",
            "epoch 190| loss: 0.16081 | val_0_rmse: 0.27377 |  0:00:36s\n",
            "epoch 191| loss: 0.14199 | val_0_rmse: 0.27505 |  0:00:37s\n",
            "epoch 192| loss: 0.14752 | val_0_rmse: 0.27285 |  0:00:37s\n",
            "epoch 193| loss: 0.14335 | val_0_rmse: 0.27928 |  0:00:37s\n",
            "epoch 194| loss: 0.14822 | val_0_rmse: 0.28061 |  0:00:37s\n",
            "epoch 195| loss: 0.14035 | val_0_rmse: 0.28255 |  0:00:37s\n",
            "epoch 196| loss: 0.13822 | val_0_rmse: 0.28481 |  0:00:38s\n",
            "epoch 197| loss: 0.14848 | val_0_rmse: 0.28973 |  0:00:38s\n",
            "epoch 198| loss: 0.1331  | val_0_rmse: 0.28474 |  0:00:38s\n",
            "epoch 199| loss: 0.14066 | val_0_rmse: 0.28252 |  0:00:38s\n",
            "epoch 200| loss: 0.1428  | val_0_rmse: 0.27954 |  0:00:39s\n",
            "epoch 201| loss: 0.14126 | val_0_rmse: 0.2878  |  0:00:39s\n",
            "epoch 202| loss: 0.14656 | val_0_rmse: 0.28649 |  0:00:39s\n",
            "epoch 203| loss: 0.13246 | val_0_rmse: 0.28096 |  0:00:39s\n",
            "epoch 204| loss: 0.12918 | val_0_rmse: 0.27741 |  0:00:39s\n",
            "epoch 205| loss: 0.13648 | val_0_rmse: 0.28114 |  0:00:40s\n",
            "epoch 206| loss: 0.14689 | val_0_rmse: 0.28354 |  0:00:40s\n",
            "epoch 207| loss: 0.14094 | val_0_rmse: 0.28549 |  0:00:40s\n",
            "epoch 208| loss: 0.13184 | val_0_rmse: 0.27653 |  0:00:40s\n",
            "epoch 209| loss: 0.13695 | val_0_rmse: 0.27565 |  0:00:40s\n",
            "epoch 210| loss: 0.13785 | val_0_rmse: 0.27129 |  0:00:41s\n",
            "epoch 211| loss: 0.12638 | val_0_rmse: 0.27191 |  0:00:41s\n",
            "epoch 212| loss: 0.12157 | val_0_rmse: 0.27237 |  0:00:41s\n",
            "epoch 213| loss: 0.13434 | val_0_rmse: 0.281   |  0:00:41s\n",
            "epoch 214| loss: 0.14725 | val_0_rmse: 0.305   |  0:00:41s\n",
            "epoch 215| loss: 0.13141 | val_0_rmse: 0.31054 |  0:00:42s\n",
            "epoch 216| loss: 0.12781 | val_0_rmse: 0.29631 |  0:00:42s\n",
            "epoch 217| loss: 0.1252  | val_0_rmse: 0.2875  |  0:00:42s\n",
            "epoch 218| loss: 0.13106 | val_0_rmse: 0.28382 |  0:00:42s\n",
            "epoch 219| loss: 0.12977 | val_0_rmse: 0.29165 |  0:00:42s\n",
            "epoch 220| loss: 0.14005 | val_0_rmse: 0.29552 |  0:00:43s\n",
            "epoch 221| loss: 0.1117  | val_0_rmse: 0.29878 |  0:00:43s\n",
            "epoch 222| loss: 0.13467 | val_0_rmse: 0.29985 |  0:00:43s\n",
            "epoch 223| loss: 0.11404 | val_0_rmse: 0.29649 |  0:00:43s\n",
            "epoch 224| loss: 0.12617 | val_0_rmse: 0.29746 |  0:00:43s\n",
            "epoch 225| loss: 0.12661 | val_0_rmse: 0.29985 |  0:00:44s\n",
            "epoch 226| loss: 0.11355 | val_0_rmse: 0.30389 |  0:00:44s\n",
            "epoch 227| loss: 0.12249 | val_0_rmse: 0.28809 |  0:00:44s\n",
            "epoch 228| loss: 0.12489 | val_0_rmse: 0.28905 |  0:00:44s\n",
            "epoch 229| loss: 0.12302 | val_0_rmse: 0.28553 |  0:00:44s\n",
            "epoch 230| loss: 0.1242  | val_0_rmse: 0.29702 |  0:00:44s\n",
            "epoch 231| loss: 0.1252  | val_0_rmse: 0.29093 |  0:00:45s\n",
            "epoch 232| loss: 0.12171 | val_0_rmse: 0.29107 |  0:00:45s\n",
            "epoch 233| loss: 0.12121 | val_0_rmse: 0.29336 |  0:00:45s\n",
            "epoch 234| loss: 0.12462 | val_0_rmse: 0.2859  |  0:00:45s\n",
            "epoch 235| loss: 0.12583 | val_0_rmse: 0.28686 |  0:00:45s\n",
            "epoch 236| loss: 0.11907 | val_0_rmse: 0.28417 |  0:00:46s\n",
            "epoch 237| loss: 0.12248 | val_0_rmse: 0.28825 |  0:00:46s\n",
            "epoch 238| loss: 0.1186  | val_0_rmse: 0.2899  |  0:00:46s\n",
            "epoch 239| loss: 0.12815 | val_0_rmse: 0.29188 |  0:00:46s\n",
            "epoch 240| loss: 0.13635 | val_0_rmse: 0.29549 |  0:00:46s\n",
            "epoch 241| loss: 0.13366 | val_0_rmse: 0.29361 |  0:00:46s\n",
            "epoch 242| loss: 0.1343  | val_0_rmse: 0.28928 |  0:00:47s\n",
            "epoch 243| loss: 0.11919 | val_0_rmse: 0.29629 |  0:00:47s\n",
            "epoch 244| loss: 0.12523 | val_0_rmse: 0.28591 |  0:00:47s\n",
            "epoch 245| loss: 0.1337  | val_0_rmse: 0.28702 |  0:00:47s\n",
            "epoch 246| loss: 0.12369 | val_0_rmse: 0.29119 |  0:00:47s\n",
            "epoch 247| loss: 0.12669 | val_0_rmse: 0.28446 |  0:00:48s\n",
            "epoch 248| loss: 0.12195 | val_0_rmse: 0.27694 |  0:00:48s\n",
            "epoch 249| loss: 0.12053 | val_0_rmse: 0.28105 |  0:00:48s\n",
            "epoch 250| loss: 0.11953 | val_0_rmse: 0.28102 |  0:00:48s\n",
            "epoch 251| loss: 0.11509 | val_0_rmse: 0.27772 |  0:00:48s\n",
            "epoch 252| loss: 0.11199 | val_0_rmse: 0.28363 |  0:00:48s\n",
            "epoch 253| loss: 0.12614 | val_0_rmse: 0.28632 |  0:00:49s\n",
            "epoch 254| loss: 0.11403 | val_0_rmse: 0.30891 |  0:00:49s\n",
            "epoch 255| loss: 0.12264 | val_0_rmse: 0.30873 |  0:00:49s\n",
            "epoch 256| loss: 0.11481 | val_0_rmse: 0.31765 |  0:00:49s\n",
            "epoch 257| loss: 0.11157 | val_0_rmse: 0.31479 |  0:00:49s\n",
            "epoch 258| loss: 0.11742 | val_0_rmse: 0.3033  |  0:00:50s\n",
            "epoch 259| loss: 0.11198 | val_0_rmse: 0.29812 |  0:00:50s\n",
            "epoch 260| loss: 0.1187  | val_0_rmse: 0.278   |  0:00:50s\n",
            "\n",
            "Early stopping occurred at epoch 260 with best_epoch = 210 and best_val_0_rmse = 0.27129\n",
            "TabNet training time: 50.5s\n",
            "[TabNet] MAE=25032.221  RMSE=36978.853  R²=0.8582\n",
            "[Naive-1]   RMSE=76369.520  R²=0.3950\n",
            "[Week-Naive]RMSE=100583.299  R²=-0.0494\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math, time\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "import torch\n",
        "\n",
        "# 데이터 분리\n",
        "TARGET = '합계'\n",
        "X = nanji.drop(columns=[c for c in ['합계', '합계_1일후','합계_2일후'] if c in nanji.columns]).copy()\n",
        "y = nanji[TARGET].copy()\n",
        "\n",
        "n = len(X)\n",
        "test_n = int(math.ceil(n * 0.10))\n",
        "train_n = n - test_n\n",
        "\n",
        "X_train, X_test = X.iloc[:train_n, :].copy(), X.iloc[train_n:, :].copy()\n",
        "y_train, y_test = y.iloc[:train_n].copy(), y.iloc[train_n:].copy()\n",
        "\n",
        "print(f\"[SPLIT] train={X_train.shape}, test={X_test.shape}\")\n",
        "\n",
        "# 스케일링\n",
        "x_scaler = StandardScaler()\n",
        "X_train_s = x_scaler.fit_transform(X_train)\n",
        "X_test_s  = x_scaler.transform(X_test)\n",
        "\n",
        "y_scaler = StandardScaler()\n",
        "y_train_s = y_scaler.fit_transform(y_train.values.reshape(-1,1)).astype(np.float32)  # (n,1)\n",
        "y_test_s  = y_scaler.transform(y_test.values.reshape(-1,1)).astype(np.float32)      # (m,1)\n",
        "\n",
        "# TabNet 입력 dtype/shape\n",
        "X_train_np = X_train_s.astype(np.float32)\n",
        "X_test_np  = X_test_s.astype(np.float32)\n",
        "\n",
        "# TabNet 설정\n",
        "tabnet = TabNetRegressor(\n",
        "    n_d=32, n_a=32, n_steps=4,\n",
        "    gamma=1.5, n_independent=2, n_shared=2,\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=1e-3, weight_decay=1e-5),\n",
        "    scheduler_fn=None,                 # 우선 스케줄러 OFF로 수렴 확인\n",
        "    mask_type='entmax',                # 엔트맥스가 일반적으로 안정적\n",
        "    verbose=1,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "t0 = time.time()\n",
        "tabnet.fit(\n",
        "    X_train=X_train_np,\n",
        "    y_train=y_train_s,                    # <-- (n,1) 2D\n",
        "    eval_set=[(X_test_np, y_test_s)],     # <-- (m,1) 2D\n",
        "    eval_metric=[\"rmse\"],                 # 표준화된 y 기준 RMSE\n",
        "    max_epochs=500,\n",
        "    patience=50,\n",
        "    batch_size=512,\n",
        "    virtual_batch_size=128,\n",
        "    num_workers=0,\n",
        "    drop_last=False\n",
        ")\n",
        "print(f\"TabNet training time: {time.time()-t0:.1f}s\")\n",
        "\n",
        "y_pred_s = tabnet.predict(X_test_np).squeeze()                 # (m,)\n",
        "y_pred = y_scaler.inverse_transform(y_pred_s.reshape(-1,1)).ravel()\n",
        "\n",
        "y_true = y_test.values\n",
        "mse  = mean_squared_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mse)   # 버전 독립 RMSE\n",
        "mae  = mean_absolute_error(y_true, y_pred)\n",
        "r2   = r2_score(y_true, y_pred)\n",
        "\n",
        "print(f\"[TabNet] MAE={mae:.3f}  RMSE={rmse:.3f}  R²={r2:.4f}\")\n",
        "\n",
        "y_test_arr = y_true\n",
        "# 어제값(naive-1): 테스트 내 시프트로 근사 비교 (첫 값 보정)\n",
        "y_na1 = np.roll(y_test_arr, 1); y_na1[0] = y_test_arr[0]\n",
        "# 7일 전(week naive): 주간 패턴 근사\n",
        "y_wk  = np.roll(y_test_arr, 7); y_wk[:7] = y_test_arr[:7]\n",
        "\n",
        "rmse_na1 = np.sqrt(mean_squared_error(y_test_arr, y_na1)); r2_na1 = r2_score(y_test_arr, y_na1)\n",
        "rmse_wk  = np.sqrt(mean_squared_error(y_test_arr, y_wk));  r2_wk  = r2_score(y_test_arr, y_wk)\n",
        "\n",
        "print(f\"[Naive-1]   RMSE={rmse_na1:.3f}  R²={r2_na1:.4f}\")\n",
        "print(f\"[Week-Naive]RMSE={rmse_wk:.3f}  R²={r2_wk:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03fb8f2c",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2f1679e",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8bcef057",
      "metadata": {},
      "source": [
        "# 새로운 TabNet\n",
        "\n",
        "**TabNet 전용 머신러닝 파이프라인 설계**\n",
        "\n",
        "기존 파이프라인을 TabNet에 맞게 수정해보겠습니다:\n",
        "\n",
        "**1. 데이터 준비** (동일)\n",
        "- 원본 데이터 로드 + 파생변수 생성\n",
        "- 누적 강수량, 체감온도, 이동평균, 계절/연도 등 추가\n",
        "\n",
        "**2. 타깃 설정** (동일)\n",
        "- 회귀: `합계_1일후`\n",
        "- 분류: `등급_1일후`\n",
        "\n",
        "**3. 타깃 분포 확인** (동일)\n",
        "- 분포 시각화 및 불균형 확인\n",
        "\n",
        "**4. 데이터 분할** (수정 필요)\n",
        "- **Temporal Split만 사용** (TabNet 특성상 시계열 순서 중요)\n",
        "- Train/Validation/Test = 70:20:10 (TabNet은 validation set 필수)\n",
        "- 분류 시에도 temporal 우선 (early stopping 위해)\n",
        "\n",
        "**5. 모델 후보군** (TabNet 전용)\n",
        "- **회귀**: TabNetRegressor\n",
        "- **분류**: TabNetClassifier\n",
        "\n",
        "**6. 성능 지표** (동일)\n",
        "- 회귀: R², MAE, RMSE, MAPE, SMAPE  \n",
        "- 분류: F1_weighted, F1_macro, Accuracy, ROC-AUC\n",
        "\n",
        "**7. 하이퍼파라미터 튜닝** (TabNet 특화)\n",
        "```python\n",
        "param_grid = {\n",
        "    'n_d': [16, 32, 64],           # decision layer width\n",
        "    'n_a': [16, 32, 64],           # attention layer width  \n",
        "    'n_steps': [3, 4, 5, 6],       # attention steps\n",
        "    'gamma': [1.0, 1.2, 1.5, 2.0], # sparsity regularization\n",
        "    'n_independent': [1, 2, 3],     # independent layers\n",
        "    'n_shared': [1, 2, 3],         # shared layers\n",
        "    'lambda_sparse': [0, 1e-4, 1e-3], # sparsity loss weight\n",
        "    'optimizer_params': [\n",
        "        {'lr': 0.02, 'weight_decay': 1e-4},\n",
        "        {'lr': 0.01, 'weight_decay': 1e-5}, \n",
        "        {'lr': 0.005, 'weight_decay': 1e-5}\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "**8. 학습 & 튜닝** (수정)\n",
        "- **32개 모델**: 센터(4) × 태스크(2) × 하이퍼파라미터 조합(4개 베스트)\n",
        "- RandomSearchCV 또는 GridSearchCV (시간 고려)\n",
        "- Early stopping 활용 (patience=50)\n",
        "\n",
        "**9. 베스트 모델 선정**\n",
        "- 센터×태스크별 최고 성능 모델 8개\n",
        "- `tabnet_best_models.csv` 저장\n",
        "\n",
        "**10. 베스트 모델 재학습**\n",
        "- 8개 베스트 TabNet 모델 재학습\n",
        "- Pickle 저장 + 성능 시각화\n",
        "- Feature Importance (TabNet attention weights)\n",
        "\n",
        "**11. 해석 가능성 분석** (TabNet 특화)\n",
        "- **TabNet Feature Importance**: attention masks 활용\n",
        "- **SHAP**: TabNet과 호환되는 TreeExplainer 대신 KernelExplainer\n",
        "- **Feature Masks 시각화**: 각 step별 attention 패턴\n",
        "\n",
        "**12. 성능 비교 분석**\n",
        "- TabNet vs 기존 베스트 모델 비교\n",
        "- Temporal split 조건에서 공정한 비교\n",
        "- 학습 시간 vs 성능 트레이드오프 분석\n",
        "\n",
        "**주의사항:**\n",
        "1. **메모리 관리**: TabNet은 GPU 메모리 많이 사용\n",
        "2. **학습 시간**: 하이퍼파라미터 튜닝에 상당한 시간 소요  \n",
        "3. **Validation 필수**: TabNet의 early stopping을 위해\n",
        "4. **배치 크기 조정**: 데이터 크기에 맞게 batch_size 최적화\n",
        "\n",
        "이런 방식으로 진행하시겠습니까? 특히 하이퍼파라미터 튜닝 범위나 학습 시간을 고려해서 조정이 필요할 수 있습니다.\n",
        "\n",
        "\n",
        "네, 맞습니다. 이 코드를 실행하면:\n",
        "\n",
        "**저장되는 결과물:**\n",
        "\n",
        "1. **모든 하이퍼파라미터 시도 결과**\n",
        "   - 파일: `tabnet_hyperparameter_results.csv`\n",
        "   - 내용: 센터별/태스크별로 시도한 모든 파라미터 조합과 성능 점수\n",
        "   - 대략 120개 행 (4센터 × 2태스크 × 15시도)\n",
        "\n",
        "2. **베스트 모델 정보**\n",
        "   - 파일: `tabnet_best_models.csv` \n",
        "   - 내용: 센터/태스크별 최고 성능 모델의 파라미터와 최종 성능\n",
        "   - 8개 행 (4센터 × 2태스크)\n",
        "\n",
        "3. **베스트 모델 파일들**\n",
        "   - 파일: `{center}_{task}_tabnet_best.pkl` (8개 파일)\n",
        "   - 내용: 학습된 모델, 파라미터, 성능 지표, 스케일러 등 모든 정보\n",
        "\n",
        "**CSV 파일에 포함될 컬럼들:**\n",
        "- 하이퍼파라미터: n_d, n_a, n_steps, gamma, learning_rate 등\n",
        "- 성능 지표: R², RMSE, MAE (회귀) / F1, Accuracy (분류)\n",
        "- 메타 정보: center, task, training_time 등\n",
        "\n",
        "이렇게 저장되므로 나중에 어떤 파라미터 조합이 어느 센터에서 잘 작동했는지 분석할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b11b77f1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔥 TabNet 전용 머신러닝 파이프라인 시작\n",
            "⏰ 시작 시간: 2025-08-31 15:03:16\n",
            "\n",
            "============================================================\n",
            "🏢 NANJI 센터 처리 시작\n",
            "============================================================\n",
            "✅ nanji 데이터 로드 완료: (3069, 38)\n",
            "📊 데이터 분할 완료: Train=2148, Val=614, Test=307\n",
            "\n",
            "🎯 REGRESSION 태스크 시작\n",
            "🔧 nanji - regression 하이퍼파라미터 튜닝 시작...\n",
            "  시도 1/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 20 and best_val_0_rmse = 0.91744\n",
            "    ✅ 새로운 베스트: 0.4762\n",
            "  시도 2/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 101 with best_epoch = 71 and best_val_0_rmse = 0.85651\n",
            "    ✅ 새로운 베스트: 0.5435\n",
            "  시도 3/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 47 with best_epoch = 17 and best_val_0_rmse = 0.84366\n",
            "    ✅ 새로운 베스트: 0.5571\n",
            "  시도 4/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 20 and best_val_0_rmse = 0.85312\n",
            "  시도 5/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 62 with best_epoch = 32 and best_val_0_rmse = 0.84886\n",
            "  시도 6/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 65 with best_epoch = 35 and best_val_0_rmse = 0.88239\n",
            "  시도 7/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 108 with best_epoch = 78 and best_val_0_rmse = 0.84332\n",
            "    ✅ 새로운 베스트: 0.5574\n",
            "  시도 8/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 12 and best_val_0_rmse = 0.88863\n",
            "  시도 9/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 18 and best_val_0_rmse = 0.86775\n",
            "  시도 10/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 49 with best_epoch = 19 and best_val_0_rmse = 0.90247\n",
            "  시도 11/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 31 and best_val_0_rmse = 0.91828\n",
            "  시도 12/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 36 with best_epoch = 6 and best_val_0_rmse = 0.94798\n",
            "  시도 13/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 52 with best_epoch = 22 and best_val_0_rmse = 0.85207\n",
            "  시도 14/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 82 with best_epoch = 52 and best_val_0_rmse = 0.83687\n",
            "    ✅ 새로운 베스트: 0.5642\n",
            "  시도 15/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 34 and best_val_0_rmse = 0.8765\n",
            "🏆 nanji - regression 튜닝 완료: Best Score = 0.5642\n",
            "💾 모델 저장 완료: ../results_tabnet/models/nanji_regression_tabnet_best.pkl\n",
            "\n",
            "🎯 CLASSIFICATION 태스크 시작\n",
            "🔧 nanji - classification 하이퍼파라미터 튜닝 시작...\n",
            "  시도 1/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 4 and best_val_0_accuracy = 0.67264\n",
            "    ✅ 새로운 베스트: 0.6655\n",
            "  시도 2/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 3 and best_val_0_accuracy = 0.58632\n",
            "  시도 3/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 31 with best_epoch = 1 and best_val_0_accuracy = 0.63355\n",
            "  시도 4/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 3 and best_val_0_accuracy = 0.68078\n",
            "  시도 5/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 4 and best_val_0_accuracy = 0.58795\n",
            "  시도 6/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 12 and best_val_0_accuracy = 0.62541\n",
            "  시도 7/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 20 and best_val_0_accuracy = 0.6759\n",
            "  시도 8/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 3 and best_val_0_accuracy = 0.59609\n",
            "  시도 9/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 18 and best_val_0_accuracy = 0.50651\n",
            "  시도 10/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 37 with best_epoch = 7 and best_val_0_accuracy = 0.65961\n",
            "  시도 11/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 51 with best_epoch = 21 and best_val_0_accuracy = 0.64007\n",
            "  시도 12/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 36 with best_epoch = 6 and best_val_0_accuracy = 0.64007\n",
            "  시도 13/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 37 with best_epoch = 7 and best_val_0_accuracy = 0.61401\n",
            "  시도 14/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 49 with best_epoch = 19 and best_val_0_accuracy = 0.59283\n",
            "  시도 15/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 12 and best_val_0_accuracy = 0.66287\n",
            "🏆 nanji - classification 튜닝 완료: Best Score = 0.6655\n",
            "💾 모델 저장 완료: ../results_tabnet/models/nanji_classification_tabnet_best.pkl\n",
            "\n",
            "============================================================\n",
            "🏢 JUNGNANG 센터 처리 시작\n",
            "============================================================\n",
            "✅ jungnang 데이터 로드 완료: (3069, 40)\n",
            "📊 데이터 분할 완료: Train=2148, Val=614, Test=307\n",
            "\n",
            "🎯 REGRESSION 태스크 시작\n",
            "🔧 jungnang - regression 하이퍼파라미터 튜닝 시작...\n",
            "  시도 1/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 74 with best_epoch = 44 and best_val_0_rmse = 0.67252\n",
            "    ✅ 새로운 베스트: 0.3991\n",
            "  시도 2/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 45 with best_epoch = 15 and best_val_0_rmse = 0.64097\n",
            "    ✅ 새로운 베스트: 0.4542\n",
            "  시도 3/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 56 with best_epoch = 26 and best_val_0_rmse = 0.61186\n",
            "    ✅ 새로운 베스트: 0.5026\n",
            "  시도 4/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 46 with best_epoch = 16 and best_val_0_rmse = 0.67024\n",
            "  시도 5/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 18 and best_val_0_rmse = 0.65512\n",
            "  시도 6/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 52 with best_epoch = 22 and best_val_0_rmse = 0.67405\n",
            "  시도 7/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 34 and best_val_0_rmse = 0.63031\n",
            "  시도 8/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 31 and best_val_0_rmse = 0.63259\n",
            "  시도 9/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 83 with best_epoch = 53 and best_val_0_rmse = 0.6538\n",
            "  시도 10/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 13 and best_val_0_rmse = 0.65148\n",
            "  시도 11/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 88 with best_epoch = 58 and best_val_0_rmse = 0.6315\n",
            "  시도 12/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 81 with best_epoch = 51 and best_val_0_rmse = 0.63786\n",
            "  시도 13/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 54 with best_epoch = 24 and best_val_0_rmse = 0.61133\n",
            "    ✅ 새로운 베스트: 0.5035\n",
            "  시도 14/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 76 with best_epoch = 46 and best_val_0_rmse = 0.67747\n",
            "  시도 15/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 44 with best_epoch = 14 and best_val_0_rmse = 0.68126\n",
            "🏆 jungnang - regression 튜닝 완료: Best Score = 0.5035\n",
            "💾 모델 저장 완료: ../results_tabnet/models/jungnang_regression_tabnet_best.pkl\n",
            "\n",
            "🎯 CLASSIFICATION 태스크 시작\n",
            "🔧 jungnang - classification 하이퍼파라미터 튜닝 시작...\n",
            "  시도 1/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 35 with best_epoch = 5 and best_val_0_accuracy = 0.64658\n",
            "    ✅ 새로운 베스트: 0.5416\n",
            "  시도 2/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 55 with best_epoch = 25 and best_val_0_accuracy = 0.67264\n",
            "    ✅ 새로운 베스트: 0.6555\n",
            "  시도 3/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 66 with best_epoch = 36 and best_val_0_accuracy = 0.68078\n",
            "    ✅ 새로운 베스트: 0.6663\n",
            "  시도 4/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 13 and best_val_0_accuracy = 0.67427\n",
            "  시도 5/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 40 with best_epoch = 10 and best_val_0_accuracy = 0.6759\n",
            "  시도 6/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 31 with best_epoch = 1 and best_val_0_accuracy = 0.57492\n",
            "  시도 7/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 37 with best_epoch = 7 and best_val_0_accuracy = 0.64169\n",
            "  시도 8/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 39 with best_epoch = 9 and best_val_0_accuracy = 0.63192\n",
            "  시도 9/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 40 with best_epoch = 10 and best_val_0_accuracy = 0.65472\n",
            "  시도 10/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 20 and best_val_0_accuracy = 0.62704\n",
            "  시도 11/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 35 with best_epoch = 5 and best_val_0_accuracy = 0.63192\n",
            "  시도 12/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 32 with best_epoch = 2 and best_val_0_accuracy = 0.61075\n",
            "  시도 13/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 4 and best_val_0_accuracy = 0.65309\n",
            "  시도 14/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 38 with best_epoch = 8 and best_val_0_accuracy = 0.67264\n",
            "  시도 15/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 37 with best_epoch = 7 and best_val_0_accuracy = 0.64169\n",
            "🏆 jungnang - classification 튜닝 완료: Best Score = 0.6663\n",
            "💾 모델 저장 완료: ../results_tabnet/models/jungnang_classification_tabnet_best.pkl\n",
            "\n",
            "============================================================\n",
            "🏢 SEONAM 센터 처리 시작\n",
            "============================================================\n",
            "✅ seonam 데이터 로드 완료: (3069, 39)\n",
            "📊 데이터 분할 완료: Train=2148, Val=614, Test=307\n",
            "\n",
            "🎯 REGRESSION 태스크 시작\n",
            "🔧 seonam - regression 하이퍼파라미터 튜닝 시작...\n",
            "  시도 1/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 83 with best_epoch = 53 and best_val_0_rmse = 0.70422\n",
            "    ✅ 새로운 베스트: 0.2811\n",
            "  시도 2/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 46 with best_epoch = 16 and best_val_0_rmse = 0.65145\n",
            "    ✅ 새로운 베스트: 0.3848\n",
            "  시도 3/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 60 with best_epoch = 30 and best_val_0_rmse = 0.62079\n",
            "    ✅ 새로운 베스트: 0.4413\n",
            "  시도 4/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 66 with best_epoch = 36 and best_val_0_rmse = 0.72533\n",
            "  시도 5/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 48 with best_epoch = 18 and best_val_0_rmse = 0.61832\n",
            "    ✅ 새로운 베스트: 0.4458\n",
            "  시도 6/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 82 with best_epoch = 52 and best_val_0_rmse = 0.67382\n",
            "  시도 7/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 88 with best_epoch = 58 and best_val_0_rmse = 0.70424\n",
            "  시도 8/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 75 with best_epoch = 45 and best_val_0_rmse = 0.57995\n",
            "    ✅ 새로운 베스트: 0.5124\n",
            "  시도 9/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 65 with best_epoch = 35 and best_val_0_rmse = 0.64355\n",
            "  시도 10/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 96 with best_epoch = 66 and best_val_0_rmse = 0.62315\n",
            "  시도 11/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 55 with best_epoch = 25 and best_val_0_rmse = 0.62692\n",
            "  시도 12/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 70 with best_epoch = 40 and best_val_0_rmse = 0.65456\n",
            "  시도 13/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 109 with best_epoch = 79 and best_val_0_rmse = 0.69332\n",
            "  시도 14/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 100 with best_epoch = 70 and best_val_0_rmse = 0.6211\n",
            "  시도 15/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 65 with best_epoch = 35 and best_val_0_rmse = 0.63974\n",
            "🏆 seonam - regression 튜닝 완료: Best Score = 0.5124\n",
            "💾 모델 저장 완료: ../results_tabnet/models/seonam_regression_tabnet_best.pkl\n",
            "\n",
            "🎯 CLASSIFICATION 태스크 시작\n",
            "🔧 seonam - classification 하이퍼파라미터 튜닝 시작...\n",
            "  시도 1/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 41 with best_epoch = 11 and best_val_0_accuracy = 0.58306\n",
            "    ✅ 새로운 베스트: 0.5156\n",
            "  시도 2/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 32 with best_epoch = 2 and best_val_0_accuracy = 0.59446\n",
            "  시도 3/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 33 with best_epoch = 3 and best_val_0_accuracy = 0.56515\n",
            "  시도 4/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 4 and best_val_0_accuracy = 0.5342\n",
            "  시도 5/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 37 with best_epoch = 7 and best_val_0_accuracy = 0.56515\n",
            "    ✅ 새로운 베스트: 0.5201\n",
            "  시도 6/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 58 with best_epoch = 28 and best_val_0_accuracy = 0.55375\n",
            "    ✅ 새로운 베스트: 0.5225\n",
            "  시도 7/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 4 and best_val_0_accuracy = 0.56352\n",
            "  시도 8/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 31 with best_epoch = 1 and best_val_0_accuracy = 0.55863\n",
            "  시도 9/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 34 with best_epoch = 4 and best_val_0_accuracy = 0.55537\n",
            "    ✅ 새로운 베스트: 0.5261\n",
            "  시도 10/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 12 and best_val_0_accuracy = 0.57492\n",
            "  시도 11/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 62 with best_epoch = 32 and best_val_0_accuracy = 0.54723\n",
            "  시도 12/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 35 with best_epoch = 5 and best_val_0_accuracy = 0.58958\n",
            "    ✅ 새로운 베스트: 0.5272\n",
            "  시도 13/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 56 with best_epoch = 26 and best_val_0_accuracy = 0.55863\n",
            "  시도 14/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 66 with best_epoch = 36 and best_val_0_accuracy = 0.57818\n",
            "  시도 15/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 37 with best_epoch = 7 and best_val_0_accuracy = 0.53583\n",
            "🏆 seonam - classification 튜닝 완료: Best Score = 0.5272\n",
            "💾 모델 저장 완료: ../results_tabnet/models/seonam_classification_tabnet_best.pkl\n",
            "\n",
            "============================================================\n",
            "🏢 TANCHEON 센터 처리 시작\n",
            "============================================================\n",
            "✅ tancheon 데이터 로드 완료: (3069, 38)\n",
            "📊 데이터 분할 완료: Train=2148, Val=614, Test=307\n",
            "\n",
            "🎯 REGRESSION 태스크 시작\n",
            "🔧 tancheon - regression 하이퍼파라미터 튜닝 시작...\n",
            "  시도 1/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 39 with best_epoch = 9 and best_val_0_rmse = 0.94273\n",
            "    ✅ 새로운 베스트: 0.2917\n",
            "  시도 2/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 83 with best_epoch = 53 and best_val_0_rmse = 0.86331\n",
            "    ✅ 새로운 베스트: 0.4061\n",
            "  시도 3/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 31 and best_val_0_rmse = 0.9255\n",
            "  시도 4/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 74 with best_epoch = 44 and best_val_0_rmse = 0.90005\n",
            "  시도 5/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 49 with best_epoch = 19 and best_val_0_rmse = 0.87372\n",
            "  시도 6/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 58 with best_epoch = 28 and best_val_0_rmse = 0.86741\n",
            "  시도 7/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 37 with best_epoch = 7 and best_val_0_rmse = 0.91432\n",
            "  시도 8/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 72 with best_epoch = 42 and best_val_0_rmse = 0.84833\n",
            "    ✅ 새로운 베스트: 0.4265\n",
            "  시도 9/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 93 with best_epoch = 63 and best_val_0_rmse = 0.92016\n",
            "  시도 10/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 41 with best_epoch = 11 and best_val_0_rmse = 0.95273\n",
            "  시도 11/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 45 with best_epoch = 15 and best_val_0_rmse = 0.92637\n",
            "  시도 12/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 67 with best_epoch = 37 and best_val_0_rmse = 0.85438\n",
            "  시도 13/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 53 with best_epoch = 23 and best_val_0_rmse = 0.86368\n",
            "  시도 14/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 13 and best_val_0_rmse = 0.93574\n",
            "  시도 15/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 43 with best_epoch = 13 and best_val_0_rmse = 0.92277\n",
            "🏆 tancheon - regression 튜닝 완료: Best Score = 0.4265\n",
            "💾 모델 저장 완료: ../results_tabnet/models/tancheon_regression_tabnet_best.pkl\n",
            "\n",
            "🎯 CLASSIFICATION 태스크 시작\n",
            "🔧 tancheon - classification 하이퍼파라미터 튜닝 시작...\n",
            "  시도 1/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 46 with best_epoch = 16 and best_val_0_accuracy = 0.59283\n",
            "    ✅ 새로운 베스트: 0.5425\n",
            "  시도 2/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 12 and best_val_0_accuracy = 0.59609\n",
            "    ✅ 새로운 베스트: 0.5567\n",
            "  시도 3/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 44 with best_epoch = 14 and best_val_0_accuracy = 0.58143\n",
            "  시도 4/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 65 with best_epoch = 35 and best_val_0_accuracy = 0.56678\n",
            "  시도 5/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 30 with best_epoch = 0 and best_val_0_accuracy = 0.53746\n",
            "  시도 6/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 36 with best_epoch = 6 and best_val_0_accuracy = 0.58469\n",
            "  시도 7/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 46 with best_epoch = 16 and best_val_0_accuracy = 0.59609\n",
            "  시도 8/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 38 with best_epoch = 8 and best_val_0_accuracy = 0.59446\n",
            "  시도 9/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 58 with best_epoch = 28 and best_val_0_accuracy = 0.61238\n",
            "    ✅ 새로운 베스트: 0.5813\n",
            "  시도 10/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 38 with best_epoch = 8 and best_val_0_accuracy = 0.59121\n",
            "  시도 11/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 54 with best_epoch = 24 and best_val_0_accuracy = 0.63029\n",
            "  시도 12/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 45 with best_epoch = 15 and best_val_0_accuracy = 0.60261\n",
            "  시도 13/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.01, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 12 and best_val_0_accuracy = 0.59772\n",
            "  시도 14/15: {'gamma': 1.5, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 16, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 12 and best_val_0_accuracy = 0.61564\n",
            "  시도 15/15: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.02, 'n_a': 16, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 55 with best_epoch = 25 and best_val_0_accuracy = 0.62541\n",
            "    ✅ 새로운 베스트: 0.6043\n",
            "🏆 tancheon - classification 튜닝 완료: Best Score = 0.6043\n",
            "💾 모델 저장 완료: ../results_tabnet/models/tancheon_classification_tabnet_best.pkl\n",
            "\n",
            "🎉 TabNet 파이프라인 완료!\n",
            "📁 결과 파일:\n",
            "  - tabnet_hyperparameter_results.csv\n",
            "  - tabnet_best_models.csv\n",
            "  - 모델 파일들: ../results_tabnet/models/\n"
          ]
        }
      ],
      "source": [
        "# TabNet 전용 머신러닝 파이프라인\n",
        "# ================================================================\n",
        "# 하수처리량 예측 - TabNet 딥러닝 모델 전용 파이프라인\n",
        "# ================================================================\n",
        "\n",
        "import os, warnings, time, pickle\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from itertools import product\n",
        "\n",
        "# TabNet\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor, TabNetClassifier\n",
        "import torch\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    mean_absolute_error, mean_squared_error, r2_score,\n",
        "    accuracy_score, f1_score, roc_auc_score, classification_report\n",
        ")\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# 해석도구\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SHAP_AVAILABLE = False\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 설정\n",
        "plt.rcParams['font.family'] = 'AppleGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# 디렉토리 생성\n",
        "directories = [\n",
        "    '../results_tabnet',\n",
        "    '../results_tabnet/models', \n",
        "    '../results_tabnet/visualizations',\n",
        "    '../results_tabnet/interpretability',\n",
        "    '../results_tabnet/hyperparams'\n",
        "]\n",
        "for directory in directories:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "print(\"🔥 TabNet 전용 머신러닝 파이프라인 시작\")\n",
        "print(f\"⏰ 시작 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# ================================================================\n",
        "# 1. 데이터 로드 및 전처리\n",
        "# ================================================================\n",
        "\n",
        "def load_and_preprocess_data(center_name):\n",
        "    \"\"\"센터별 데이터 로드 및 전처리\"\"\"\n",
        "    file_path = f'../data/add_feature/{center_name}_add_feature.csv'\n",
        "    \n",
        "    try:\n",
        "        data = pd.read_csv(file_path, encoding='utf-8-sig')\n",
        "        \n",
        "        # 불필요한 컬럼 제거\n",
        "        drop_cols = ['날짜', '요일', '1처리장', '2처리장', '정화조', '중계펌프장']\n",
        "        existing_drop_cols = [col for col in drop_cols if col in data.columns]\n",
        "        data = data.drop(existing_drop_cols, axis=1)\n",
        "        \n",
        "        # 결측치 제거\n",
        "        data = data.dropna()\n",
        "        \n",
        "        print(f\"✅ {center_name} 데이터 로드 완료: {data.shape}\")\n",
        "        return data\n",
        "        \n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ {center_name} 데이터 파일을 찾을 수 없습니다: {file_path}\")\n",
        "        return None\n",
        "\n",
        "# ================================================================\n",
        "# 2. 데이터 분할 (Temporal Split)\n",
        "# ================================================================\n",
        "\n",
        "def temporal_split_data(data, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
        "    \"\"\"시계열 순서를 유지한 데이터 분할\"\"\"\n",
        "    n = len(data)\n",
        "    train_end = int(n * train_ratio)\n",
        "    val_end = int(n * (train_ratio + val_ratio))\n",
        "    \n",
        "    train_data = data.iloc[:train_end].copy()\n",
        "    val_data = data.iloc[train_end:val_end].copy() \n",
        "    test_data = data.iloc[val_end:].copy()\n",
        "    \n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "def prepare_features_target(train_data, val_data, test_data, task='regression'):\n",
        "    \"\"\"피처와 타겟 분리 및 스케일링\"\"\"\n",
        "    \n",
        "    if task == 'regression':\n",
        "        target_col = '합계_1일후'\n",
        "        exclude_cols = [\n",
        "            '1처리장','2처리장','정화조','중계펌프장','합계','시설현대화',\n",
        "            '3처리장','4처리장','합계', '합계_1일후','합계_2일후',\n",
        "            '등급','등급_1일후','등급_2일후'\n",
        "        ]\n",
        "    else:  # classification\n",
        "        target_col = '등급_1일후'\n",
        "        exclude_cols = [\n",
        "            '1처리장','2처리장','정화조','중계펌프장','합계','시설현대화',\n",
        "            '3처리장','4처리장','합계', '합계_1일후','합계_2일후',\n",
        "            '등급','등급_1일후','등급_2일후'\n",
        "        ]\n",
        "    \n",
        "    # 존재하는 컬럼만 제외\n",
        "    existing_exclude_cols = [col for col in exclude_cols if col in train_data.columns]\n",
        "    \n",
        "    # 피처 분리\n",
        "    X_train = train_data.drop(existing_exclude_cols, axis=1)\n",
        "    X_val = val_data.drop(existing_exclude_cols, axis=1)\n",
        "    X_test = test_data.drop(existing_exclude_cols, axis=1)\n",
        "    \n",
        "    # 타겟 분리\n",
        "    y_train = train_data[target_col].copy()\n",
        "    y_val = val_data[target_col].copy()\n",
        "    y_test = test_data[target_col].copy()\n",
        "    \n",
        "    # 피처 스케일링\n",
        "    x_scaler = StandardScaler()\n",
        "    X_train_scaled = x_scaler.fit_transform(X_train).astype(np.float32)\n",
        "    X_val_scaled = x_scaler.transform(X_val).astype(np.float32)\n",
        "    X_test_scaled = x_scaler.transform(X_test).astype(np.float32)\n",
        "    \n",
        "    # 회귀의 경우 타겟도 스케일링\n",
        "    if task == 'regression':\n",
        "        y_scaler = StandardScaler()\n",
        "        y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1,1)).astype(np.float32)\n",
        "        y_val_scaled = y_scaler.transform(y_val.values.reshape(-1,1)).astype(np.float32)\n",
        "        y_test_scaled = y_scaler.transform(y_test.values.reshape(-1,1)).astype(np.float32)\n",
        "        \n",
        "        return {\n",
        "            'X_train': X_train_scaled, 'X_val': X_val_scaled, 'X_test': X_test_scaled,\n",
        "            'y_train': y_train_scaled, 'y_val': y_val_scaled, 'y_test': y_test_scaled,\n",
        "            'y_train_orig': y_train, 'y_val_orig': y_val, 'y_test_orig': y_test,\n",
        "            'x_scaler': x_scaler, 'y_scaler': y_scaler,\n",
        "            'feature_names': X_train.columns.tolist()\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            'X_train': X_train_scaled, 'X_val': X_val_scaled, 'X_test': X_test_scaled,\n",
        "            'y_train': y_train.values.astype(int), 'y_val': y_val.values.astype(int), \n",
        "            'y_test': y_test.values.astype(int),\n",
        "            'x_scaler': x_scaler, 'y_scaler': None,\n",
        "            'feature_names': X_train.columns.tolist()\n",
        "        }\n",
        "\n",
        "# ================================================================\n",
        "# 3. 하이퍼파라미터 설정\n",
        "# ================================================================\n",
        "\n",
        "def get_tabnet_param_grid():\n",
        "    \"\"\"TabNet 하이퍼파라미터 그리드\"\"\"\n",
        "    \n",
        "    # 기본 파라미터 (빠른 실험용)\n",
        "    basic_params = {\n",
        "        'n_d': [16, 32],\n",
        "        'n_a': [16, 32], \n",
        "        'n_steps': [3, 4],\n",
        "        'gamma': [1.0, 1.5],\n",
        "        'n_independent': [2],\n",
        "        'n_shared': [2],\n",
        "        'lambda_sparse': [1e-4],\n",
        "        'learning_rate': [0.02, 0.01],\n",
        "        'weight_decay': [1e-5]\n",
        "    }\n",
        "    \n",
        "    # 상세 파라미터 (시간 여유 시)\n",
        "    detailed_params = {\n",
        "        'n_d': [16, 32, 64],\n",
        "        'n_a': [16, 32, 64],\n",
        "        'n_steps': [3, 4, 5, 6],\n",
        "        'gamma': [1.0, 1.2, 1.5, 2.0],\n",
        "        'n_independent': [1, 2, 3],\n",
        "        'n_shared': [1, 2, 3], \n",
        "        'lambda_sparse': [0, 1e-4, 1e-3],\n",
        "        'learning_rate': [0.02, 0.015, 0.01, 0.005],\n",
        "        'weight_decay': [1e-4, 1e-5, 1e-6]\n",
        "    }\n",
        "    \n",
        "    return basic_params  # 기본으로 시작\n",
        "\n",
        "# ================================================================\n",
        "# 4. 모델 학습 및 평가\n",
        "# ================================================================\n",
        "\n",
        "def train_tabnet_model(data_dict, task='regression', **params):\n",
        "    \"\"\"TabNet 모델 학습\"\"\"\n",
        "    \n",
        "    # 모델 초기화\n",
        "    if task == 'regression':\n",
        "        model = TabNetRegressor(\n",
        "            n_d=params.get('n_d', 32),\n",
        "            n_a=params.get('n_a', 32),\n",
        "            n_steps=params.get('n_steps', 4),\n",
        "            gamma=params.get('gamma', 1.0),\n",
        "            n_independent=params.get('n_independent', 2),\n",
        "            n_shared=params.get('n_shared', 2),\n",
        "            lambda_sparse=params.get('lambda_sparse', 1e-4),\n",
        "            optimizer_fn=torch.optim.Adam,\n",
        "            optimizer_params={\n",
        "                'lr': params.get('learning_rate', 0.02),\n",
        "                'weight_decay': params.get('weight_decay', 1e-5)\n",
        "            },\n",
        "            mask_type='entmax',\n",
        "            verbose=0,\n",
        "            seed=42\n",
        "        )\n",
        "    else:  # classification\n",
        "        model = TabNetClassifier(\n",
        "            n_d=params.get('n_d', 32),\n",
        "            n_a=params.get('n_a', 32), \n",
        "            n_steps=params.get('n_steps', 4),\n",
        "            gamma=params.get('gamma', 1.0),\n",
        "            n_independent=params.get('n_independent', 2),\n",
        "            n_shared=params.get('n_shared', 2),\n",
        "            lambda_sparse=params.get('lambda_sparse', 1e-4),\n",
        "            optimizer_fn=torch.optim.Adam,\n",
        "            optimizer_params={\n",
        "                'lr': params.get('learning_rate', 0.02),\n",
        "                'weight_decay': params.get('weight_decay', 1e-5)\n",
        "            },\n",
        "            mask_type='entmax',\n",
        "            verbose=0,\n",
        "            seed=42\n",
        "        )\n",
        "    \n",
        "    # 학습\n",
        "    start_time = time.time()\n",
        "    \n",
        "    model.fit(\n",
        "        X_train=data_dict['X_train'],\n",
        "        y_train=data_dict['y_train'],\n",
        "        eval_set=[(data_dict['X_val'], data_dict['y_val'])],\n",
        "        eval_metric=['rmse'] if task == 'regression' else ['logloss', 'accuracy'],\n",
        "        max_epochs=200,\n",
        "        patience=30,\n",
        "        batch_size=min(512, len(data_dict['X_train']) // 4),\n",
        "        virtual_batch_size=128,\n",
        "        num_workers=0,\n",
        "        drop_last=False\n",
        "    )\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    return model, training_time\n",
        "\n",
        "def evaluate_model(model, data_dict, task='regression'):\n",
        "    \"\"\"모델 성능 평가\"\"\"\n",
        "    \n",
        "    # 예측\n",
        "    y_pred = model.predict(data_dict['X_test'])\n",
        "    \n",
        "    if task == 'regression':\n",
        "        # 역변환\n",
        "        if data_dict['y_scaler'] is not None:\n",
        "            y_pred_orig = data_dict['y_scaler'].inverse_transform(y_pred.reshape(-1,1)).ravel()\n",
        "            y_true_orig = data_dict['y_test_orig'].values\n",
        "        else:\n",
        "            y_pred_orig = y_pred\n",
        "            y_true_orig = data_dict['y_test']\n",
        "        \n",
        "        # 회귀 지표\n",
        "        r2 = r2_score(y_true_orig, y_pred_orig)\n",
        "        rmse = np.sqrt(mean_squared_error(y_true_orig, y_pred_orig))\n",
        "        mae = mean_absolute_error(y_true_orig, y_pred_orig)\n",
        "        \n",
        "        # MAPE, SMAPE 계산\n",
        "        def mean_absolute_percentage_error(y_true, y_pred):\n",
        "            return np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
        "        \n",
        "        def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
        "            return np.mean(2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred) + 1e-8)) * 100\n",
        "        \n",
        "        mape = mean_absolute_percentage_error(y_true_orig, y_pred_orig)\n",
        "        smape = symmetric_mean_absolute_percentage_error(y_true_orig, y_pred_orig)\n",
        "        \n",
        "        metrics = {\n",
        "            'R2': r2, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'SMAPE': smape,\n",
        "            'y_pred': y_pred_orig, 'y_true': y_true_orig\n",
        "        }\n",
        "        \n",
        "    else:  # classification\n",
        "        y_true = data_dict['y_test']\n",
        "        \n",
        "        # 분류 지표\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
        "        f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "        \n",
        "        # ROC-AUC (다중클래스)\n",
        "        try:\n",
        "            y_pred_proba = model.predict_proba(data_dict['X_test'])\n",
        "            if len(np.unique(y_true)) == 2:\n",
        "                roc_auc = roc_auc_score(y_true, y_pred_proba[:, 1])\n",
        "            else:\n",
        "                roc_auc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr')\n",
        "        except:\n",
        "            roc_auc = None\n",
        "        \n",
        "        metrics = {\n",
        "            'Accuracy': accuracy, 'F1_weighted': f1_weighted, \n",
        "            'F1_macro': f1_macro, 'ROC_AUC': roc_auc,\n",
        "            'y_pred': y_pred, 'y_true': y_true\n",
        "        }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# ================================================================\n",
        "# 5. 하이퍼파라미터 튜닝\n",
        "# ================================================================\n",
        "\n",
        "def hyperparameter_tuning(center_name, data_dict, task='regression', max_trials=20):\n",
        "    \"\"\"하이퍼파라미터 튜닝 (Random Search)\"\"\"\n",
        "    \n",
        "    print(f\"🔧 {center_name} - {task} 하이퍼파라미터 튜닝 시작...\")\n",
        "    \n",
        "    param_grid = get_tabnet_param_grid()\n",
        "    param_combinations = list(ParameterGrid(param_grid))\n",
        "    \n",
        "    # 최대 시도 횟수 제한\n",
        "    if len(param_combinations) > max_trials:\n",
        "        import random\n",
        "        param_combinations = random.sample(param_combinations, max_trials)\n",
        "    \n",
        "    best_score = -np.inf if task == 'regression' else -np.inf\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "    results = []\n",
        "    \n",
        "    for i, params in enumerate(param_combinations):\n",
        "        try:\n",
        "            print(f\"  시도 {i+1}/{len(param_combinations)}: {params}\")\n",
        "            \n",
        "            # 모델 학습\n",
        "            model, train_time = train_tabnet_model(data_dict, task, **params)\n",
        "            \n",
        "            # 검증셋 평가\n",
        "            val_data_dict = {\n",
        "                'X_test': data_dict['X_val'],\n",
        "                'y_test': data_dict['y_val'] if task == 'classification' else data_dict['y_val_orig'],\n",
        "                'y_scaler': data_dict['y_scaler']\n",
        "            }\n",
        "            \n",
        "            if task == 'regression':\n",
        "                val_data_dict['y_test_orig'] = data_dict['y_val_orig']\n",
        "            \n",
        "            metrics = evaluate_model(model, val_data_dict, task)\n",
        "            \n",
        "            # 점수 기준\n",
        "            score = metrics['R2'] if task == 'regression' else metrics['F1_weighted']\n",
        "            \n",
        "            # 결과 저장\n",
        "            result = {\n",
        "                'center': center_name, 'task': task, 'trial': i+1,\n",
        "                'train_time': train_time, 'score': score, **params, **metrics\n",
        "            }\n",
        "            results.append(result)\n",
        "            \n",
        "            # 베스트 모델 업데이트\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_params = params.copy()\n",
        "                best_model = model\n",
        "                print(f\"    ✅ 새로운 베스트: {score:.4f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"    ❌ 실패: {str(e)}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"🏆 {center_name} - {task} 튜닝 완료: Best Score = {best_score:.4f}\")\n",
        "    \n",
        "    return best_model, best_params, best_score, results\n",
        "\n",
        "# ================================================================\n",
        "# 6. 전체 파이프라인 실행\n",
        "# ================================================================\n",
        "\n",
        "def run_tabnet_pipeline():\n",
        "    \"\"\"전체 TabNet 파이프라인 실행\"\"\"\n",
        "    \n",
        "    centers = ['nanji', 'jungnang', 'seonam', 'tancheon']\n",
        "    tasks = ['regression', 'classification']\n",
        "    \n",
        "    all_results = []\n",
        "    best_models_info = []\n",
        "    \n",
        "    for center in centers:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"🏢 {center.upper()} 센터 처리 시작\")\n",
        "        print('='*60)\n",
        "        \n",
        "        # 데이터 로드\n",
        "        data = load_and_preprocess_data(center)\n",
        "        if data is None:\n",
        "            continue\n",
        "        \n",
        "        # 데이터 분할\n",
        "        train_data, val_data, test_data = temporal_split_data(data)\n",
        "        print(f\"📊 데이터 분할 완료: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}\")\n",
        "        \n",
        "        for task in tasks:\n",
        "            print(f\"\\n🎯 {task.upper()} 태스크 시작\")\n",
        "            \n",
        "            try:\n",
        "                # 피처/타겟 준비\n",
        "                data_dict = prepare_features_target(train_data, val_data, test_data, task)\n",
        "                \n",
        "                # 하이퍼파라미터 튜닝\n",
        "                best_model, best_params, best_score, tuning_results = hyperparameter_tuning(\n",
        "                    center, data_dict, task, max_trials=15\n",
        "                )\n",
        "                \n",
        "                if best_model is not None:\n",
        "                    # 테스트셋 최종 평가\n",
        "                    final_metrics = evaluate_model(best_model, data_dict, task)\n",
        "                    \n",
        "                    # 베스트 모델 정보 저장\n",
        "                    best_info = {\n",
        "                        'center': center, 'task': task, 'best_score': best_score,\n",
        "                        'final_score': final_metrics.get('R2' if task == 'regression' else 'F1_weighted'),\n",
        "                        'best_params': best_params, **final_metrics\n",
        "                    }\n",
        "                    best_models_info.append(best_info)\n",
        "                    \n",
        "                    # 모델 저장\n",
        "                    model_filename = f\"../results_tabnet/models/{center}_{task}_tabnet_best.pkl\"\n",
        "                    with open(model_filename, 'wb') as f:\n",
        "                        pickle.dump({\n",
        "                            'model': best_model,\n",
        "                            'params': best_params,\n",
        "                            'metrics': final_metrics,\n",
        "                            'data_info': data_dict,\n",
        "                            'center': center,\n",
        "                            'task': task\n",
        "                        }, f)\n",
        "                    \n",
        "                    print(f\"💾 모델 저장 완료: {model_filename}\")\n",
        "                \n",
        "                # 튜닝 결과 추가\n",
        "                all_results.extend(tuning_results)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"❌ {center} - {task} 처리 실패: {str(e)}\")\n",
        "                continue\n",
        "    \n",
        "    # 결과 저장\n",
        "    if all_results:\n",
        "        results_df = pd.DataFrame(all_results)\n",
        "        results_df.to_csv('../results_tabnet/tabnet_hyperparameter_results.csv', \n",
        "                         index=False, encoding='utf-8-sig')\n",
        "    \n",
        "    if best_models_info:\n",
        "        best_df = pd.DataFrame(best_models_info)\n",
        "        best_df.to_csv('../results_tabnet/tabnet_best_models.csv', \n",
        "                      index=False, encoding='utf-8-sig')\n",
        "    \n",
        "    print(f\"\\n🎉 TabNet 파이프라인 완료!\")\n",
        "    print(f\"📁 결과 파일:\")\n",
        "    print(f\"  - tabnet_hyperparameter_results.csv\")\n",
        "    print(f\"  - tabnet_best_models.csv\") \n",
        "    print(f\"  - 모델 파일들: ../results_tabnet/models/\")\n",
        "    \n",
        "    return results_df, best_df\n",
        "\n",
        "# ================================================================\n",
        "# 7. 실행\n",
        "# ================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results, best_models = run_tabnet_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64628a87",
      "metadata": {
        "id": "64628a87"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4fcf7849",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔥 TabNet 전용 머신러닝 파이프라인 시작\n",
            "⏰ 시작 시간: 2025-08-31 15:30:13\n",
            "\n",
            "============================================================\n",
            "🏢 NANJI 센터 처리 시작\n",
            "============================================================\n",
            "✅ nanji 데이터 로드 완료: (3069, 38)\n",
            "📊 데이터 분할 완료: Train=2148, Val=614, Test=307\n",
            "\n",
            "🎯 REGRESSION 태스크 시작\n",
            "🔧 nanji - regression 하이퍼파라미터 튜닝 시작...\n",
            "  시도 1/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 88 with best_epoch = 38 and best_val_0_rmse = 0.93037\n",
            "    ✅ 새로운 베스트: 0.4614\n",
            "  시도 2/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 161 with best_epoch = 111 and best_val_0_rmse = 0.95365\n",
            "  시도 3/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 114 with best_epoch = 64 and best_val_0_rmse = 0.89758\n",
            "    ✅ 새로운 베스트: 0.4987\n",
            "  시도 4/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 90 with best_epoch = 40 and best_val_0_rmse = 0.86967\n",
            "    ✅ 새로운 베스트: 0.5293\n",
            "  시도 5/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 124 with best_epoch = 74 and best_val_0_rmse = 0.95406\n",
            "  시도 6/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 81 with best_epoch = 31 and best_val_0_rmse = 0.85824\n",
            "    ✅ 새로운 베스트: 0.5416\n",
            "  시도 7/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 144 with best_epoch = 94 and best_val_0_rmse = 0.87104\n",
            "  시도 8/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 100 with best_epoch = 50 and best_val_0_rmse = 0.88281\n",
            "  시도 9/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 165 with best_epoch = 115 and best_val_0_rmse = 1.01716\n",
            "  시도 10/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 144 with best_epoch = 94 and best_val_0_rmse = 0.82272\n",
            "    ✅ 새로운 베스트: 0.5788\n",
            "  시도 11/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 129 with best_epoch = 79 and best_val_0_rmse = 0.98815\n",
            "  시도 12/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 111 with best_epoch = 61 and best_val_0_rmse = 0.84631\n",
            "  시도 13/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 134 with best_epoch = 84 and best_val_0_rmse = 0.89694\n",
            "  시도 14/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 117 with best_epoch = 67 and best_val_0_rmse = 0.8475\n",
            "  시도 15/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 99 with best_epoch = 49 and best_val_0_rmse = 0.95399\n",
            "  시도 16/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 150 with best_epoch = 100 and best_val_0_rmse = 0.86892\n",
            "  시도 17/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 185 with best_epoch = 135 and best_val_0_rmse = 0.95561\n",
            "  시도 18/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 123 with best_epoch = 73 and best_val_0_rmse = 1.05348\n",
            "  시도 19/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 223 with best_epoch = 173 and best_val_0_rmse = 0.89895\n",
            "  시도 20/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 158 with best_epoch = 108 and best_val_0_rmse = 1.01124\n",
            "  시도 21/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 98 with best_epoch = 48 and best_val_0_rmse = 1.11475\n",
            "  시도 22/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 137 with best_epoch = 87 and best_val_0_rmse = 0.86252\n",
            "  시도 23/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 117 with best_epoch = 67 and best_val_0_rmse = 0.90893\n",
            "  시도 24/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 105 with best_epoch = 55 and best_val_0_rmse = 0.98465\n",
            "  시도 25/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 141 with best_epoch = 91 and best_val_0_rmse = 0.85282\n",
            "🏆 nanji - regression 튜닝 완료: Best Score = 0.5788\n",
            "💾 모델 저장 완료: ../results_tabnet/models/nanji_regression_tabnet_best.pkl\n",
            "\n",
            "🎯 CLASSIFICATION 태스크 시작\n",
            "🔧 nanji - classification 하이퍼파라미터 튜닝 시작...\n",
            "  시도 1/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 73 with best_epoch = 23 and best_val_0_accuracy = 0.67427\n",
            "    ✅ 새로운 베스트: 0.6366\n",
            "  시도 2/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 128 with best_epoch = 78 and best_val_0_accuracy = 0.6645\n",
            "    ✅ 새로운 베스트: 0.6450\n",
            "  시도 3/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 70 with best_epoch = 20 and best_val_0_accuracy = 0.67101\n",
            "  시도 4/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 58 with best_epoch = 8 and best_val_0_accuracy = 0.63518\n",
            "  시도 5/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 72 with best_epoch = 22 and best_val_0_accuracy = 0.61889\n",
            "  시도 6/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 105 with best_epoch = 55 and best_val_0_accuracy = 0.64495\n",
            "  시도 7/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 83 with best_epoch = 33 and best_val_0_accuracy = 0.61889\n",
            "  시도 8/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 132 with best_epoch = 82 and best_val_0_accuracy = 0.63518\n",
            "  시도 9/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 113 with best_epoch = 63 and best_val_0_accuracy = 0.67264\n",
            "  시도 10/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 74 with best_epoch = 24 and best_val_0_accuracy = 0.65961\n",
            "  시도 11/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 107 with best_epoch = 57 and best_val_0_accuracy = 0.64169\n",
            "  시도 12/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 60 with best_epoch = 10 and best_val_0_accuracy = 0.64495\n",
            "  시도 13/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 67 with best_epoch = 17 and best_val_0_accuracy = 0.58795\n",
            "  시도 14/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 14 and best_val_0_accuracy = 0.62215\n",
            "  시도 15/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 103 with best_epoch = 53 and best_val_0_accuracy = 0.63029\n",
            "  시도 16/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 83 with best_epoch = 33 and best_val_0_accuracy = 0.63518\n",
            "  시도 17/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 63 with best_epoch = 13 and best_val_0_accuracy = 0.63355\n",
            "  시도 18/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 66 with best_epoch = 16 and best_val_0_accuracy = 0.66938\n",
            "    ✅ 새로운 베스트: 0.6472\n",
            "  시도 19/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 94 with best_epoch = 44 and best_val_0_accuracy = 0.64821\n",
            "  시도 20/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 86 with best_epoch = 36 and best_val_0_accuracy = 0.65147\n",
            "  시도 21/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 89 with best_epoch = 39 and best_val_0_accuracy = 0.64169\n",
            "  시도 22/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 91 with best_epoch = 41 and best_val_0_accuracy = 0.58795\n",
            "  시도 23/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 98 with best_epoch = 48 and best_val_0_accuracy = 0.64658\n",
            "  시도 24/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 69 with best_epoch = 19 and best_val_0_accuracy = 0.63844\n",
            "  시도 25/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 82 with best_epoch = 32 and best_val_0_accuracy = 0.68404\n",
            "    ✅ 새로운 베스트: 0.6487\n",
            "🏆 nanji - classification 튜닝 완료: Best Score = 0.6487\n",
            "💾 모델 저장 완료: ../results_tabnet/models/nanji_classification_tabnet_best.pkl\n",
            "\n",
            "============================================================\n",
            "🏢 JUNGNANG 센터 처리 시작\n",
            "============================================================\n",
            "✅ jungnang 데이터 로드 완료: (3069, 40)\n",
            "📊 데이터 분할 완료: Train=2148, Val=614, Test=307\n",
            "\n",
            "🎯 REGRESSION 태스크 시작\n",
            "🔧 jungnang - regression 하이퍼파라미터 튜닝 시작...\n",
            "  시도 1/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 101 with best_epoch = 51 and best_val_0_rmse = 0.80118\n",
            "    ✅ 새로운 베스트: 0.1472\n",
            "  시도 2/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 116 with best_epoch = 66 and best_val_0_rmse = 0.67293\n",
            "    ✅ 새로운 베스트: 0.3984\n",
            "  시도 3/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 173 with best_epoch = 123 and best_val_0_rmse = 0.64473\n",
            "    ✅ 새로운 베스트: 0.4478\n",
            "  시도 4/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 105 with best_epoch = 55 and best_val_0_rmse = 0.70807\n",
            "  시도 5/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 98 with best_epoch = 48 and best_val_0_rmse = 0.64963\n",
            "  시도 6/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 162 with best_epoch = 112 and best_val_0_rmse = 0.65658\n",
            "  시도 7/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 137 with best_epoch = 87 and best_val_0_rmse = 0.68358\n",
            "  시도 8/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 70 with best_epoch = 20 and best_val_0_rmse = 0.71318\n",
            "  시도 9/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 177 with best_epoch = 127 and best_val_0_rmse = 0.63988\n",
            "    ✅ 새로운 베스트: 0.4560\n",
            "  시도 10/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 173 with best_epoch = 123 and best_val_0_rmse = 0.68559\n",
            "  시도 11/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 160 with best_epoch = 110 and best_val_0_rmse = 0.66869\n",
            "  시도 12/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 142 with best_epoch = 92 and best_val_0_rmse = 0.70229\n",
            "  시도 13/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 151 with best_epoch = 101 and best_val_0_rmse = 0.69963\n",
            "  시도 14/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 75 with best_epoch = 25 and best_val_0_rmse = 0.69447\n",
            "  시도 15/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 74 with best_epoch = 24 and best_val_0_rmse = 0.72517\n",
            "  시도 16/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 197 with best_epoch = 147 and best_val_0_rmse = 0.72494\n",
            "  시도 17/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 248 with best_epoch = 198 and best_val_0_rmse = 0.67627\n",
            "  시도 18/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 201 with best_epoch = 151 and best_val_0_rmse = 0.69974\n",
            "  시도 19/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 147 with best_epoch = 97 and best_val_0_rmse = 0.68752\n",
            "  시도 20/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 95 with best_epoch = 45 and best_val_0_rmse = 0.6667\n",
            "  시도 21/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 170 with best_epoch = 120 and best_val_0_rmse = 0.70048\n",
            "  시도 22/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 138 with best_epoch = 88 and best_val_0_rmse = 0.66118\n",
            "  시도 23/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 178 with best_epoch = 128 and best_val_0_rmse = 0.69458\n",
            "  시도 24/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 122 with best_epoch = 72 and best_val_0_rmse = 0.70389\n",
            "  시도 25/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 95 with best_epoch = 45 and best_val_0_rmse = 0.6385\n",
            "    ✅ 새로운 베스트: 0.4584\n",
            "🏆 jungnang - regression 튜닝 완료: Best Score = 0.4584\n",
            "💾 모델 저장 완료: ../results_tabnet/models/jungnang_regression_tabnet_best.pkl\n",
            "\n",
            "🎯 CLASSIFICATION 태스크 시작\n",
            "🔧 jungnang - classification 하이퍼파라미터 튜닝 시작...\n",
            "  시도 1/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 55 with best_epoch = 5 and best_val_0_accuracy = 0.64658\n",
            "    ✅ 새로운 베스트: 0.5928\n",
            "  시도 2/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 99 with best_epoch = 49 and best_val_0_accuracy = 0.64169\n",
            "    ✅ 새로운 베스트: 0.6163\n",
            "  시도 3/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 69 with best_epoch = 19 and best_val_0_accuracy = 0.63192\n",
            "  시도 4/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 67 with best_epoch = 17 and best_val_0_accuracy = 0.62378\n",
            "  시도 5/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 57 with best_epoch = 7 and best_val_0_accuracy = 0.64007\n",
            "  시도 6/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 109 with best_epoch = 59 and best_val_0_accuracy = 0.61726\n",
            "  시도 7/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 89 with best_epoch = 39 and best_val_0_accuracy = 0.61889\n",
            "  시도 8/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 65 with best_epoch = 15 and best_val_0_accuracy = 0.60749\n",
            "  시도 9/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 67 with best_epoch = 17 and best_val_0_accuracy = 0.63844\n",
            "  시도 10/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 57 with best_epoch = 7 and best_val_0_accuracy = 0.63029\n",
            "  시도 11/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 80 with best_epoch = 30 and best_val_0_accuracy = 0.63192\n",
            "  시도 12/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 9 and best_val_0_accuracy = 0.65635\n",
            "    ✅ 새로운 베스트: 0.6276\n",
            "  시도 13/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 101 with best_epoch = 51 and best_val_0_accuracy = 0.62866\n",
            "  시도 14/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 63 with best_epoch = 13 and best_val_0_accuracy = 0.62378\n",
            "  시도 15/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 70 with best_epoch = 20 and best_val_0_accuracy = 0.64007\n",
            "  시도 16/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 58 with best_epoch = 8 and best_val_0_accuracy = 0.64658\n",
            "  시도 17/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 68 with best_epoch = 18 and best_val_0_accuracy = 0.62704\n",
            "  시도 18/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 65 with best_epoch = 15 and best_val_0_accuracy = 0.63518\n",
            "  시도 19/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 55 with best_epoch = 5 and best_val_0_accuracy = 0.61889\n",
            "  시도 20/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 54 with best_epoch = 4 and best_val_0_accuracy = 0.61075\n",
            "  시도 21/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 60 with best_epoch = 10 and best_val_0_accuracy = 0.63192\n",
            "  시도 22/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 58 with best_epoch = 8 and best_val_0_accuracy = 0.65147\n",
            "  시도 23/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 56 with best_epoch = 6 and best_val_0_accuracy = 0.66287\n",
            "    ✅ 새로운 베스트: 0.6354\n",
            "  시도 24/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 63 with best_epoch = 13 and best_val_0_accuracy = 0.65635\n",
            "  시도 25/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 105 with best_epoch = 55 and best_val_0_accuracy = 0.61889\n",
            "🏆 jungnang - classification 튜닝 완료: Best Score = 0.6354\n",
            "💾 모델 저장 완료: ../results_tabnet/models/jungnang_classification_tabnet_best.pkl\n",
            "\n",
            "============================================================\n",
            "🏢 SEONAM 센터 처리 시작\n",
            "============================================================\n",
            "✅ seonam 데이터 로드 완료: (3069, 39)\n",
            "📊 데이터 분할 완료: Train=2148, Val=614, Test=307\n",
            "\n",
            "🎯 REGRESSION 태스크 시작\n",
            "🔧 seonam - regression 하이퍼파라미터 튜닝 시작...\n",
            "  시도 1/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 123 with best_epoch = 73 and best_val_0_rmse = 0.77829\n",
            "    ✅ 새로운 베스트: 0.1219\n",
            "  시도 2/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 157 with best_epoch = 107 and best_val_0_rmse = 0.79503\n",
            "  시도 3/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 127 with best_epoch = 77 and best_val_0_rmse = 0.65973\n",
            "    ✅ 새로운 베스트: 0.3691\n",
            "  시도 4/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 168 with best_epoch = 118 and best_val_0_rmse = 0.73091\n",
            "  시도 5/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 118 with best_epoch = 68 and best_val_0_rmse = 0.64005\n",
            "    ✅ 새로운 베스트: 0.4061\n",
            "  시도 6/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 97 with best_epoch = 47 and best_val_0_rmse = 0.65893\n",
            "  시도 7/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 166 with best_epoch = 116 and best_val_0_rmse = 0.76723\n",
            "  시도 8/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 97 with best_epoch = 47 and best_val_0_rmse = 0.70001\n",
            "  시도 9/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 148 with best_epoch = 98 and best_val_0_rmse = 0.70496\n",
            "  시도 10/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 147 with best_epoch = 97 and best_val_0_rmse = 0.77849\n",
            "  시도 11/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 114 with best_epoch = 64 and best_val_0_rmse = 0.68538\n",
            "  시도 12/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 179 with best_epoch = 129 and best_val_0_rmse = 0.64766\n",
            "  시도 13/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 144 with best_epoch = 94 and best_val_0_rmse = 0.69568\n",
            "  시도 14/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 167 with best_epoch = 117 and best_val_0_rmse = 0.65496\n",
            "  시도 15/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 175 with best_epoch = 125 and best_val_0_rmse = 0.77108\n",
            "  시도 16/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 123 with best_epoch = 73 and best_val_0_rmse = 0.74392\n",
            "  시도 17/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 187 with best_epoch = 137 and best_val_0_rmse = 0.6627\n",
            "  시도 18/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 121 with best_epoch = 71 and best_val_0_rmse = 0.74174\n",
            "  시도 19/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 126 with best_epoch = 76 and best_val_0_rmse = 0.62874\n",
            "    ✅ 새로운 베스트: 0.4269\n",
            "  시도 20/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 154 with best_epoch = 104 and best_val_0_rmse = 0.61816\n",
            "    ✅ 새로운 베스트: 0.4461\n",
            "  시도 21/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 97 with best_epoch = 47 and best_val_0_rmse = 0.72887\n",
            "  시도 22/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 230 with best_epoch = 180 and best_val_0_rmse = 0.71683\n",
            "  시도 23/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 215 with best_epoch = 165 and best_val_0_rmse = 0.70846\n",
            "  시도 24/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 135 with best_epoch = 85 and best_val_0_rmse = 0.74729\n",
            "  시도 25/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 130 with best_epoch = 80 and best_val_0_rmse = 0.6836\n",
            "🏆 seonam - regression 튜닝 완료: Best Score = 0.4461\n",
            "💾 모델 저장 완료: ../results_tabnet/models/seonam_regression_tabnet_best.pkl\n",
            "\n",
            "🎯 CLASSIFICATION 태스크 시작\n",
            "🔧 seonam - classification 하이퍼파라미터 튜닝 시작...\n",
            "  시도 1/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 75 with best_epoch = 25 and best_val_0_accuracy = 0.54397\n",
            "    ✅ 새로운 베스트: 0.4860\n",
            "  시도 2/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 72 with best_epoch = 22 and best_val_0_accuracy = 0.56678\n",
            "    ✅ 새로운 베스트: 0.5269\n",
            "  시도 3/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 55 with best_epoch = 5 and best_val_0_accuracy = 0.57818\n",
            "  시도 4/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 82 with best_epoch = 32 and best_val_0_accuracy = 0.53746\n",
            "  시도 5/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 55 with best_epoch = 5 and best_val_0_accuracy = 0.58958\n",
            "  시도 6/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 54 with best_epoch = 4 and best_val_0_accuracy = 0.57818\n",
            "  시도 7/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 133 with best_epoch = 83 and best_val_0_accuracy = 0.53909\n",
            "  시도 8/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 52 with best_epoch = 2 and best_val_0_accuracy = 0.56352\n",
            "  시도 9/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 94 with best_epoch = 44 and best_val_0_accuracy = 0.58958\n",
            "    ✅ 새로운 베스트: 0.5403\n",
            "  시도 10/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 72 with best_epoch = 22 and best_val_0_accuracy = 0.50814\n",
            "  시도 11/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 103 with best_epoch = 53 and best_val_0_accuracy = 0.55049\n",
            "  시도 12/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 72 with best_epoch = 22 and best_val_0_accuracy = 0.58143\n",
            "  시도 13/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 53 with best_epoch = 3 and best_val_0_accuracy = 0.51954\n",
            "  시도 14/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 75 with best_epoch = 25 and best_val_0_accuracy = 0.5684\n",
            "  시도 15/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 82 with best_epoch = 32 and best_val_0_accuracy = 0.55537\n",
            "  시도 16/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 86 with best_epoch = 36 and best_val_0_accuracy = 0.58795\n",
            "    ✅ 새로운 베스트: 0.5883\n",
            "  시도 17/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 116 with best_epoch = 66 and best_val_0_accuracy = 0.54886\n",
            "  시도 18/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 51 with best_epoch = 1 and best_val_0_accuracy = 0.5684\n",
            "  시도 19/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 81 with best_epoch = 31 and best_val_0_accuracy = 0.55049\n",
            "  시도 20/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 56 with best_epoch = 6 and best_val_0_accuracy = 0.57003\n",
            "  시도 21/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 57 with best_epoch = 7 and best_val_0_accuracy = 0.5798\n",
            "  시도 22/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 87 with best_epoch = 37 and best_val_0_accuracy = 0.56189\n",
            "  시도 23/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 52 with best_epoch = 2 and best_val_0_accuracy = 0.58469\n",
            "  시도 24/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 82 with best_epoch = 32 and best_val_0_accuracy = 0.5342\n",
            "  시도 25/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 60 with best_epoch = 10 and best_val_0_accuracy = 0.58795\n",
            "🏆 seonam - classification 튜닝 완료: Best Score = 0.5883\n",
            "💾 모델 저장 완료: ../results_tabnet/models/seonam_classification_tabnet_best.pkl\n",
            "\n",
            "============================================================\n",
            "🏢 TANCHEON 센터 처리 시작\n",
            "============================================================\n",
            "✅ tancheon 데이터 로드 완료: (3069, 38)\n",
            "📊 데이터 분할 완료: Train=2148, Val=614, Test=307\n",
            "\n",
            "🎯 REGRESSION 태스크 시작\n",
            "🔧 tancheon - regression 하이퍼파라미터 튜닝 시작...\n",
            "  시도 1/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 139 with best_epoch = 89 and best_val_0_rmse = 0.87276\n",
            "    ✅ 새로운 베스트: 0.3930\n",
            "  시도 2/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 148 with best_epoch = 98 and best_val_0_rmse = 0.9112\n",
            "  시도 3/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 155 with best_epoch = 105 and best_val_0_rmse = 0.92108\n",
            "  시도 4/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 94 with best_epoch = 44 and best_val_0_rmse = 0.87407\n",
            "  시도 5/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 174 with best_epoch = 124 and best_val_0_rmse = 0.99562\n",
            "  시도 6/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 101 with best_epoch = 51 and best_val_0_rmse = 0.92055\n",
            "  시도 7/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 113 with best_epoch = 63 and best_val_0_rmse = 1.03024\n",
            "  시도 8/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 107 with best_epoch = 57 and best_val_0_rmse = 0.93345\n",
            "  시도 9/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 91 with best_epoch = 41 and best_val_0_rmse = 0.92384\n",
            "  시도 10/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 109 with best_epoch = 59 and best_val_0_rmse = 1.10369\n",
            "  시도 11/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 125 with best_epoch = 75 and best_val_0_rmse = 0.8977\n",
            "  시도 12/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 219 with best_epoch = 169 and best_val_0_rmse = 1.11916\n",
            "  시도 13/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 79 with best_epoch = 29 and best_val_0_rmse = 1.01983\n",
            "  시도 14/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 156 with best_epoch = 106 and best_val_0_rmse = 0.91647\n",
            "  시도 15/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 102 with best_epoch = 52 and best_val_0_rmse = 0.99606\n",
            "  시도 16/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 107 with best_epoch = 57 and best_val_0_rmse = 0.93795\n",
            "  시도 17/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 80 with best_epoch = 30 and best_val_0_rmse = 0.94313\n",
            "  시도 18/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 175 with best_epoch = 125 and best_val_0_rmse = 0.95735\n",
            "  시도 19/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 81 with best_epoch = 31 and best_val_0_rmse = 0.97559\n",
            "  시도 20/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 110 with best_epoch = 60 and best_val_0_rmse = 0.88833\n",
            "  시도 21/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 72 with best_epoch = 22 and best_val_0_rmse = 0.87885\n",
            "  시도 22/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 95 with best_epoch = 45 and best_val_0_rmse = 0.92697\n",
            "  시도 23/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 122 with best_epoch = 72 and best_val_0_rmse = 1.01422\n",
            "  시도 24/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 124 with best_epoch = 74 and best_val_0_rmse = 0.95257\n",
            "  시도 25/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 138 with best_epoch = 88 and best_val_0_rmse = 0.93888\n",
            "🏆 tancheon - regression 튜닝 완료: Best Score = 0.3930\n",
            "💾 모델 저장 완료: ../results_tabnet/models/tancheon_regression_tabnet_best.pkl\n",
            "\n",
            "🎯 CLASSIFICATION 태스크 시작\n",
            "🔧 tancheon - classification 하이퍼파라미터 튜닝 시작...\n",
            "  시도 1/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 68 with best_epoch = 18 and best_val_0_accuracy = 0.59609\n",
            "    ✅ 새로운 베스트: 0.5364\n",
            "  시도 2/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 104 with best_epoch = 54 and best_val_0_accuracy = 0.58632\n",
            "    ✅ 새로운 베스트: 0.5585\n",
            "  시도 3/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 73 with best_epoch = 23 and best_val_0_accuracy = 0.59283\n",
            "  시도 4/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 175 with best_epoch = 125 and best_val_0_accuracy = 0.59609\n",
            "    ✅ 새로운 베스트: 0.5680\n",
            "  시도 5/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 75 with best_epoch = 25 and best_val_0_accuracy = 0.57655\n",
            "  시도 6/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 98 with best_epoch = 48 and best_val_0_accuracy = 0.58958\n",
            "  시도 7/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 80 with best_epoch = 30 and best_val_0_accuracy = 0.60749\n",
            "    ✅ 새로운 베스트: 0.5748\n",
            "  시도 8/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 77 with best_epoch = 27 and best_val_0_accuracy = 0.59283\n",
            "  시도 9/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 105 with best_epoch = 55 and best_val_0_accuracy = 0.56515\n",
            "  시도 10/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 105 with best_epoch = 55 and best_val_0_accuracy = 0.57818\n",
            "  시도 11/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 73 with best_epoch = 23 and best_val_0_accuracy = 0.5684\n",
            "  시도 12/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 121 with best_epoch = 71 and best_val_0_accuracy = 0.61564\n",
            "  시도 13/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 90 with best_epoch = 40 and best_val_0_accuracy = 0.60423\n",
            "    ✅ 새로운 베스트: 0.5853\n",
            "  시도 14/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 75 with best_epoch = 25 and best_val_0_accuracy = 0.57818\n",
            "  시도 15/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 70 with best_epoch = 20 and best_val_0_accuracy = 0.60749\n",
            "  시도 16/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 145 with best_epoch = 95 and best_val_0_accuracy = 0.60261\n",
            "  시도 17/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 108 with best_epoch = 58 and best_val_0_accuracy = 0.58632\n",
            "  시도 18/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 75 with best_epoch = 25 and best_val_0_accuracy = 0.60749\n",
            "  시도 19/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 64 with best_epoch = 14 and best_val_0_accuracy = 0.56678\n",
            "  시도 20/25: {'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 84 with best_epoch = 34 and best_val_0_accuracy = 0.58306\n",
            "  시도 21/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 75 with best_epoch = 25 and best_val_0_accuracy = 0.58469\n",
            "  시도 22/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 105 with best_epoch = 55 and best_val_0_accuracy = 0.57329\n",
            "  시도 23/25: {'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 4, 'weight_decay': 1e-05}\n",
            "\n",
            "Early stopping occurred at epoch 105 with best_epoch = 55 and best_val_0_accuracy = 0.56678\n",
            "  시도 24/25: {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 90 with best_epoch = 40 and best_val_0_accuracy = 0.57492\n",
            "  시도 25/25: {'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 32, 'n_independent': 2, 'n_shared': 2, 'n_steps': 5, 'weight_decay': 1e-06}\n",
            "\n",
            "Early stopping occurred at epoch 148 with best_epoch = 98 and best_val_0_accuracy = 0.58795\n",
            "🏆 tancheon - classification 튜닝 완료: Best Score = 0.5853\n",
            "💾 모델 저장 완료: ../results_tabnet/models/tancheon_classification_tabnet_best.pkl\n",
            "\n",
            "🎉 TabNet 파이프라인 완료!\n",
            "📁 결과 파일:\n",
            "  - tabnet_hyperparameter_results.csv\n",
            "  - tabnet_best_models.csv\n",
            "  - 모델 파일들: ../results_tabnet/models/\n"
          ]
        }
      ],
      "source": [
        "# TabNet 전용 머신러닝 파이프라인\n",
        "# ================================================================\n",
        "# 하수처리량 예측 - TabNet 딥러닝 모델 전용 파이프라인\n",
        "# ================================================================\n",
        "\n",
        "import os, warnings, time, pickle\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from itertools import product\n",
        "\n",
        "# TabNet\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor, TabNetClassifier\n",
        "import torch\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    mean_absolute_error, mean_squared_error, r2_score,\n",
        "    accuracy_score, f1_score, roc_auc_score, classification_report\n",
        ")\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# 해석도구\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SHAP_AVAILABLE = False\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 설정\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['font.family'] = 'AppleGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 디렉토리 생성\n",
        "directories = [\n",
        "    '../results_tabnet',\n",
        "    '../results_tabnet/models', \n",
        "    '../results_tabnet/visualizations',\n",
        "    '../results_tabnet/interpretability',\n",
        "    '../results_tabnet/hyperparams'\n",
        "]\n",
        "for directory in directories:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "print(\"🔥 TabNet 전용 머신러닝 파이프라인 시작\")\n",
        "print(f\"⏰ 시작 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# ================================================================\n",
        "# 1. 데이터 로드 및 전처리\n",
        "# ================================================================\n",
        "\n",
        "def load_and_preprocess_data(center_name):\n",
        "    \"\"\"센터별 데이터 로드 및 전처리\"\"\"\n",
        "    file_path = f'../data/add_feature/{center_name}_add_feature.csv'\n",
        "    \n",
        "    try:\n",
        "        data = pd.read_csv(file_path, encoding='utf-8-sig')\n",
        "        \n",
        "        # 불필요한 컬럼 제거\n",
        "        drop_cols = ['날짜', '요일', '1처리장', '2처리장', '정화조', '중계펌프장']\n",
        "        existing_drop_cols = [col for col in drop_cols if col in data.columns]\n",
        "        data = data.drop(existing_drop_cols, axis=1)\n",
        "        \n",
        "        # 결측치 제거\n",
        "        data = data.dropna()\n",
        "        \n",
        "        print(f\"✅ {center_name} 데이터 로드 완료: {data.shape}\")\n",
        "        return data\n",
        "        \n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ {center_name} 데이터 파일을 찾을 수 없습니다: {file_path}\")\n",
        "        return None\n",
        "\n",
        "# ================================================================\n",
        "# 2. 데이터 분할 (Temporal Split)\n",
        "# ================================================================\n",
        "\n",
        "def temporal_split_data(data, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
        "    \"\"\"시계열 순서를 유지한 데이터 분할\"\"\"\n",
        "    n = len(data)\n",
        "    train_end = int(n * train_ratio)\n",
        "    val_end = int(n * (train_ratio + val_ratio))\n",
        "    \n",
        "    train_data = data.iloc[:train_end].copy()\n",
        "    val_data = data.iloc[train_end:val_end].copy() \n",
        "    test_data = data.iloc[val_end:].copy()\n",
        "    \n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "def prepare_features_target(train_data, val_data, test_data, task='regression'):\n",
        "    \"\"\"피처와 타겟 분리 및 스케일링\"\"\"\n",
        "    \n",
        "    if task == 'regression':\n",
        "        target_col = '합계_1일후'\n",
        "        exclude_cols = [\n",
        "            '1처리장','2처리장','정화조','중계펌프장','합계','시설현대화',\n",
        "            '3처리장','4처리장','합계', '합계_1일후','합계_2일후',\n",
        "            '등급','등급_1일후','등급_2일후'\n",
        "        ]\n",
        "    else:  # classification\n",
        "        target_col = '등급_1일후'\n",
        "        exclude_cols = [\n",
        "            '1처리장','2처리장','정화조','중계펌프장','합계','시설현대화',\n",
        "            '3처리장','4처리장','합계', '합계_1일후','합계_2일후',\n",
        "            '등급','등급_1일후','등급_2일후'\n",
        "        ]\n",
        "    \n",
        "    # 존재하는 컬럼만 제외\n",
        "    existing_exclude_cols = [col for col in exclude_cols if col in train_data.columns]\n",
        "    \n",
        "    # 피처 분리\n",
        "    X_train = train_data.drop(existing_exclude_cols, axis=1)\n",
        "    X_val = val_data.drop(existing_exclude_cols, axis=1)\n",
        "    X_test = test_data.drop(existing_exclude_cols, axis=1)\n",
        "    \n",
        "    # 타겟 분리\n",
        "    y_train = train_data[target_col].copy()\n",
        "    y_val = val_data[target_col].copy()\n",
        "    y_test = test_data[target_col].copy()\n",
        "    \n",
        "    # 피처 스케일링\n",
        "    x_scaler = StandardScaler()\n",
        "    X_train_scaled = x_scaler.fit_transform(X_train).astype(np.float32)\n",
        "    X_val_scaled = x_scaler.transform(X_val).astype(np.float32)\n",
        "    X_test_scaled = x_scaler.transform(X_test).astype(np.float32)\n",
        "    \n",
        "    # 회귀의 경우 타겟도 스케일링\n",
        "    if task == 'regression':\n",
        "        y_scaler = StandardScaler()\n",
        "        y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1,1)).astype(np.float32)\n",
        "        y_val_scaled = y_scaler.transform(y_val.values.reshape(-1,1)).astype(np.float32)\n",
        "        y_test_scaled = y_scaler.transform(y_test.values.reshape(-1,1)).astype(np.float32)\n",
        "        \n",
        "        return {\n",
        "            'X_train': X_train_scaled, 'X_val': X_val_scaled, 'X_test': X_test_scaled,\n",
        "            'y_train': y_train_scaled, 'y_val': y_val_scaled, 'y_test': y_test_scaled,\n",
        "            'y_train_orig': y_train, 'y_val_orig': y_val, 'y_test_orig': y_test,\n",
        "            'x_scaler': x_scaler, 'y_scaler': y_scaler,\n",
        "            'feature_names': X_train.columns.tolist()\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            'X_train': X_train_scaled, 'X_val': X_val_scaled, 'X_test': X_test_scaled,\n",
        "            'y_train': y_train.values.astype(int), 'y_val': y_val.values.astype(int), \n",
        "            'y_test': y_test.values.astype(int),\n",
        "            'x_scaler': x_scaler, 'y_scaler': None,\n",
        "            'feature_names': X_train.columns.tolist()\n",
        "        }\n",
        "\n",
        "# ================================================================\n",
        "# 3. 하이퍼파라미터 설정\n",
        "# ================================================================\n",
        "\n",
        "def get_tabnet_param_grid():\n",
        "    \"\"\"TabNet 하이퍼파라미터 그리드 (기존 성능에 맞춘 조정)\"\"\"\n",
        "    \n",
        "    # 기존 우수 성능을 반영한 파라미터\n",
        "    optimized_params = {\n",
        "        'n_d': [32, 64],  # 기존 32에서 확장\n",
        "        'n_a': [32, 64], \n",
        "        'n_steps': [4, 5],  # 기존 4 중심\n",
        "        'gamma': [1.0, 1.2],  # 기존 1.0 중심\n",
        "        'n_independent': [2],\n",
        "        'n_shared': [2],\n",
        "        'lambda_sparse': [1e-4, 1e-5],\n",
        "        'learning_rate': [0.001, 0.002, 0.005],  # 기존 1e-3 중심으로 낮춤\n",
        "        'weight_decay': [1e-5, 1e-6]  # 기존 1e-5 중심\n",
        "    }\n",
        "    \n",
        "    # 상세 파라미터 (시간 여유 시)\n",
        "    detailed_params = {\n",
        "        'n_d': [16, 32, 64],\n",
        "        'n_a': [16, 32, 64],\n",
        "        'n_steps': [3, 4, 5, 6],\n",
        "        'gamma': [1.0, 1.2, 1.5, 2.0],\n",
        "        'n_independent': [1, 2, 3],\n",
        "        'n_shared': [1, 2, 3], \n",
        "        'lambda_sparse': [0, 1e-4, 1e-3],\n",
        "        'learning_rate': [0.001, 0.002, 0.005, 0.01],\n",
        "        'weight_decay': [1e-4, 1e-5, 1e-6]\n",
        "    }\n",
        "    \n",
        "    return optimized_params  # 최적화된 파라미터 사용\n",
        "\n",
        "# ================================================================\n",
        "# 4. 모델 학습 및 평가\n",
        "# ================================================================\n",
        "\n",
        "def train_tabnet_model(data_dict, task='regression', **params):\n",
        "    \"\"\"TabNet 모델 학습\"\"\"\n",
        "    \n",
        "    # 모델 초기화\n",
        "    if task == 'regression':\n",
        "        model = TabNetRegressor(\n",
        "            n_d=params.get('n_d', 32),\n",
        "            n_a=params.get('n_a', 32),\n",
        "            n_steps=params.get('n_steps', 4),\n",
        "            gamma=params.get('gamma', 1.0),\n",
        "            n_independent=params.get('n_independent', 2),\n",
        "            n_shared=params.get('n_shared', 2),\n",
        "            lambda_sparse=params.get('lambda_sparse', 1e-4),\n",
        "            optimizer_fn=torch.optim.Adam,\n",
        "            optimizer_params={\n",
        "                'lr': params.get('learning_rate', 0.02),\n",
        "                'weight_decay': params.get('weight_decay', 1e-5)\n",
        "            },\n",
        "            mask_type='entmax',\n",
        "            verbose=0,\n",
        "            seed=42\n",
        "        )\n",
        "    else:  # classification\n",
        "        model = TabNetClassifier(\n",
        "            n_d=params.get('n_d', 32),\n",
        "            n_a=params.get('n_a', 32), \n",
        "            n_steps=params.get('n_steps', 4),\n",
        "            gamma=params.get('gamma', 1.0),\n",
        "            n_independent=params.get('n_independent', 2),\n",
        "            n_shared=params.get('n_shared', 2),\n",
        "            lambda_sparse=params.get('lambda_sparse', 1e-4),\n",
        "            optimizer_fn=torch.optim.Adam,\n",
        "            optimizer_params={\n",
        "                'lr': params.get('learning_rate', 0.02),\n",
        "                'weight_decay': params.get('weight_decay', 1e-5)\n",
        "            },\n",
        "            mask_type='entmax',\n",
        "            verbose=0,\n",
        "            seed=42\n",
        "        )\n",
        "    \n",
        "    # 학습\n",
        "    start_time = time.time()\n",
        "    \n",
        "    model.fit(\n",
        "        X_train=data_dict['X_train'],\n",
        "        y_train=data_dict['y_train'],\n",
        "        eval_set=[(data_dict['X_val'], data_dict['y_val'])],\n",
        "        eval_metric=['rmse'] if task == 'regression' else ['logloss', 'accuracy'],\n",
        "        max_epochs=500,  # 기존과 동일하게 증가\n",
        "        patience=50,     # 기존과 동일하게 증가\n",
        "        batch_size=min(512, len(data_dict['X_train']) // 4),\n",
        "        virtual_batch_size=128,\n",
        "        num_workers=0,\n",
        "        drop_last=False\n",
        "    )\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    return model, training_time\n",
        "\n",
        "def evaluate_model(model, data_dict, task='regression'):\n",
        "    \"\"\"모델 성능 평가\"\"\"\n",
        "    \n",
        "    # 예측\n",
        "    y_pred = model.predict(data_dict['X_test'])\n",
        "    \n",
        "    if task == 'regression':\n",
        "        # 역변환\n",
        "        if data_dict['y_scaler'] is not None:\n",
        "            y_pred_orig = data_dict['y_scaler'].inverse_transform(y_pred.reshape(-1,1)).ravel()\n",
        "            y_true_orig = data_dict['y_test_orig'].values\n",
        "        else:\n",
        "            y_pred_orig = y_pred\n",
        "            y_true_orig = data_dict['y_test']\n",
        "        \n",
        "        # 회귀 지표\n",
        "        r2 = r2_score(y_true_orig, y_pred_orig)\n",
        "        rmse = np.sqrt(mean_squared_error(y_true_orig, y_pred_orig))\n",
        "        mae = mean_absolute_error(y_true_orig, y_pred_orig)\n",
        "        \n",
        "        # MAPE, SMAPE 계산\n",
        "        def mean_absolute_percentage_error(y_true, y_pred):\n",
        "            return np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
        "        \n",
        "        def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
        "            return np.mean(2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred) + 1e-8)) * 100\n",
        "        \n",
        "        mape = mean_absolute_percentage_error(y_true_orig, y_pred_orig)\n",
        "        smape = symmetric_mean_absolute_percentage_error(y_true_orig, y_pred_orig)\n",
        "        \n",
        "        metrics = {\n",
        "            'R2': r2, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'SMAPE': smape,\n",
        "            'y_pred': y_pred_orig, 'y_true': y_true_orig\n",
        "        }\n",
        "        \n",
        "    else:  # classification\n",
        "        y_true = data_dict['y_test']\n",
        "        \n",
        "        # 분류 지표\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
        "        f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "        \n",
        "        # ROC-AUC (다중클래스)\n",
        "        try:\n",
        "            y_pred_proba = model.predict_proba(data_dict['X_test'])\n",
        "            if len(np.unique(y_true)) == 2:\n",
        "                roc_auc = roc_auc_score(y_true, y_pred_proba[:, 1])\n",
        "            else:\n",
        "                roc_auc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr')\n",
        "        except:\n",
        "            roc_auc = None\n",
        "        \n",
        "        metrics = {\n",
        "            'Accuracy': accuracy, 'F1_weighted': f1_weighted, \n",
        "            'F1_macro': f1_macro, 'ROC_AUC': roc_auc,\n",
        "            'y_pred': y_pred, 'y_true': y_true\n",
        "        }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# ================================================================\n",
        "# 5. 하이퍼파라미터 튜닝\n",
        "# ================================================================\n",
        "\n",
        "def hyperparameter_tuning(center_name, data_dict, task='regression', max_trials=25):\n",
        "    \"\"\"하이퍼파라미터 튜닝 (Random Search) - 시도 횟수 증가\"\"\"\n",
        "    \n",
        "    print(f\"🔧 {center_name} - {task} 하이퍼파라미터 튜닝 시작...\")\n",
        "    \n",
        "    param_grid = get_tabnet_param_grid()\n",
        "    param_combinations = list(ParameterGrid(param_grid))\n",
        "    \n",
        "    # 최대 시도 횟수 제한 (기존 15에서 25로 증가)\n",
        "    if len(param_combinations) > max_trials:\n",
        "        import random\n",
        "        param_combinations = random.sample(param_combinations, max_trials)\n",
        "    \n",
        "    best_score = -np.inf if task == 'regression' else -np.inf\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "    results = []\n",
        "    \n",
        "    for i, params in enumerate(param_combinations):\n",
        "        try:\n",
        "            print(f\"  시도 {i+1}/{len(param_combinations)}: {params}\")\n",
        "            \n",
        "            # 모델 학습\n",
        "            model, train_time = train_tabnet_model(data_dict, task, **params)\n",
        "            \n",
        "            # 검증셋 평가\n",
        "            val_data_dict = {\n",
        "                'X_test': data_dict['X_val'],\n",
        "                'y_test': data_dict['y_val'] if task == 'classification' else data_dict['y_val_orig'],\n",
        "                'y_scaler': data_dict['y_scaler']\n",
        "            }\n",
        "            \n",
        "            if task == 'regression':\n",
        "                val_data_dict['y_test_orig'] = data_dict['y_val_orig']\n",
        "            \n",
        "            metrics = evaluate_model(model, val_data_dict, task)\n",
        "            \n",
        "            # 점수 기준\n",
        "            score = metrics['R2'] if task == 'regression' else metrics['F1_weighted']\n",
        "            \n",
        "            # 결과 저장\n",
        "            result = {\n",
        "                'center': center_name, 'task': task, 'trial': i+1,\n",
        "                'train_time': train_time, 'score': score, **params, **metrics\n",
        "            }\n",
        "            results.append(result)\n",
        "            \n",
        "            # 베스트 모델 업데이트\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_params = params.copy()\n",
        "                best_model = model\n",
        "                print(f\"    ✅ 새로운 베스트: {score:.4f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"    ❌ 실패: {str(e)}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"🏆 {center_name} - {task} 튜닝 완료: Best Score = {best_score:.4f}\")\n",
        "    \n",
        "    return best_model, best_params, best_score, results\n",
        "\n",
        "# ================================================================\n",
        "# 6. 전체 파이프라인 실행\n",
        "# ================================================================\n",
        "\n",
        "def run_tabnet_pipeline():\n",
        "    \"\"\"전체 TabNet 파이프라인 실행\"\"\"\n",
        "    \n",
        "    centers = ['nanji', 'jungnang', 'seonam', 'tancheon']\n",
        "    tasks = ['regression', 'classification']\n",
        "    \n",
        "    all_results = []\n",
        "    best_models_info = []\n",
        "    \n",
        "    for center in centers:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"🏢 {center.upper()} 센터 처리 시작\")\n",
        "        print('='*60)\n",
        "        \n",
        "        # 데이터 로드\n",
        "        data = load_and_preprocess_data(center)\n",
        "        if data is None:\n",
        "            continue\n",
        "        \n",
        "        # 데이터 분할\n",
        "        train_data, val_data, test_data = temporal_split_data(data)\n",
        "        print(f\"📊 데이터 분할 완료: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}\")\n",
        "        \n",
        "        for task in tasks:\n",
        "            print(f\"\\n🎯 {task.upper()} 태스크 시작\")\n",
        "            \n",
        "            try:\n",
        "                # 피처/타겟 준비\n",
        "                data_dict = prepare_features_target(train_data, val_data, test_data, task)\n",
        "                \n",
        "                # 하이퍼파라미터 튜닝 (시도 횟수 증가)\n",
        "                best_model, best_params, best_score, tuning_results = hyperparameter_tuning(\n",
        "                    center, data_dict, task, max_trials=25\n",
        "                )\n",
        "                \n",
        "                if best_model is not None:\n",
        "                    # 테스트셋 최종 평가\n",
        "                    final_metrics = evaluate_model(best_model, data_dict, task)\n",
        "                    \n",
        "                    # 베스트 모델 정보 저장\n",
        "                    best_info = {\n",
        "                        'center': center, 'task': task, 'best_score': best_score,\n",
        "                        'final_score': final_metrics.get('R2' if task == 'regression' else 'F1_weighted'),\n",
        "                        'best_params': best_params, **final_metrics\n",
        "                    }\n",
        "                    best_models_info.append(best_info)\n",
        "                    \n",
        "                    # 모델 저장\n",
        "                    model_filename = f\"../results_tabnet/models/{center}_{task}_tabnet_best.pkl\"\n",
        "                    with open(model_filename, 'wb') as f:\n",
        "                        pickle.dump({\n",
        "                            'model': best_model,\n",
        "                            'params': best_params,\n",
        "                            'metrics': final_metrics,\n",
        "                            'data_info': data_dict,\n",
        "                            'center': center,\n",
        "                            'task': task\n",
        "                        }, f)\n",
        "                    \n",
        "                    print(f\"💾 모델 저장 완료: {model_filename}\")\n",
        "                \n",
        "                # 튜닝 결과 추가\n",
        "                all_results.extend(tuning_results)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"❌ {center} - {task} 처리 실패: {str(e)}\")\n",
        "                continue\n",
        "    \n",
        "    # 결과 저장\n",
        "    if all_results:\n",
        "        results_df = pd.DataFrame(all_results)\n",
        "        results_df.to_csv('../results_tabnet/tabnet_hyperparameter_results.csv', \n",
        "                         index=False, encoding='utf-8-sig')\n",
        "    \n",
        "    if best_models_info:\n",
        "        best_df = pd.DataFrame(best_models_info)\n",
        "        best_df.to_csv('../results_tabnet/tabnet_best_models.csv', \n",
        "                      index=False, encoding='utf-8-sig')\n",
        "    \n",
        "    print(f\"\\n🎉 TabNet 파이프라인 완료!\")\n",
        "    print(f\"📁 결과 파일:\")\n",
        "    print(f\"  - tabnet_hyperparameter_results.csv\")\n",
        "    print(f\"  - tabnet_best_models.csv\") \n",
        "    print(f\"  - 모델 파일들: ../results_tabnet/models/\")\n",
        "    \n",
        "    return results_df, best_df\n",
        "\n",
        "# ================================================================\n",
        "# 7. 실행\n",
        "# ================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results, best_models = run_tabnet_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7d691236",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📁 파일 로드 완료\n",
            "베스트 모델 데이터: (8, 16)\n",
            "전체 결과 데이터: (200, 25)\n",
            "\n",
            "📋 베스트 모델 컬럼:\n",
            "['center', 'task', 'best_score', 'final_score', 'best_params', 'R2', 'RMSE', 'MAE', 'MAPE', 'SMAPE', 'y_pred', 'y_true', 'Accuracy', 'F1_weighted', 'F1_macro', 'ROC_AUC']\n",
            "\n",
            "📋 전체 결과 컬럼:\n",
            "['center', 'task', 'trial', 'train_time', 'score', 'gamma', 'lambda_sparse', 'learning_rate', 'n_a', 'n_d', 'n_independent', 'n_shared', 'n_steps', 'weight_decay', 'R2', 'RMSE', 'MAE', 'MAPE', 'SMAPE', 'y_pred', 'y_true', 'Accuracy', 'F1_weighted', 'F1_macro', 'ROC_AUC']\n",
            "\n",
            "🎯 TabNet 베스트 모델 성능 요약\n",
            "============================================================\n",
            "NANJI - REGRESSION\n",
            "  R² Score: 0.1495\n",
            "  RMSE: 87090.8294, MAE: 56594.428542345275\n",
            "\n",
            "NANJI - CLASSIFICATION\n",
            "  F1 Weighted: 0.6970\n",
            "  Accuracy: 0.7134\n",
            "\n",
            "JUNGNANG - REGRESSION\n",
            "  R² Score: 0.0314\n",
            "  RMSE: 145671.1017, MAE: 109623.23656351792\n",
            "\n",
            "JUNGNANG - CLASSIFICATION\n",
            "  F1 Weighted: 0.5374\n",
            "  Accuracy: 0.5537\n",
            "\n",
            "SEONAM - REGRESSION\n",
            "  R² Score: -0.1750\n",
            "  RMSE: 180637.2365, MAE: 143198.8273615635\n",
            "\n",
            "SEONAM - CLASSIFICATION\n",
            "  F1 Weighted: 0.4418\n",
            "  Accuracy: 0.3876\n",
            "\n",
            "TANCHEON - REGRESSION\n",
            "  R² Score: -0.0124\n",
            "  RMSE: 89500.3052, MAE: 71870.46030130293\n",
            "\n",
            "TANCHEON - CLASSIFICATION\n",
            "  F1 Weighted: 0.4468\n",
            "  Accuracy: 0.4756\n",
            "\n",
            "🔍 데이터 샘플:\n",
            "베스트 모델 첫 번째 행:\n",
            "center                                                     nanji\n",
            "task                                                  regression\n",
            "best_score                                              0.578787\n",
            "final_score                                             0.149485\n",
            "best_params    {'gamma': 1.0, 'lambda_sparse': 1e-05, 'learni...\n",
            "R2                                                      0.149485\n",
            "RMSE                                                87090.829424\n",
            "MAE                                                 56594.428542\n",
            "MAPE                                                     8.63217\n",
            "SMAPE                                                   8.645789\n",
            "y_pred         [ 891049.8   786021.25  757387.94  706082.3   ...\n",
            "y_true         [1094990.  873775.  804430.  765831.  729316. ...\n",
            "Accuracy                                                     NaN\n",
            "F1_weighted                                                  NaN\n",
            "F1_macro                                                     NaN\n",
            "ROC_AUC                                                      NaN\n",
            "Name: 0, dtype: object\n",
            "\n",
            "전체 결과 첫 번째 행:\n",
            "center                                                       nanji\n",
            "task                                                    regression\n",
            "trial                                                            1\n",
            "train_time                                               17.430891\n",
            "score                                                     0.461352\n",
            "gamma                                                          1.0\n",
            "lambda_sparse                                               0.0001\n",
            "learning_rate                                                0.005\n",
            "n_a                                                             32\n",
            "n_d                                                             64\n",
            "n_independent                                                    2\n",
            "n_shared                                                         2\n",
            "n_steps                                                          4\n",
            "weight_decay                                              0.000001\n",
            "R2                                                        0.461352\n",
            "RMSE                                                 118172.743696\n",
            "MAE                                                   80578.428797\n",
            "MAPE                                                     11.689336\n",
            "SMAPE                                                    11.574484\n",
            "y_pred           [ 563336.44  557421.    574265.    559522.44  ...\n",
            "y_true           [ 510829.  516767.  512137.  508921.  520705. ...\n",
            "Accuracy                                                       NaN\n",
            "F1_weighted                                                    NaN\n",
            "F1_macro                                                       NaN\n",
            "ROC_AUC                                                        NaN\n",
            "Name: 0, dtype: object\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x1200 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 하이퍼파라미터 중요도 분석\n",
            "============================================================\n",
            "learning_rate:\n",
            "  최적값: 0.005 (평균 성능: 0.4903)\n",
            "  성능 범위: 0.4255 - 0.4903\n",
            "\n",
            "n_d:\n",
            "  최적값: 32 (평균 성능: 0.4725)\n",
            "  성능 범위: 0.4384 - 0.4725\n",
            "\n",
            "n_a:\n",
            "  최적값: 32 (평균 성능: 0.4630)\n",
            "  성능 범위: 0.4497 - 0.4630\n",
            "\n",
            "n_steps:\n",
            "  최적값: 5 (평균 성능: 0.4605)\n",
            "  성능 범위: 0.4515 - 0.4605\n",
            "\n",
            "gamma:\n",
            "  최적값: 1.0 (평균 성능: 0.4724)\n",
            "  성능 범위: 0.4374 - 0.4724\n",
            "\n",
            "🏢 센터별 최적 파라미터 패턴\n",
            "============================================================\n",
            "best_params 컬럼에서 파라미터 정보 추출:\n",
            "nanji - regression:\n",
            "  gamma: 1.0\n",
            "  lambda_sparse: 1e-05\n",
            "  learning_rate: 0.005\n",
            "  n_a: 32\n",
            "  n_d: 32\n",
            "  n_independent: 2\n",
            "  n_shared: 2\n",
            "  n_steps: 5\n",
            "  weight_decay: 1e-05\n",
            "\n",
            "nanji - classification:\n",
            "  gamma: 1.0\n",
            "  lambda_sparse: 1e-05\n",
            "  learning_rate: 0.005\n",
            "  n_a: 64\n",
            "  n_d: 32\n",
            "  n_independent: 2\n",
            "  n_shared: 2\n",
            "  n_steps: 5\n",
            "  weight_decay: 1e-05\n",
            "\n",
            "jungnang - regression:\n",
            "  gamma: 1.0\n",
            "  lambda_sparse: 1e-05\n",
            "  learning_rate: 0.005\n",
            "  n_a: 64\n",
            "  n_d: 32\n",
            "  n_independent: 2\n",
            "  n_shared: 2\n",
            "  n_steps: 4\n",
            "  weight_decay: 1e-05\n",
            "\n",
            "jungnang - classification:\n",
            "  gamma: 1.0\n",
            "  lambda_sparse: 1e-05\n",
            "  learning_rate: 0.005\n",
            "  n_a: 32\n",
            "  n_d: 32\n",
            "  n_independent: 2\n",
            "  n_shared: 2\n",
            "  n_steps: 4\n",
            "  weight_decay: 1e-05\n",
            "\n",
            "seonam - regression:\n",
            "  gamma: 1.2\n",
            "  lambda_sparse: 1e-05\n",
            "  learning_rate: 0.005\n",
            "  n_a: 64\n",
            "  n_d: 32\n",
            "  n_independent: 2\n",
            "  n_shared: 2\n",
            "  n_steps: 4\n",
            "  weight_decay: 1e-06\n",
            "\n",
            "seonam - classification:\n",
            "  gamma: 1.0\n",
            "  lambda_sparse: 1e-05\n",
            "  learning_rate: 0.001\n",
            "  n_a: 64\n",
            "  n_d: 64\n",
            "  n_independent: 2\n",
            "  n_shared: 2\n",
            "  n_steps: 5\n",
            "  weight_decay: 1e-06\n",
            "\n",
            "tancheon - regression:\n",
            "  gamma: 1.0\n",
            "  lambda_sparse: 1e-05\n",
            "  learning_rate: 0.002\n",
            "  n_a: 64\n",
            "  n_d: 32\n",
            "  n_independent: 2\n",
            "  n_shared: 2\n",
            "  n_steps: 5\n",
            "  weight_decay: 1e-05\n",
            "\n",
            "tancheon - classification:\n",
            "  gamma: 1.0\n",
            "  lambda_sparse: 1e-05\n",
            "  learning_rate: 0.002\n",
            "  n_a: 64\n",
            "  n_d: 64\n",
            "  n_independent: 2\n",
            "  n_shared: 2\n",
            "  n_steps: 4\n",
            "  weight_decay: 1e-06\n",
            "\n",
            "\n",
            "💾 분석 결과가 저장되었습니다: ../results_tabnet 복사본/tabnet_performance_analysis.png\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# 한글 폰트 설정\n",
        "plt.rcParams['font.family'] = 'AppleGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 결과 파일 로드\n",
        "results_path = '../results_tabnet 복사본'\n",
        "\n",
        "# 파일 존재 확인 후 로드\n",
        "try:\n",
        "    best_models = pd.read_csv(f'{results_path}/tabnet_best_models.csv')\n",
        "    all_results = pd.read_csv(f'{results_path}/tabnet_hyperparameter_results.csv')\n",
        "    \n",
        "    print(\"📁 파일 로드 완료\")\n",
        "    print(f\"베스트 모델 데이터: {best_models.shape}\")\n",
        "    print(f\"전체 결과 데이터: {all_results.shape}\")\n",
        "    print()\n",
        "    \n",
        "    # 컬럼명 확인\n",
        "    print(\"📋 베스트 모델 컬럼:\")\n",
        "    print(best_models.columns.tolist())\n",
        "    print()\n",
        "    print(\"📋 전체 결과 컬럼:\")  \n",
        "    print(all_results.columns.tolist())\n",
        "    print()\n",
        "    \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ 파일을 찾을 수 없습니다: {e}\")\n",
        "    print(\"결과 파일이 생성되었는지 확인해주세요.\")\n",
        "    exit()\n",
        "\n",
        "print(\"🎯 TabNet 베스트 모델 성능 요약\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. 베스트 모델 성능 요약 (사용 가능한 컬럼으로 수정)\n",
        "for _, row in best_models.iterrows():\n",
        "    center = row['center'].upper()\n",
        "    task = row['task'].upper()\n",
        "    \n",
        "    if task == 'REGRESSION':\n",
        "        if 'final_score' in row:\n",
        "            main_score = row['final_score']\n",
        "        else:\n",
        "            main_score = row.get('R2', row.get('best_score', 0))\n",
        "        print(f\"{center} - {task}\")\n",
        "        print(f\"  R² Score: {main_score:.4f}\")\n",
        "        if 'RMSE' in row:\n",
        "            print(f\"  RMSE: {row['RMSE']:.4f}, MAE: {row.get('MAE', 'N/A')}\")\n",
        "    else:\n",
        "        if 'final_score' in row:\n",
        "            main_score = row['final_score'] \n",
        "        else:\n",
        "            main_score = row.get('F1_weighted', row.get('best_score', 0))\n",
        "        print(f\"{center} - {task}\")\n",
        "        print(f\"  F1 Weighted: {main_score:.4f}\")\n",
        "        if 'Accuracy' in row:\n",
        "            print(f\"  Accuracy: {row['Accuracy']:.4f}\")\n",
        "    \n",
        "    # 하이퍼파라미터 정보 (있는 것만 출력)\n",
        "    param_info = []\n",
        "    for param in ['n_d', 'n_a', 'n_steps', 'learning_rate']:\n",
        "        if param in row and pd.notna(row[param]):\n",
        "            param_info.append(f\"{param}={row[param]}\")\n",
        "    \n",
        "    if param_info:\n",
        "        print(f\"  최적 파라미터: {', '.join(param_info)}\")\n",
        "    print()\n",
        "\n",
        "# 데이터 샘플 확인\n",
        "print(\"🔍 데이터 샘플:\")\n",
        "print(\"베스트 모델 첫 번째 행:\")\n",
        "print(best_models.iloc[0])\n",
        "print()\n",
        "print(\"전체 결과 첫 번째 행:\")\n",
        "print(all_results.iloc[0])\n",
        "\n",
        "# 2. 성능 비교 시각화\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('TabNet 센터별 성능 비교', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 회귀 성능\n",
        "reg_data = best_models[best_models['task'] == 'regression']\n",
        "ax1 = axes[0, 0]\n",
        "bars1 = ax1.bar(reg_data['center'], reg_data['R2'], \n",
        "                color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "ax1.set_title('회귀 성능 (R²)', fontweight='bold')\n",
        "ax1.set_ylabel('R² Score')\n",
        "ax1.set_ylim(0, max(reg_data['R2']) * 1.1)\n",
        "for i, bar in enumerate(bars1):\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{height:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 분류 성능  \n",
        "cls_data = best_models[best_models['task'] == 'classification']\n",
        "ax2 = axes[0, 1]\n",
        "bars2 = ax2.bar(cls_data['center'], cls_data['F1_weighted'],\n",
        "                color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "ax2.set_title('분류 성능 (F1 Weighted)', fontweight='bold')\n",
        "ax2.set_ylabel('F1 Weighted Score')\n",
        "ax2.set_ylim(0, max(cls_data['F1_weighted']) * 1.1)\n",
        "for i, bar in enumerate(bars2):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{height:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 3. 하이퍼파라미터 중요도 분석\n",
        "# learning_rate 영향\n",
        "ax3 = axes[1, 0]\n",
        "lr_performance = all_results.groupby(['learning_rate', 'task'])['score'].mean().reset_index()\n",
        "for task in lr_performance['task'].unique():\n",
        "    task_data = lr_performance[lr_performance['task'] == task]\n",
        "    ax3.plot(task_data['learning_rate'], task_data['score'], \n",
        "             marker='o', label=f'{task.capitalize()}', linewidth=2, markersize=6)\n",
        "ax3.set_title('Learning Rate vs 성능', fontweight='bold')\n",
        "ax3.set_xlabel('Learning Rate')\n",
        "ax3.set_ylabel('평균 성능 점수')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# n_d vs n_a 효과\n",
        "ax4 = axes[1, 1]\n",
        "nd_na_perf = all_results.groupby(['n_d', 'n_a'])['score'].mean().reset_index()\n",
        "pivot_data = nd_na_perf.pivot(index='n_d', columns='n_a', values='score')\n",
        "sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='YlOrRd', \n",
        "            ax=ax4, cbar_kws={'label': '평균 성능'})\n",
        "ax4.set_title('n_d vs n_a 성능 히트맵', fontweight='bold')\n",
        "ax4.set_xlabel('n_a (Attention Layer Width)')\n",
        "ax4.set_ylabel('n_d (Decision Layer Width)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{results_path}/tabnet_performance_analysis.png', \n",
        "            dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 4. 상세 하이퍼파라미터 분석\n",
        "print(\"\\n📊 하이퍼파라미터 중요도 분석\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "important_params = ['learning_rate', 'n_d', 'n_a', 'n_steps', 'gamma']\n",
        "for param in important_params:\n",
        "    param_importance = all_results.groupby(param)['score'].agg(['mean', 'std', 'count'])\n",
        "    best_value = param_importance['mean'].idxmax()\n",
        "    best_score = param_importance['mean'].max()\n",
        "    \n",
        "    print(f\"{param}:\")\n",
        "    print(f\"  최적값: {best_value} (평균 성능: {best_score:.4f})\")\n",
        "    print(f\"  성능 범위: {param_importance['mean'].min():.4f} - {param_importance['mean'].max():.4f}\")\n",
        "    print()\n",
        "\n",
        "# 5. 센터별 최적 파라미터 패턴\n",
        "print(\"🏢 센터별 최적 파라미터 패턴\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# best_params가 딕셔너리 형태로 저장되었는지 확인\n",
        "if 'best_params' in best_models.columns:\n",
        "    print(\"best_params 컬럼에서 파라미터 정보 추출:\")\n",
        "    for _, row in best_models.iterrows():\n",
        "        print(f\"{row['center']} - {row['task']}:\")\n",
        "        if pd.notna(row['best_params']):\n",
        "            # 문자열 형태의 딕셔너리를 실제 딕셔너리로 변환\n",
        "            try:\n",
        "                import ast\n",
        "                params = ast.literal_eval(row['best_params']) if isinstance(row['best_params'], str) else row['best_params']\n",
        "                for key, value in params.items():\n",
        "                    print(f\"  {key}: {value}\")\n",
        "            except:\n",
        "                print(f\"  파라미터 정보: {row['best_params']}\")\n",
        "        print()\n",
        "else:\n",
        "    # 하이퍼파라미터 컬럼이 직접 있는지 확인\n",
        "    available_params = [col for col in important_params if col in best_models.columns]\n",
        "    \n",
        "    if available_params:\n",
        "        print(\"사용 가능한 파라미터 컬럼:\", available_params)\n",
        "        center_patterns = best_models.groupby('center')[available_params].first()\n",
        "        print(center_patterns)\n",
        "    else:\n",
        "        print(\"하이퍼파라미터 정보가 베스트 모델 파일에 없습니다.\")\n",
        "        print(\"전체 결과에서 최고 성능 파라미터를 찾아보겠습니다:\")\n",
        "        \n",
        "        # 각 센터/태스크별 최고 성능 행 찾기\n",
        "        best_configs = []\n",
        "        for center in all_results['center'].unique():\n",
        "            for task in all_results['task'].unique():\n",
        "                subset = all_results[(all_results['center'] == center) & (all_results['task'] == task)]\n",
        "                if not subset.empty:\n",
        "                    best_row = subset.loc[subset['score'].idxmax()]\n",
        "                    best_configs.append(best_row)\n",
        "        \n",
        "        if best_configs:\n",
        "            best_config_df = pd.DataFrame(best_configs)\n",
        "            for _, row in best_config_df.iterrows():\n",
        "                print(f\"{row['center']} - {row['task']}:\")\n",
        "                for param in important_params:\n",
        "                    if param in row:\n",
        "                        print(f\"  {param}: {row[param]}\")\n",
        "                print()\n",
        "\n",
        "print(f\"\\n💾 분석 결과가 저장되었습니다: {results_path}/tabnet_performance_analysis.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b8a21248",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 TabNet 성능 문제 진단\n",
            "==================================================\n",
            "📊 모델 데이터 구조:\n",
            "Keys: dict_keys(['model', 'params', 'metrics', 'data_info', 'center', 'task'])\n",
            "\n",
            "📈 데이터 분할 정보:\n",
            "Train shape: (2148, 32)\n",
            "Val shape: (614, 32)\n",
            "Test shape: (307, 32)\n",
            "\n",
            "🔧 스케일링 정보:\n",
            "X scaler: <class 'sklearn.preprocessing._data.StandardScaler'>\n",
            "Y scaler: <class 'sklearn.preprocessing._data.StandardScaler'>\n",
            "\n",
            "📊 상세 성능 지표:\n",
            "R2: 0.14948542224804306\n",
            "RMSE: 87090.82942449063\n",
            "MAE: 56594.428542345275\n",
            "MAPE: 8.632169828833494\n",
            "SMAPE: 8.645789300197373\n",
            "\n",
            "\n",
            "🛠️ 문제 해결 방안:\n",
            "==================================================\n",
            "\n",
            "1. **데이터 리키지 확인**:\n",
            "   - 타겟 변수와 관련된 미래 정보가 피처에 포함되었는지 확인\n",
            "   - 'lag' 피처들이 올바르게 생성되었는지 검증\n",
            "\n",
            "2. **스케일링 문제**:\n",
            "   - Y 타겟 스케일링 후 역변환이 제대로 되었는지 확인\n",
            "   - 원본 스케일에서의 성능 계산 검증\n",
            "\n",
            "3. **시간 분할 검증**:\n",
            "   - Temporal split이 제대로 되었는지 확인\n",
            "   - 미래 데이터가 과거 학습에 사용되지 않았는지 검증\n",
            "\n",
            "4. **모델 설정**:\n",
            "   - Early stopping 기준 재검토\n",
            "   - Validation loss가 실제 성능을 반영하는지 확인\n",
            "\n",
            "\n",
            "🔧 수정된 TabNet 파이프라인:\n",
            "==================================================\n",
            "\n",
            "# 개선된 prepare_features_target 함수\n",
            "def prepare_features_target_fixed(train_data, val_data, test_data, task='regression'):\n",
            "    \"\"\"피처와 타겟 분리 및 스케일링 - 수정 버전\"\"\"\n",
            "    \n",
            "    if task == 'regression':\n",
            "        target_col = '합계_1일후'\n",
            "        # 미래 정보 제거 - 중요!\n",
            "        exclude_cols = [\n",
            "            '1처리장','2처리장','정화조','중계펌프장','시설현대화',\n",
            "            '3처리장','4처리장',\n",
            "            '합계_1일후','합계_2일후',  # 미래 타겟들\n",
            "            '등급','등급_1일후','등급_2일후'  # 미래 등급들\n",
            "            # '합계'는 현재 시점이므로 제거하지 않음\n",
            "        ]\n",
            "    else:  # classification\n",
            "        target_col = '등급_1일후'\n",
            "        exclude_cols = [\n",
            "            '1처리장','2처리장','정화조','중계펌프장','시설현대화',\n",
            "            '3처리장','4처리장',\n",
            "            '합계_1일후','합계_2일후',\n",
            "            '등급','등급_1일후','등급_2일후'\n",
            "        ]\n",
            "    \n",
            "    # 존재하는 컬럼만 제외\n",
            "    existing_exclude_cols = [col for col in exclude_cols if col in train_data.columns]\n",
            "    \n",
            "    # 피처 분리\n",
            "    X_train = train_data.drop(existing_exclude_cols, axis=1)\n",
            "    X_val = val_data.drop(existing_exclude_cols, axis=1)\n",
            "    X_test = test_data.drop(existing_exclude_cols, axis=1)\n",
            "    \n",
            "    # 타겟 분리\n",
            "    y_train = train_data[target_col].copy()\n",
            "    y_val = val_data[target_col].copy()\n",
            "    y_test = test_data[target_col].copy()\n",
            "    \n",
            "    # 피처 스케일링\n",
            "    x_scaler = StandardScaler()\n",
            "    X_train_scaled = x_scaler.fit_transform(X_train).astype(np.float32)\n",
            "    X_val_scaled = x_scaler.transform(X_val).astype(np.float32)\n",
            "    X_test_scaled = x_scaler.transform(X_test).astype(np.float32)\n",
            "    \n",
            "    # 회귀의 경우 - 타겟 스케일링 제거 또는 주의깊게 처리\n",
            "    if task == 'regression':\n",
            "        # 옵션 1: 스케일링 없이 진행 (권장)\n",
            "        return {\n",
            "            'X_train': X_train_scaled, 'X_val': X_val_scaled, 'X_test': X_test_scaled,\n",
            "            'y_train': y_train.values.astype(np.float32), \n",
            "            'y_val': y_val.values.astype(np.float32), \n",
            "            'y_test': y_test.values.astype(np.float32),\n",
            "            'y_train_orig': y_train, 'y_val_orig': y_val, 'y_test_orig': y_test,\n",
            "            'x_scaler': x_scaler, 'y_scaler': None,\n",
            "            'feature_names': X_train.columns.tolist()\n",
            "        }\n",
            "    else:\n",
            "        return {\n",
            "            'X_train': X_train_scaled, 'X_val': X_val_scaled, 'X_test': X_test_scaled,\n",
            "            'y_train': y_train.values.astype(int), 'y_val': y_val.values.astype(int), \n",
            "            'y_test': y_test.values.astype(int),\n",
            "            'x_scaler': x_scaler, 'y_scaler': None,\n",
            "            'feature_names': X_train.columns.tolist()\n",
            "        }\n",
            "\n",
            "# 개선된 평가 함수\n",
            "def evaluate_model_fixed(model, data_dict, task='regression'):\n",
            "    \"\"\"수정된 모델 성능 평가\"\"\"\n",
            "    \n",
            "    # 예측\n",
            "    y_pred = model.predict(data_dict['X_test'])\n",
            "    \n",
            "    if task == 'regression':\n",
            "        # 스케일링 없이 직접 계산\n",
            "        y_true_orig = data_dict['y_test']  # 이미 원본 스케일\n",
            "        y_pred_orig = y_pred.ravel()  # 예측값도 원본 스케일\n",
            "        \n",
            "        # 성능 지표 계산\n",
            "        r2 = r2_score(y_true_orig, y_pred_orig)\n",
            "        rmse = np.sqrt(mean_squared_error(y_true_orig, y_pred_orig))\n",
            "        mae = mean_absolute_error(y_true_orig, y_pred_orig)\n",
            "        \n",
            "        # MAPE, SMAPE 계산 (분모 0 방지)\n",
            "        def safe_mape(y_true, y_pred):\n",
            "            mask = y_true != 0\n",
            "            return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
            "        \n",
            "        def safe_smape(y_true, y_pred):\n",
            "            denominator = (np.abs(y_true) + np.abs(y_pred))\n",
            "            mask = denominator != 0\n",
            "            return np.mean(2 * np.abs(y_true[mask] - y_pred[mask]) / denominator[mask]) * 100\n",
            "        \n",
            "        mape = safe_mape(y_true_orig, y_pred_orig)\n",
            "        smape = safe_smape(y_true_orig, y_pred_orig)\n",
            "        \n",
            "        metrics = {\n",
            "            'R2': r2, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'SMAPE': smape,\n",
            "            'y_pred': y_pred_orig, 'y_true': y_true_orig\n",
            "        }\n",
            "        \n",
            "        return metrics\n",
            "    \n",
            "    # 분류는 기존과 동일\n",
            "    else:\n",
            "        y_true = data_dict['y_test']\n",
            "        accuracy = accuracy_score(y_true, y_pred)\n",
            "        f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
            "        f1_macro = f1_score(y_true, y_pred, average='macro')\n",
            "        \n",
            "        try:\n",
            "            y_pred_proba = model.predict_proba(data_dict['X_test'])\n",
            "            if len(np.unique(y_true)) == 2:\n",
            "                roc_auc = roc_auc_score(y_true, y_pred_proba[:, 1])\n",
            "            else:\n",
            "                roc_auc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr')\n",
            "        except:\n",
            "            roc_auc = None\n",
            "        \n",
            "        metrics = {\n",
            "            'Accuracy': accuracy, 'F1_weighted': f1_weighted, \n",
            "            'F1_macro': f1_macro, 'ROC_AUC': roc_auc,\n",
            "            'y_pred': y_pred, 'y_true': y_true\n",
            "        }\n",
            "        \n",
            "        return metrics\n",
            "\n",
            "\n",
            "📋 권장사항:\n",
            "==================================================\n",
            "\n",
            "1. 타겟 스케일링 제거: 회귀에서 불필요한 복잡성 제거\n",
            "2. 데이터 리키지 재검토: 미래 정보가 피처에 포함되지 않도록 확인  \n",
            "3. Cross-validation 추가: 시간 기반 교차검증으로 안정성 확보\n",
            "4. 하이퍼파라미터 재조정: 과적합 방지를 위한 정규화 강화\n",
            "5. Early stopping 기준 강화: validation loss 모니터링 개선\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TabNet 성능 문제 진단 및 해결\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "print(\"🔍 TabNet 성능 문제 진단\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. 모델 파일 로드해서 상세 분석\n",
        "model_path = \"../results_tabnet 복사본/models/nanji_regression_tabnet_best.pkl\"\n",
        "\n",
        "try:\n",
        "    with open(model_path, 'rb') as f:\n",
        "        model_data = pickle.load(f)\n",
        "    \n",
        "    print(\"📊 모델 데이터 구조:\")\n",
        "    print(f\"Keys: {model_data.keys()}\")\n",
        "    print()\n",
        "    \n",
        "    # 데이터 분할 정보 확인\n",
        "    data_info = model_data['data_info']\n",
        "    print(\"📈 데이터 분할 정보:\")\n",
        "    print(f\"Train shape: {data_info['X_train'].shape}\")\n",
        "    print(f\"Val shape: {data_info['X_val'].shape}\") \n",
        "    print(f\"Test shape: {data_info['X_test'].shape}\")\n",
        "    print()\n",
        "    \n",
        "    # 스케일링 정보 확인\n",
        "    print(\"🔧 스케일링 정보:\")\n",
        "    print(f\"X scaler: {type(data_info['x_scaler'])}\")\n",
        "    print(f\"Y scaler: {type(data_info.get('y_scaler', 'None'))}\")\n",
        "    print()\n",
        "    \n",
        "    # 성능 지표 확인\n",
        "    metrics = model_data['metrics']\n",
        "    print(\"📊 상세 성능 지표:\")\n",
        "    for key, value in metrics.items():\n",
        "        if key not in ['y_pred', 'y_true']:\n",
        "            print(f\"{key}: {value}\")\n",
        "    print()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ 모델 파일 로드 실패: {e}\")\n",
        "\n",
        "print(\"\\n🛠️ 문제 해결 방안:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\"\"\n",
        "1. **데이터 리키지 확인**:\n",
        "   - 타겟 변수와 관련된 미래 정보가 피처에 포함되었는지 확인\n",
        "   - 'lag' 피처들이 올바르게 생성되었는지 검증\n",
        "\n",
        "2. **스케일링 문제**:\n",
        "   - Y 타겟 스케일링 후 역변환이 제대로 되었는지 확인\n",
        "   - 원본 스케일에서의 성능 계산 검증\n",
        "\n",
        "3. **시간 분할 검증**:\n",
        "   - Temporal split이 제대로 되었는지 확인\n",
        "   - 미래 데이터가 과거 학습에 사용되지 않았는지 검증\n",
        "\n",
        "4. **모델 설정**:\n",
        "   - Early stopping 기준 재검토\n",
        "   - Validation loss가 실제 성능을 반영하는지 확인\n",
        "\"\"\")\n",
        "\n",
        "# 해결된 TabNet 파이프라인 코드 제안\n",
        "print(\"\\n🔧 수정된 TabNet 파이프라인:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "improved_code = '''\n",
        "# 개선된 prepare_features_target 함수\n",
        "def prepare_features_target_fixed(train_data, val_data, test_data, task='regression'):\n",
        "    \"\"\"피처와 타겟 분리 및 스케일링 - 수정 버전\"\"\"\n",
        "    \n",
        "    if task == 'regression':\n",
        "        target_col = '합계_1일후'\n",
        "        # 미래 정보 제거 - 중요!\n",
        "        exclude_cols = [\n",
        "            '1처리장','2처리장','정화조','중계펌프장','시설현대화',\n",
        "            '3처리장','4처리장',\n",
        "            '합계_1일후','합계_2일후',  # 미래 타겟들\n",
        "            '등급','등급_1일후','등급_2일후'  # 미래 등급들\n",
        "            # '합계'는 현재 시점이므로 제거하지 않음\n",
        "        ]\n",
        "    else:  # classification\n",
        "        target_col = '등급_1일후'\n",
        "        exclude_cols = [\n",
        "            '1처리장','2처리장','정화조','중계펌프장','시설현대화',\n",
        "            '3처리장','4처리장',\n",
        "            '합계_1일후','합계_2일후',\n",
        "            '등급','등급_1일후','등급_2일후'\n",
        "        ]\n",
        "    \n",
        "    # 존재하는 컬럼만 제외\n",
        "    existing_exclude_cols = [col for col in exclude_cols if col in train_data.columns]\n",
        "    \n",
        "    # 피처 분리\n",
        "    X_train = train_data.drop(existing_exclude_cols, axis=1)\n",
        "    X_val = val_data.drop(existing_exclude_cols, axis=1)\n",
        "    X_test = test_data.drop(existing_exclude_cols, axis=1)\n",
        "    \n",
        "    # 타겟 분리\n",
        "    y_train = train_data[target_col].copy()\n",
        "    y_val = val_data[target_col].copy()\n",
        "    y_test = test_data[target_col].copy()\n",
        "    \n",
        "    # 피처 스케일링\n",
        "    x_scaler = StandardScaler()\n",
        "    X_train_scaled = x_scaler.fit_transform(X_train).astype(np.float32)\n",
        "    X_val_scaled = x_scaler.transform(X_val).astype(np.float32)\n",
        "    X_test_scaled = x_scaler.transform(X_test).astype(np.float32)\n",
        "    \n",
        "    # 회귀의 경우 - 타겟 스케일링 제거 또는 주의깊게 처리\n",
        "    if task == 'regression':\n",
        "        # 옵션 1: 스케일링 없이 진행 (권장)\n",
        "        return {\n",
        "            'X_train': X_train_scaled, 'X_val': X_val_scaled, 'X_test': X_test_scaled,\n",
        "            'y_train': y_train.values.astype(np.float32), \n",
        "            'y_val': y_val.values.astype(np.float32), \n",
        "            'y_test': y_test.values.astype(np.float32),\n",
        "            'y_train_orig': y_train, 'y_val_orig': y_val, 'y_test_orig': y_test,\n",
        "            'x_scaler': x_scaler, 'y_scaler': None,\n",
        "            'feature_names': X_train.columns.tolist()\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            'X_train': X_train_scaled, 'X_val': X_val_scaled, 'X_test': X_test_scaled,\n",
        "            'y_train': y_train.values.astype(int), 'y_val': y_val.values.astype(int), \n",
        "            'y_test': y_test.values.astype(int),\n",
        "            'x_scaler': x_scaler, 'y_scaler': None,\n",
        "            'feature_names': X_train.columns.tolist()\n",
        "        }\n",
        "\n",
        "# 개선된 평가 함수\n",
        "def evaluate_model_fixed(model, data_dict, task='regression'):\n",
        "    \"\"\"수정된 모델 성능 평가\"\"\"\n",
        "    \n",
        "    # 예측\n",
        "    y_pred = model.predict(data_dict['X_test'])\n",
        "    \n",
        "    if task == 'regression':\n",
        "        # 스케일링 없이 직접 계산\n",
        "        y_true_orig = data_dict['y_test']  # 이미 원본 스케일\n",
        "        y_pred_orig = y_pred.ravel()  # 예측값도 원본 스케일\n",
        "        \n",
        "        # 성능 지표 계산\n",
        "        r2 = r2_score(y_true_orig, y_pred_orig)\n",
        "        rmse = np.sqrt(mean_squared_error(y_true_orig, y_pred_orig))\n",
        "        mae = mean_absolute_error(y_true_orig, y_pred_orig)\n",
        "        \n",
        "        # MAPE, SMAPE 계산 (분모 0 방지)\n",
        "        def safe_mape(y_true, y_pred):\n",
        "            mask = y_true != 0\n",
        "            return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "        \n",
        "        def safe_smape(y_true, y_pred):\n",
        "            denominator = (np.abs(y_true) + np.abs(y_pred))\n",
        "            mask = denominator != 0\n",
        "            return np.mean(2 * np.abs(y_true[mask] - y_pred[mask]) / denominator[mask]) * 100\n",
        "        \n",
        "        mape = safe_mape(y_true_orig, y_pred_orig)\n",
        "        smape = safe_smape(y_true_orig, y_pred_orig)\n",
        "        \n",
        "        metrics = {\n",
        "            'R2': r2, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'SMAPE': smape,\n",
        "            'y_pred': y_pred_orig, 'y_true': y_true_orig\n",
        "        }\n",
        "        \n",
        "        return metrics\n",
        "    \n",
        "    # 분류는 기존과 동일\n",
        "    else:\n",
        "        y_true = data_dict['y_test']\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
        "        f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "        \n",
        "        try:\n",
        "            y_pred_proba = model.predict_proba(data_dict['X_test'])\n",
        "            if len(np.unique(y_true)) == 2:\n",
        "                roc_auc = roc_auc_score(y_true, y_pred_proba[:, 1])\n",
        "            else:\n",
        "                roc_auc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr')\n",
        "        except:\n",
        "            roc_auc = None\n",
        "        \n",
        "        metrics = {\n",
        "            'Accuracy': accuracy, 'F1_weighted': f1_weighted, \n",
        "            'F1_macro': f1_macro, 'ROC_AUC': roc_auc,\n",
        "            'y_pred': y_pred, 'y_true': y_true\n",
        "        }\n",
        "        \n",
        "        return metrics\n",
        "'''\n",
        "\n",
        "print(improved_code)\n",
        "\n",
        "print(\"\\n📋 권장사항:\")\n",
        "print(\"=\"*50)\n",
        "print(\"\"\"\n",
        "1. 타겟 스케일링 제거: 회귀에서 불필요한 복잡성 제거\n",
        "2. 데이터 리키지 재검토: 미래 정보가 피처에 포함되지 않도록 확인  \n",
        "3. Cross-validation 추가: 시간 기반 교차검증으로 안정성 확보\n",
        "4. 하이퍼파라미터 재조정: 과적합 방지를 위한 정규화 강화\n",
        "5. Early stopping 기준 강화: validation loss 모니터링 개선\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3b18529",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "youngwon",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
