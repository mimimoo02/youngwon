{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04ddea0a",
   "metadata": {},
   "source": [
    "# 센터별. 회귀. 분류 모델링 각각 randomforest, catboost, xgboost, gradientboost, logistic, linear\n",
    "\n",
    "네, 이게 **완전한 모든 코드**입니다.\n",
    "\n",
    "**포함된 모든 기능:**\n",
    "- 데이터 로드 및 피처 엔지니어링 (Data Leakage 방지)\n",
    "- 12개 모델 (회귀 6개 + 분류 6개) 정의\n",
    "- Stratified vs 시계열 분할 비교 평가\n",
    "- 센터별 최고 성능 모델 자동 선택\n",
    "- 모델 학습 및 pickle 파일로 저장\n",
    "- Feature Importance 분석 및 시각화\n",
    "- 새로운 데이터로 예측\n",
    "- 결과 저장 및 요약 출력\n",
    "\n",
    "**실행 방법:**\n",
    "```python\n",
    "# 1. 위 코드 전체 실행 (함수들 로드)\n",
    "\n",
    "# 2. 모델 학습 (한 번만)\n",
    "saved_models, summary = train_and_save_best_models()\n",
    "\n",
    "# 3. 예측 (필요할 때마다)\n",
    "results = predict_with_saved_models('새_데이터.csv')\n",
    "```\n",
    "\n",
    "**단, 주의사항:**\n",
    "- 데이터 경로가 `../data/processed/center_season/...` 구조여야 함\n",
    "- 선택적 라이브러리 (XGBoost, LightGBM, CatBoost, SHAP)가 없어도 작동하지만 일부 모델/기능 제외됨\n",
    "- 메모리 충분히 확보 필요 (대용량 데이터 처리)\n",
    "\n",
    "이 하나의 코드로 학습부터 예측까지 모든 과정을 처리할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0c8273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "하수처리장 예측 파이프라인 초기화 완료\n",
      "XGBoost: ✓\n",
      "LightGBM: ✓\n",
      "CatBoost: ✓\n",
      "SHAP: ✓\n",
      "================================================================================\n",
      "================================================================================\n",
      "하수처리장 예측 파이프라인 사용 가이드\n",
      "================================================================================\n",
      "\n",
      "1단계: 모델 학습 및 저장 (한 번만 실행)\n",
      "==================================================\n",
      "saved_models, summary = train_and_save_best_models(cutoff_date='2025-05-20')\n",
      "\n",
      "생성되는 파일:\n",
      "- ./trained_models/ 폴더\n",
      "  ├── nanji_RandomForest_Reg_regression_20250101_123456.pkl\n",
      "  ├── nanji_LogisticRegression_Clf_classification_20250101_123456.pkl\n",
      "  ├── ... (최대 8개 모델)\n",
      "  ├── training_summary.csv\n",
      "  └── feature_importance_*.png (시각화 이미지들)\n",
      "\n",
      "2단계: 새로운 데이터로 예측 (반복 실행 가능)\n",
      "==================================================\n",
      "# CSV 파일로 예측\n",
      "results = predict_with_saved_models('새로운_데이터.csv')\n",
      "\n",
      "# DataFrame으로 예측\n",
      "new_df = pd.read_csv('새로운_데이터.csv')\n",
      "results = predict_with_saved_models(new_df)\n",
      "\n",
      "# 과거 데이터와 함께 예측 (더 정확한 시차 변수)\n",
      "historical_data = load_original_data()\n",
      "results = predict_with_saved_models('새로운_데이터.csv',\n",
      "                                   historical_data_for_features=historical_data)\n",
      "\n",
      "새로운 데이터 형식 예시:\n",
      "==============================\n",
      "날짜,일_평균기온(°C),일_일강수량(mm),평균습도(%),계절,불쾌지수등급\n",
      "2025-06-01,25.3,0,65,여름,약간 불쾌\n",
      "2025-06-02,26.1,2.5,70,여름,불쾌\n",
      "2025-06-03,24.8,15.2,75,여름,매우 불쾌\n",
      "\n",
      "예측 결과 형식:\n",
      "====================\n",
      "date,center,task_type,model_name,predicted_value\n",
      "2025-06-01,nanji,regression,RandomForest_Reg,1250.5\n",
      "2025-06-01,nanji,classification,LogisticRegression_Clf,2\n",
      "\n",
      "주의사항:\n",
      "===============\n",
      "- 모델 학습은 한 번만 실행하면 됩니다\n",
      "- 예측은 새로운 데이터가 있을 때마다 실행 가능\n",
      "- 데이터 경로가 올바른지 확인하세요\n",
      "- 새 데이터의 컬럼 구조는 기존 학습 데이터와 유사해야 합니다\n",
      "\n",
      "================================================================================\n",
      "완전한 하수처리장 예측 파이프라인 준비 완료!\n",
      "================================================================================\n",
      "\n",
      "다음 명령어로 시작하세요:\n",
      "\n",
      "# 1단계: 모델 학습\n",
      "saved_models, summary = train_and_save_best_models()\n",
      "\n",
      "# 2단계: 예측\n",
      "results = predict_with_saved_models('새로운_데이터.csv')\n",
      "\n",
      "사용 가이드: example_usage()\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================================\n",
    "# 완전한 하수처리장 예측 파이프라인 (학습-저장-예측)\n",
    "# ================================================================================================\n",
    "\n",
    "# 필수 라이브러리 import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# 선택적 라이브러리 import\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGB = True\n",
    "except ImportError:\n",
    "    HAS_LGB = False\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    HAS_CATBOOST = True\n",
    "except ImportError:\n",
    "    HAS_CATBOOST = False\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    HAS_SHAP = True\n",
    "except ImportError:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "try:\n",
    "    plt.rcParams['font.family'] = 'AppleGothic' # 맥\n",
    "except Exception:\n",
    "    plt.rcParams['font.family'] ='Malgun Gothic' # 윈도우\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"하수처리장 예측 파이프라인 초기화 완료\")\n",
    "print(f\"XGBoost: {'✓' if HAS_XGB else '✗'}\")\n",
    "print(f\"LightGBM: {'✓' if HAS_LGB else '✗'}\")\n",
    "print(f\"CatBoost: {'✓' if HAS_CATBOOST else '✗'}\")\n",
    "print(f\"SHAP: {'✓' if HAS_SHAP else '✗'}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ================================================================================================\n",
    "# 1. 모델 정의 함수들\n",
    "# ================================================================================================\n",
    "def build_regression_models():\n",
    "    \"\"\"회귀 모델들\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    models[\"RandomForest_Reg\"] = RandomForestRegressor(\n",
    "        n_estimators=300, min_samples_leaf=2, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    models[\"LinearRegression\"] = LinearRegression()\n",
    "    \n",
    "    models[\"GradientBoosting_Reg\"] = GradientBoostingRegressor(\n",
    "        n_estimators=200, learning_rate=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    if HAS_XGB:\n",
    "        models[\"XGBoost_Reg\"] = xgb.XGBRegressor(\n",
    "            n_estimators=400, max_depth=5, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            random_state=42, n_jobs=-1, verbosity=0\n",
    "        )\n",
    "    \n",
    "    if HAS_LGB:\n",
    "        models[\"LightGBM_Reg\"] = lgb.LGBMRegressor(\n",
    "            n_estimators=500, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            random_state=42, n_jobs=-1, verbosity=-1\n",
    "        )\n",
    "    \n",
    "    if HAS_CATBOOST:\n",
    "        models[\"CatBoost_Reg\"] = cb.CatBoostRegressor(\n",
    "            iterations=500, learning_rate=0.05, depth=6,\n",
    "            random_state=42, verbose=False\n",
    "        )\n",
    "    \n",
    "    return models\n",
    "\n",
    "def build_classification_models():\n",
    "    \"\"\"분류 모델들 (4등급)\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    models[\"RandomForest_Clf\"] = RandomForestClassifier(\n",
    "        n_estimators=300, min_samples_leaf=2, random_state=42, \n",
    "        n_jobs=-1, class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    models[\"GradientBoosting_Clf\"] = GradientBoostingClassifier(\n",
    "        n_estimators=200, learning_rate=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    models[\"LogisticRegression_Clf\"] = LogisticRegression(\n",
    "        multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=1000,\n",
    "        random_state=42, class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    if HAS_XGB:\n",
    "        models[\"XGBoost_Clf\"] = xgb.XGBClassifier(\n",
    "            n_estimators=400, max_depth=5, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            objective=\"multi:softprob\", num_class=4,\n",
    "            tree_method=\"hist\", random_state=42, n_jobs=-1, verbosity=0\n",
    "        )\n",
    "    \n",
    "    if HAS_LGB:\n",
    "        models[\"LightGBM_Clf\"] = lgb.LGBMClassifier(\n",
    "            n_estimators=500, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            objective=\"multiclass\", num_class=4,\n",
    "            random_state=42, n_jobs=-1, verbosity=-1, is_unbalance=True\n",
    "        )\n",
    "    \n",
    "    if HAS_CATBOOST:\n",
    "        models[\"CatBoost_Clf\"] = cb.CatBoostClassifier(\n",
    "            iterations=500, learning_rate=0.05, depth=6,\n",
    "            random_state=42, verbose=False, auto_class_weights='Balanced'\n",
    "        )\n",
    "    \n",
    "    return models\n",
    "\n",
    "# ================================================================================================\n",
    "# 2. 데이터 처리 함수들\n",
    "# ================================================================================================\n",
    "def make_pipeline_unified(model, model_name, model_type):\n",
    "    \"\"\"통합 전처리 파이프라인\"\"\"\n",
    "    if model_name in [\"LinearRegression\", \"LogisticRegression_Clf\"]:\n",
    "        pre = Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ])\n",
    "    else:\n",
    "        pre = Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        ])\n",
    "    return Pipeline(steps=[(\"pre\", pre), (\"model\", model)])\n",
    "\n",
    "def prepare_data_stratified(df, target_col, model_type, test_size=0.2, split_method='stratified'):\n",
    "    \"\"\"데이터 준비 - Stratified vs 시계열 분할\"\"\"\n",
    "    work = df.sort_values('날짜').reset_index(drop=True).copy()\n",
    "    dates = pd.to_datetime(work['날짜'])\n",
    "\n",
    "    not_use_col = [\n",
    "        '날짜',\n",
    "        '1처리장','2처리장','정화조','중계펌프장','합계','시설현대화',\n",
    "        '3처리장','4처리장','합계', '합계_1일후','합계_2일후',\n",
    "        '등급','등급_1일후','등급_2일후'\n",
    "    ]\n",
    "    \n",
    "    drop_cols = [c for c in (set(not_use_col) | {target_col}) if c in work.columns]\n",
    "    X_raw = work.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    \n",
    "    for c in X_raw.columns:\n",
    "        X_raw[c] = pd.to_numeric(X_raw[c], errors=\"coerce\")\n",
    "\n",
    "    if model_type == \"regression\":\n",
    "        y = pd.to_numeric(work[target_col], errors=\"coerce\")\n",
    "    else:\n",
    "        y = work[target_col].astype(\"int64\")\n",
    "\n",
    "    valid_idx = (~X_raw.isnull().all(axis=1)) & (~pd.isnull(y))\n",
    "    X_raw = X_raw[valid_idx].reset_index(drop=True)\n",
    "    y = y[valid_idx].reset_index(drop=True)\n",
    "    dates = dates[valid_idx].reset_index(drop=True)\n",
    "    \n",
    "    if split_method == 'stratified':\n",
    "        if model_type == \"classification\":\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "            train_idx, test_idx = next(sss.split(X_raw, y))\n",
    "        else:\n",
    "            train_idx, test_idx = train_test_split(\n",
    "                range(len(X_raw)), test_size=test_size, random_state=42\n",
    "            )\n",
    "            \n",
    "        X_train, X_test = X_raw.iloc[train_idx].copy(), X_raw.iloc[test_idx].copy()\n",
    "        y_train, y_test = y.iloc[train_idx].copy(), y.iloc[test_idx].copy()\n",
    "        dates_train, dates_test = dates.iloc[train_idx].copy(), dates.iloc[test_idx].copy()\n",
    "        \n",
    "    else:  # temporal split\n",
    "        n = len(X_raw)\n",
    "        split = int(n * (1 - test_size))\n",
    "        X_train, X_test = X_raw.iloc[:split].copy(), X_raw.iloc[split:].copy()\n",
    "        y_train, y_test = y.iloc[:split].copy(), y.iloc[split:].copy()\n",
    "        dates_train, dates_test = dates.iloc[:split].copy(), dates.iloc[split:].copy()\n",
    "\n",
    "    feature_names = list(X_raw.columns)\n",
    "    return X_train, X_test, y_train, y_test, feature_names, dates_train, dates_test\n",
    "\n",
    "def make_features(df, cutoff_date=None):\n",
    "    \"\"\"파생변수 생성 함수 - Data Leakage 방지 버전\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['날짜'] = pd.to_datetime(df['날짜'])\n",
    "    df = df.sort_values('날짜').reset_index(drop=True)\n",
    "\n",
    "    df['월'] = df['날짜'].dt.month\n",
    "    df['요일'] = df['날짜'].dt.weekday\n",
    "\n",
    "    season_map = {'봄': 0, '여름': 1, '가을': 2, '겨울': 3}\n",
    "    discomfort_map = {'쾌적': 0, '약간 불쾌': 1, '불쾌': 2, '매우 불쾌': 3, '극심한 불쾌': 4}\n",
    "    df['계절'] = df['계절'].map(season_map).astype('Int64')\n",
    "    df['불쾌지수등급'] = df['불쾌지수등급'].map(discomfort_map).astype('Int64')\n",
    "\n",
    "    # 강수량 시차 피처\n",
    "    df['강수량_1일전'] = df['일_일강수량(mm)'].shift(1)\n",
    "    df['강수량_2일전'] = df['일_일강수량(mm)'].shift(2)\n",
    "    df['강수량_1일_누적'] = df['일_일강수량(mm)'].rolling(1, min_periods=1).sum()\n",
    "    df['강수량_2일_누적'] = df['일_일강수량(mm)'].rolling(2, min_periods=1).sum()\n",
    "    df['강수량_3일_누적'] = df['일_일강수량(mm)'].rolling(3, min_periods=1).sum()\n",
    "    df['강수량_5일_누적'] = df['일_일강수량(mm)'].rolling(5, min_periods=1).sum()\n",
    "    df['강수량_7일_누적'] = df['일_일강수량(mm)'].rolling(7, min_periods=1).sum()\n",
    "\n",
    "    df['일교차'] = df['일_최고기온(°C)'] - df['일_최저기온(°C)']\n",
    "    df['폭우_여부'] = (df['일_일강수량(mm)'] >= 80).astype(int)\n",
    "    \n",
    "    # 체감온도 계산 (간단 버전)\n",
    "    T = pd.to_numeric(df.get('일_평균기온(°C)', np.nan), errors='coerce')\n",
    "    V_ms = pd.to_numeric(df.get('일_평균풍속(m/s)', np.nan), errors='coerce')\n",
    "    RH = pd.to_numeric(df.get('평균습도(%)', np.nan), errors='coerce')\n",
    "    \n",
    "    e = (RH/100.0) * 6.105 * np.exp(17.27*T/(237.7 + T))\n",
    "    df['체감온도(°C)'] = T + 0.33*e - 0.70*V_ms - 4.00\n",
    "    \n",
    "    # 분류용 등급 계산\n",
    "    q = df['합계'].dropna().quantile([0.15, 0.70, 0.90])\n",
    "    q15, q70, q90 = float(q.loc[0.15]), float(q.loc[0.70]), float(q.loc[0.90])\n",
    "\n",
    "    def categorize(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        if x < q15:\n",
    "            return 0\n",
    "        elif x < q70:\n",
    "            return 1\n",
    "        elif x < q90:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    df['등급'] = df['합계'].apply(categorize)\n",
    "    \n",
    "    # 타겟 변수 생성 (Data Leakage 방지)\n",
    "    if cutoff_date is not None:\n",
    "        cutoff = pd.to_datetime(cutoff_date)\n",
    "        \n",
    "        df['합계_1일후'] = np.nan\n",
    "        df['합계_2일후'] = np.nan\n",
    "        df['등급_1일후'] = np.nan\n",
    "        df['등급_2일후'] = np.nan\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            current_date = df.loc[i, '날짜']\n",
    "            \n",
    "            if i + 1 < len(df) and current_date <= cutoff:\n",
    "                next_date = df.loc[i+1, '날짜']\n",
    "                if next_date <= cutoff:\n",
    "                    df.loc[i, '합계_1일후'] = df.loc[i+1, '합계']\n",
    "                    df.loc[i, '등급_1일후'] = df.loc[i+1, '등급']\n",
    "            \n",
    "            if i + 2 < len(df) and current_date <= cutoff:\n",
    "                next2_date = df.loc[i+2, '날짜']\n",
    "                if next2_date <= cutoff:\n",
    "                    df.loc[i, '합계_2일후'] = df.loc[i+2, '합계']\n",
    "                    df.loc[i, '등급_2일후'] = df.loc[i+2, '등급']\n",
    "    else:\n",
    "        df['합계_1일후'] = df['합계'].shift(-1)\n",
    "        df['합계_2일후'] = df['합계'].shift(-2)\n",
    "        df['등급_1일후'] = df['등급'].shift(-1).astype('Int64')\n",
    "        df['등급_2일후'] = df['등급'].shift(-2).astype('Int64')\n",
    "\n",
    "    df.attrs['cutoffs'] = {\"q15\": q15, \"q70\": q70, \"q90\": q90}\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    df = df[df[\"날짜\"] < \"2025-06-01\"]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def make_features_for_prediction(historical_df, future_df):\n",
    "    \"\"\"새로운 데이터에 대한 파생변수 생성 (과거 데이터 활용)\"\"\"\n",
    "    combined_df = pd.concat([historical_df, future_df], ignore_index=True)\n",
    "    combined_df['날짜'] = pd.to_datetime(combined_df['날짜'])\n",
    "    combined_df = combined_df.sort_values('날짜').reset_index(drop=True)\n",
    "    \n",
    "    combined_df['월'] = combined_df['날짜'].dt.month\n",
    "    combined_df['요일'] = combined_df['날짜'].dt.weekday\n",
    "    \n",
    "    season_map = {'봄': 0, '여름': 1, '가을': 2, '겨울': 3}\n",
    "    discomfort_map = {'쾌적': 0, '약간 불쾌': 1, '불쾌': 2, '매우 불쾌': 3, '극심한 불쾌': 4}\n",
    "    combined_df['계절'] = combined_df['계절'].map(season_map).astype('Int64')\n",
    "    combined_df['불쾌지수등급'] = combined_df['불쾌지수등급'].map(discomfort_map).astype('Int64')\n",
    "    \n",
    "    # 시차 변수들\n",
    "    combined_df['강수량_1일전'] = combined_df['일_일강수량(mm)'].shift(1)\n",
    "    combined_df['강수량_2일전'] = combined_df['일_일강수량(mm)'].shift(2)\n",
    "    combined_df['강수량_1일_누적'] = combined_df['일_일강수량(mm)'].rolling(1, min_periods=1).sum()\n",
    "    combined_df['강수량_2일_누적'] = combined_df['일_일강수량(mm)'].rolling(2, min_periods=1).sum()\n",
    "    combined_df['강수량_3일_누적'] = combined_df['일_일강수량(mm)'].rolling(3, min_periods=1).sum()\n",
    "    combined_df['강수량_5일_누적'] = combined_df['일_일강수량(mm)'].rolling(5, min_periods=1).sum()\n",
    "    combined_df['강수량_7일_누적'] = combined_df['일_일강수량(mm)'].rolling(7, min_periods=1).sum()\n",
    "    \n",
    "    combined_df['일교차'] = combined_df['일_최고기온(°C)'] - combined_df['일_최저기온(°C)']\n",
    "    combined_df['폭우_여부'] = (combined_df['일_일강수량(mm)'] >= 80).astype(int)\n",
    "    \n",
    "    # 체감온도 계산\n",
    "    T = pd.to_numeric(combined_df.get('일_평균기온(°C)', np.nan), errors='coerce')\n",
    "    V_ms = pd.to_numeric(combined_df.get('일_평균풍속(m/s)', np.nan), errors='coerce')\n",
    "    RH = pd.to_numeric(combined_df.get('평균습도(%)', np.nan), errors='coerce')\n",
    "    \n",
    "    e = (RH/100.0) * 6.105 * np.exp(17.27*T/(237.7 + T))\n",
    "    combined_df['체감온도(°C)'] = T + 0.33*e - 0.70*V_ms - 4.00\n",
    "    \n",
    "    # 새 데이터 부분만 반환\n",
    "    historical_len = len(historical_df)\n",
    "    return combined_df.iloc[historical_len:].reset_index(drop=True)\n",
    "\n",
    "def make_simple_features(data):\n",
    "    \"\"\"간단한 피처 생성 (시차 변수 제외)\"\"\"\n",
    "    df = data.copy()\n",
    "    df['날짜'] = pd.to_datetime(df['날짜'])\n",
    "    df = df.sort_values('날짜').reset_index(drop=True)\n",
    "    \n",
    "    # 기본 피처들\n",
    "    df['월'] = df['날짜'].dt.month\n",
    "    df['요일'] = df['날짜'].dt.weekday\n",
    "    \n",
    "    # 계절/불쾌지수 매핑\n",
    "    season_map = {'봄': 0, '여름': 1, '가을': 2, '겨울': 3}\n",
    "    discomfort_map = {'쾌적': 0, '약간 불쾌': 1, '불쾌': 2, '매우 불쾌': 3, '극심한 불쾌': 4}\n",
    "    \n",
    "    if '계절' in df.columns:\n",
    "        df['계절'] = df['계절'].map(season_map).astype('Int64')\n",
    "    if '불쾌지수등급' in df.columns:\n",
    "        df['불쾌지수등급'] = df['불쾌지수등급'].map(discomfort_map).astype('Int64')\n",
    "    \n",
    "    # 기본 계산 피처들\n",
    "    if '일_최고기온(°C)' in df.columns and '일_최저기온(°C)' in df.columns:\n",
    "        df['일교차'] = df['일_최고기온(°C)'] - df['일_최저기온(°C)']\n",
    "    \n",
    "    if '일_일강수량(mm)' in df.columns:\n",
    "        df['폭우_여부'] = (df['일_일강수량(mm)'] >= 80).astype(int)\n",
    "    \n",
    "    # 체감온도 계산\n",
    "    T = pd.to_numeric(df.get('일_평균기온(°C)', np.nan), errors='coerce')\n",
    "    V_ms = pd.to_numeric(df.get('일_평균풍속(m/s)', np.nan), errors='coerce')\n",
    "    RH = pd.to_numeric(df.get('평균습도(%)', np.nan), errors='coerce')\n",
    "    \n",
    "    e = (RH/100.0) * 6.105 * np.exp(17.27*T/(237.7 + T))\n",
    "    df['체감온도(°C)'] = T + 0.33*e - 0.70*V_ms - 4.00\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ================================================================================================\n",
    "# 3. 평가 함수들\n",
    "# ================================================================================================\n",
    "def evaluate_regression_model(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"회귀 모델 평가\"\"\"\n",
    "    try:\n",
    "        pipe = make_pipeline_unified(model, model_name, \"regression\")\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = pipe.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-8))) * 100\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'regression',\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'mape': mape,\n",
    "            'success': True\n",
    "        }, pipe, y_pred\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'regression',\n",
    "            'mae': np.nan,\n",
    "            'rmse': np.nan,\n",
    "            'r2': np.nan,\n",
    "            'mape': np.nan,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }, None, None\n",
    "\n",
    "def evaluate_classification_model(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"분류 모델 평가\"\"\"\n",
    "    try:\n",
    "        pipe = make_pipeline_unified(model, model_name, \"classification\")\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = pipe.predict(X_test)\n",
    "        \n",
    "        if isinstance(y_pred, np.ndarray) and y_pred.ndim > 1:\n",
    "            y_pred = y_pred.ravel()\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1_macro = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "        f1_weighted = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "        \n",
    "        extreme_classes = [0, 3]\n",
    "        y_true_extreme = pd.Series(y_test).isin(extreme_classes).astype(int)\n",
    "        y_pred_extreme = pd.Series(y_pred).isin(extreme_classes).astype(int)\n",
    "        extreme_f1 = f1_score(y_true_extreme, y_pred_extreme, zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'classification',\n",
    "            'accuracy': acc,\n",
    "            'macro_f1': f1_macro,\n",
    "            'weighted_f1': f1_weighted,\n",
    "            'extreme_f1': extreme_f1,\n",
    "            'success': True\n",
    "        }, pipe, y_pred\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'classification',\n",
    "            'accuracy': np.nan,\n",
    "            'macro_f1': np.nan,\n",
    "            'weighted_f1': np.nan,\n",
    "            'extreme_f1': np.nan,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }, None, None\n",
    "\n",
    "def comprehensive_evaluation_comparison(center_name, df):\n",
    "    \"\"\"Stratified vs 시계열 분할 비교 평가\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"센터: {center_name} - Stratified vs 시계열 분할 비교\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"데이터 크기: {len(df)}행, {len(df.columns)}컬럼\")\n",
    "    \n",
    "    if '등급_1일후' in df.columns:\n",
    "        grade_dist = df['등급_1일후'].value_counts().sort_index()\n",
    "        print(f\"등급 분포: {dict(grade_dist)}\")\n",
    "        \n",
    "        min_class = grade_dist.min()\n",
    "        max_class = grade_dist.max()\n",
    "        imbalance_ratio = max_class / min_class\n",
    "        print(f\"클래스 불균형 비율: {imbalance_ratio:.1f}:1 (최대:{max_class}, 최소:{min_class})\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for split_method in ['temporal', 'stratified']:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"분할 방법: {split_method.upper()}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # 회귀 모델 평가\n",
    "        reg_method_name = \"random_shuffle\" if split_method == \"stratified\" else split_method\n",
    "        print(f\"\\n--- 회귀 모델 평가 ({reg_method_name}) ---\")\n",
    "        \n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test, feature_names, dates_train, dates_test = prepare_data_stratified(\n",
    "                df, target_col=\"합계_1일후\", model_type=\"regression\", test_size=0.2, split_method=split_method\n",
    "            )\n",
    "            \n",
    "            print(f\"회귀용 데이터: 학습 {len(X_train)}행, 테스트 {len(X_test)}행\")\n",
    "            \n",
    "            regression_models = build_regression_models()\n",
    "            \n",
    "            for model_name, model in tqdm(regression_models.items(), desc=f\"회귀({reg_method_name})\", leave=False):\n",
    "                result, pipe, y_pred = evaluate_regression_model(model, model_name, X_train, X_test, y_train, y_test)\n",
    "                result['center'] = center_name\n",
    "                result['split_method'] = split_method\n",
    "                results.append(result)\n",
    "                \n",
    "                if result['success']:\n",
    "                    print(f\"  {model_name:18s}: R²={result['r2']:.3f}, MAE={result['mae']:.0f}, MAPE={result['mape']:.1f}%\")\n",
    "                else:\n",
    "                    print(f\"  {model_name:18s}: 실패 - {result.get('error', '')[:50]}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"회귀 모델 평가 실패 ({reg_method_name}): {e}\")\n",
    "        \n",
    "        # 분류 모델 평가\n",
    "        print(f\"\\n--- 분류 모델 평가 ({split_method}) ---\")\n",
    "        \n",
    "        try:\n",
    "            X_train_clf, X_test_clf, y_train_clf, y_test_clf, feature_names_clf, _, _ = prepare_data_stratified(\n",
    "                df, target_col=\"등급_1일후\", model_type=\"classification\", test_size=0.2, split_method=split_method\n",
    "            )\n",
    "            \n",
    "            print(f\"분류용 데이터: 학습 {len(X_train_clf)}행, 테스트 {len(X_test_clf)}행\")\n",
    "            \n",
    "            test_dist = pd.Series(y_test_clf).value_counts().sort_index()\n",
    "            train_dist = pd.Series(y_train_clf).value_counts().sort_index()\n",
    "            print(f\"학습 세트 등급 분포: {dict(train_dist)}\")\n",
    "            print(f\"테스트 세트 등급 분포: {dict(test_dist)}\")\n",
    "            \n",
    "            classification_models = build_classification_models()\n",
    "            \n",
    "            for model_name, model in tqdm(classification_models.items(), desc=f\"분류({split_method})\", leave=False):\n",
    "                result, pipe, y_pred = evaluate_classification_model(model, model_name, X_train_clf, X_test_clf, y_train_clf, y_test_clf)\n",
    "                result['center'] = center_name\n",
    "                result['split_method'] = split_method\n",
    "                results.append(result)\n",
    "                \n",
    "                if result['success']:\n",
    "                    print(f\"  {model_name:18s}: ACC={result['accuracy']:.3f}, F1={result['macro_f1']:.3f}, 극값F1={result['extreme_f1']:.3f}\")\n",
    "                else:\n",
    "                    print(f\"  {model_name:18s}: 실패 - {result.get('error', '')[:50]}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"분류 모델 평가 실패 ({split_method}): {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ================================================================================================\n",
    "# 4. Feature Importance & SHAP 분석 함수들 (선택사항)\n",
    "# ================================================================================================\n",
    "def extract_feature_importance(model, model_name, feature_names):\n",
    "    \"\"\"모델별 Feature Importance 추출\"\"\"\n",
    "    try:\n",
    "        mdl = model.named_steps['model']\n",
    "        if hasattr(mdl, 'feature_importances_'):\n",
    "            importance = mdl.feature_importances_\n",
    "        elif hasattr(mdl, 'coef_'):\n",
    "            coef = mdl.coef_\n",
    "            if isinstance(coef, np.ndarray) and coef.ndim == 2:\n",
    "                importance = np.mean(np.abs(coef), axis=0)\n",
    "            else:\n",
    "                importance = np.abs(coef)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        if len(importance) != len(feature_names):\n",
    "            print(f\"[경고] importance 길이({len(importance)}) != feature_names({len(feature_names)})\")\n",
    "            m = min(len(importance), len(feature_names))\n",
    "            importance = np.asarray(importance)[:m]\n",
    "            feature_names = list(feature_names)[:m]\n",
    "\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        return importance_df\n",
    "    except Exception as e:\n",
    "        print(f\"Feature importance 추출 실패 ({model_name}): {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_feature_importance(importance_df, model_name, top_n=15):\n",
    "    \"\"\"Feature Importance 시각화\"\"\"\n",
    "    if importance_df is None or len(importance_df) == 0:\n",
    "        return None\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    top_features = importance_df.head(top_n)\n",
    "    \n",
    "    ax.barh(range(len(top_features)), top_features['importance'], color='skyblue')\n",
    "    ax.set_yticks(range(len(top_features)))\n",
    "    ax.set_yticklabels(top_features['feature'])\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title(f'{model_name} - Top {top_n} Feature Importance')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    for i, v in enumerate(top_features['importance']):\n",
    "        ax.text(v + 0.001, i, f'{v:.3f}', va='center')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analyze_model_with_shap(model, X_test, feature_names, model_name, max_samples=100):\n",
    "    \"\"\"SHAP 분석\"\"\"\n",
    "    if not HAS_SHAP:\n",
    "        print(\"SHAP 라이브러리가 설치되지 않았습니다.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if len(X_test) > max_samples:\n",
    "            sample_idx = np.random.choice(len(X_test), max_samples, replace=False)\n",
    "            X_sample = X_test.iloc[sample_idx]\n",
    "        else:\n",
    "            X_sample = X_test\n",
    "        \n",
    "        X_processed = model.named_steps['pre'].transform(X_sample)\n",
    "        \n",
    "        if 'RandomForest' in model_name or 'GradientBoosting' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        elif 'XGBoost' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        elif 'LightGBM' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        elif 'CatBoost' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        else:\n",
    "            explainer = shap.LinearExplainer(model.named_steps['model'], X_processed)\n",
    "        \n",
    "        shap_values = explainer.shap_values(X_processed)\n",
    "        \n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[0]\n",
    "        \n",
    "        return shap_values, X_processed, explainer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"SHAP 분석 실패 ({model_name}): {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_shap_summary(shap_values, X_processed, feature_names, model_name):\n",
    "    \"\"\"SHAP Summary Plot\"\"\"\n",
    "    if shap_values is None or not HAS_SHAP:\n",
    "        return []\n",
    "\n",
    "    figs = []\n",
    "    try:\n",
    "        # Bar plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values, X_processed,\n",
    "                          feature_names=feature_names,\n",
    "                          plot_type=\"bar\", show=False)\n",
    "        ax = plt.gca()\n",
    "        ax.set_title(f'{model_name} - SHAP Feature Importance')\n",
    "        figs.append(plt.gcf())\n",
    "\n",
    "        # Beeswarm plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values, X_processed,\n",
    "                          feature_names=feature_names,\n",
    "                          show=False)\n",
    "        ax = plt.gca()\n",
    "        ax.set_title(f'{model_name} - SHAP Summary Plot')\n",
    "        figs.append(plt.gcf())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"SHAP 시각화 실패 ({model_name}): {e}\")\n",
    "\n",
    "    return figs\n",
    "\n",
    "# ================================================================================================\n",
    "# 5. 유틸리티 함수들\n",
    "# ================================================================================================\n",
    "def load_original_data():\n",
    "    \"\"\"원본 데이터 로드\"\"\"\n",
    "    nanji_raw = pd.read_csv('../data/processed/center_season/nanji/난지_merged.csv', encoding='utf-8-sig')\n",
    "    jungnang_raw = pd.read_csv('../data/processed/center_season/jungnang/중랑_merged.csv', encoding='utf-8-sig')\n",
    "    seonam_raw = pd.read_csv('../data/processed/center_season/seonam/서남_merged.csv', encoding='utf-8-sig')\n",
    "    tancheon_raw = pd.read_csv('../data/processed/center_season/tancheon/탄천_merged.csv', encoding='utf-8-sig')\n",
    "    \n",
    "    return {\n",
    "        \"nanji\": nanji_raw,\n",
    "        \"jungnang\": jungnang_raw,\n",
    "        \"seonam\": seonam_raw,\n",
    "        \"tancheon\": tancheon_raw\n",
    "    }\n",
    "\n",
    "def prepare_prediction_features(future_data, expected_features):\n",
    "    \"\"\"예측용 피처 준비\"\"\"\n",
    "    not_use_col = [\n",
    "        '날짜', '1처리장','2처리장','정화조','중계펌프장','합계','시설현대화',\n",
    "        '3처리장','4처리장','합계', '합계_1일후','합계_2일후',\n",
    "        '등급','등급_1일후','등급_2일후'\n",
    "    ]\n",
    "    \n",
    "    available_cols = [col for col in future_data.columns if col not in not_use_col]\n",
    "    X_future = future_data[available_cols].copy()\n",
    "    \n",
    "    for c in X_future.columns:\n",
    "        X_future[c] = pd.to_numeric(X_future[c], errors=\"coerce\")\n",
    "    \n",
    "    missing_features = set(expected_features) - set(X_future.columns)\n",
    "    if missing_features:\n",
    "        for feature in missing_features:\n",
    "            X_future[feature] = 0\n",
    "    \n",
    "    X_future = X_future[expected_features].copy()\n",
    "    return X_future\n",
    "\n",
    "# ================================================================================================\n",
    "# 6. 1단계: 모델 학습 및 저장 파이프라인\n",
    "# ================================================================================================\n",
    "def train_and_save_best_models(cutoff_date='2025-05-20', save_dir='./trained_models'):\n",
    "    \"\"\"각 센터별 최고 성능 모델 학습 및 저장\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"1단계: 모델 학습 및 저장\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 저장 디렉토리 생성\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 데이터 로드\n",
    "    centers_data = load_original_data()\n",
    "    cutoff = pd.to_datetime(cutoff_date)\n",
    "    \n",
    "    saved_models = {}\n",
    "    training_summary = []\n",
    "    \n",
    "    for center_name, df_raw in centers_data.items():\n",
    "        print(f\"\\n[{center_name.upper()} 센터 모델 학습]\")\n",
    "        \n",
    "        # 학습 데이터 준비\n",
    "        df_raw['날짜'] = pd.to_datetime(df_raw['날짜'])\n",
    "        raw_train_data = df_raw[df_raw['날짜'] <= cutoff].copy()\n",
    "        \n",
    "        if len(raw_train_data) < 50:\n",
    "            print(f\"  학습 데이터 부족\")\n",
    "            continue\n",
    "        \n",
    "        # 피처 엔지니어링\n",
    "        train_data = make_features(raw_train_data, cutoff_date=cutoff_date)\n",
    "        print(f\"  학습 데이터: {len(train_data)}행\")\n",
    "        \n",
    "        # 모델 평가\n",
    "        training_results = comprehensive_evaluation_comparison(center_name, train_data)\n",
    "        \n",
    "        # 최고 성능 모델 선택 및 학습\n",
    "        best_models = select_and_save_best_models(center_name, train_data, training_results, save_dir)\n",
    "        \n",
    "        if best_models:\n",
    "            saved_models[center_name] = best_models\n",
    "            \n",
    "            # 학습 요약 정보 저장\n",
    "            for task_type, model_info in best_models.items():\n",
    "                training_summary.append({\n",
    "                    'center': center_name,\n",
    "                    'task_type': task_type,\n",
    "                    'model_name': model_info['model_name'],\n",
    "                    'performance': model_info['performance'],\n",
    "                    'saved_path': model_info['saved_path'],\n",
    "                    'training_date': datetime.now().isoformat()\n",
    "                })\n",
    "    \n",
    "    # 학습 요약 저장\n",
    "    summary_path = os.path.join(save_dir, 'training_summary.csv')\n",
    "    summary_df = pd.DataFrame(training_summary)\n",
    "    summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n={'='*60}\")\n",
    "    print(f\"모델 학습 완료!\")\n",
    "    print(f\"저장 위치: {save_dir}\")\n",
    "    print(f\"학습된 모델: {len(training_summary)}개\")\n",
    "    print(f\"학습 요약: {summary_path}\")\n",
    "    print(f\"={'='*60}\")\n",
    "    \n",
    "    return saved_models, training_summary\n",
    "\n",
    "def select_and_save_best_models(center_name, train_data, training_results, save_dir):\n",
    "    \"\"\"센터별 최고 성능 모델 선택 및 저장\"\"\"\n",
    "    results_df = pd.DataFrame(training_results)\n",
    "    successful_results = results_df[results_df['success'] == True]\n",
    "    \n",
    "    if len(successful_results) == 0:\n",
    "        return None\n",
    "    \n",
    "    saved_models = {}\n",
    "    \n",
    "    # 회귀 최고 성능 모델\n",
    "    reg_results = successful_results[successful_results['type'] == 'regression']\n",
    "    if len(reg_results) > 0:\n",
    "        best_reg = reg_results.loc[reg_results['r2'].idxmax()]\n",
    "        print(f\"  최고 회귀 모델: {best_reg['model']} (R²={best_reg['r2']:.3f})\")\n",
    "        \n",
    "        # 모델 학습 및 저장\n",
    "        model_data = train_and_save_single_model(\n",
    "            center_name, train_data, best_reg, 'regression', save_dir\n",
    "        )\n",
    "        \n",
    "        if model_data:\n",
    "            saved_models['regression'] = model_data\n",
    "    \n",
    "    # 분류 최고 성능 모델\n",
    "    clf_results = successful_results[successful_results['type'] == 'classification']\n",
    "    if len(clf_results) > 0:\n",
    "        best_clf = clf_results.loc[clf_results['macro_f1'].idxmax()]\n",
    "        print(f\"  최고 분류 모델: {best_clf['model']} (F1={best_clf['macro_f1']:.3f})\")\n",
    "        \n",
    "        # 모델 학습 및 저장\n",
    "        model_data = train_and_save_single_model(\n",
    "            center_name, train_data, best_clf, 'classification', save_dir\n",
    "        )\n",
    "        \n",
    "        if model_data:\n",
    "            saved_models['classification'] = model_data\n",
    "    \n",
    "    return saved_models\n",
    "\n",
    "def train_and_save_single_model(center_name, train_data, best_result, task_type, save_dir):\n",
    "    \"\"\"개별 모델 학습 및 저장\"\"\"\n",
    "    try:\n",
    "        model_name = best_result['model']\n",
    "        split_method = best_result['split_method']\n",
    "        target_col = \"합계_1일후\" if task_type == \"regression\" else \"등급_1일후\"\n",
    "        \n",
    "        # 전체 데이터로 학습\n",
    "        X_train, X_test, y_train, y_test, feature_names, _, _ = prepare_data_stratified(\n",
    "            train_data, target_col=target_col, model_type=task_type, \n",
    "            test_size=0.05, split_method=split_method\n",
    "        )\n",
    "        \n",
    "        X_all = pd.concat([X_train, X_test], ignore_index=True)\n",
    "        y_all = pd.concat([y_train, y_test], ignore_index=True)\n",
    "        \n",
    "        # 모델 구축 및 학습\n",
    "        if task_type == \"regression\":\n",
    "            models = build_regression_models()\n",
    "        else:\n",
    "            models = build_classification_models()\n",
    "        \n",
    "        model = models[model_name]\n",
    "        pipeline = make_pipeline_unified(model, model_name, task_type)\n",
    "        pipeline.fit(X_all, y_all)\n",
    "        \n",
    "        # 모델 저장\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{center_name}_{model_name}_{task_type}_{timestamp}.pkl\"\n",
    "        model_path = os.path.join(save_dir, filename)\n",
    "        \n",
    "        model_data = {\n",
    "            'pipeline': pipeline,\n",
    "            'feature_names': feature_names,\n",
    "            'model_name': model_name,\n",
    "            'center_name': center_name,\n",
    "            'task_type': task_type,\n",
    "            'performance': dict(best_result),\n",
    "            'split_method': split_method,\n",
    "            'training_date': datetime.now().isoformat(),\n",
    "            'target_column': target_col\n",
    "        }\n",
    "        \n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"    저장됨: {filename}\")\n",
    "        \n",
    "        # Feature Importance 분석 및 저장 (선택사항)\n",
    "        try:\n",
    "            importance_df = extract_feature_importance(pipeline, model_name, feature_names)\n",
    "            if importance_df is not None:\n",
    "                print(f\"    Top 5 피처: {', '.join(importance_df.head(5)['feature'].tolist())}\")\n",
    "                \n",
    "                # Feature Importance 시각화 저장\n",
    "                fig = plot_feature_importance(importance_df, f\"{center_name}_{model_name}\")\n",
    "                if fig:\n",
    "                    img_path = os.path.join(save_dir, f\"feature_importance_{center_name}_{model_name}_{task_type}_{timestamp}.png\")\n",
    "                    fig.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "                    plt.close(fig)\n",
    "                    print(f\"    Feature Importance 저장: {os.path.basename(img_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Feature Importance 분석 실패: {e}\")\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'performance': dict(best_result),\n",
    "            'saved_path': model_path,\n",
    "            'feature_names': feature_names,\n",
    "            'target_column': target_col\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    모델 저장 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "# ================================================================================================\n",
    "# 7. 2단계: 새로운 데이터로 예측 파이프라인\n",
    "# ================================================================================================\n",
    "def predict_with_saved_models(new_data_path, models_dir='./trained_models', \n",
    "                             historical_data_for_features=None):\n",
    "    \"\"\"저장된 모델들로 새로운 데이터 예측\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"2단계: 새로운 데이터 예측\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 새로운 데이터 로드\n",
    "    if isinstance(new_data_path, str):\n",
    "        new_data = pd.read_csv(new_data_path, encoding='utf-8-sig')\n",
    "        print(f\"새 데이터 로드: {new_data_path} ({len(new_data)}행)\")\n",
    "    else:\n",
    "        new_data = new_data_path.copy()\n",
    "        print(f\"새 데이터 제공됨: {len(new_data)}행\")\n",
    "    \n",
    "    # 저장된 모델 목록 확인\n",
    "    saved_models = load_all_saved_models(models_dir)\n",
    "    \n",
    "    if not saved_models:\n",
    "        print(\"저장된 모델이 없습니다. 먼저 train_and_save_best_models()를 실행하세요.\")\n",
    "        return None\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    # 센터별 예측\n",
    "    for center_name in saved_models.keys():\n",
    "        print(f\"\\n[{center_name.upper()} 센터 예측]\")\n",
    "        \n",
    "        # 해당 센터 데이터 필터링 (현재는 전체 데이터 사용)\n",
    "        center_data = new_data.copy()\n",
    "        \n",
    "        if len(center_data) == 0:\n",
    "            print(f\"  해당 센터 데이터 없음\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  예측 데이터: {len(center_data)}행\")\n",
    "        \n",
    "        # 피처 생성\n",
    "        if historical_data_for_features is not None:\n",
    "            processed_data = make_features_for_prediction(\n",
    "                historical_data_for_features[center_name], center_data\n",
    "            )\n",
    "        else:\n",
    "            processed_data = make_simple_features(center_data)\n",
    "        \n",
    "        # 센터별 모델로 예측\n",
    "        center_predictions = predict_for_center(center_name, processed_data, saved_models[center_name])\n",
    "        all_predictions.extend(center_predictions)\n",
    "    \n",
    "    # 결과 정리\n",
    "    if all_predictions:\n",
    "        results_df = pd.DataFrame(all_predictions)\n",
    "        results_df = results_df.sort_values(['center', 'date', 'task_type'])\n",
    "        \n",
    "        # 결과 저장\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = f\"predictions_{timestamp}.csv\"\n",
    "        results_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"\\n={'='*60}\")\n",
    "        print(f\"예측 완료!\")\n",
    "        print(f\"총 예측 건수: {len(results_df)}\")\n",
    "        print(f\"결과 저장: {output_path}\")\n",
    "        print(f\"={'='*60}\")\n",
    "        \n",
    "        # 예측 결과 요약 출력\n",
    "        print_prediction_summary(results_df)\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    else:\n",
    "        print(\"예측 결과가 없습니다.\")\n",
    "        return None\n",
    "\n",
    "def load_all_saved_models(models_dir):\n",
    "    \"\"\"저장된 모든 모델 로드\"\"\"\n",
    "    saved_models = {}\n",
    "    \n",
    "    if not os.path.exists(models_dir):\n",
    "        return saved_models\n",
    "    \n",
    "    model_files = [f for f in os.listdir(models_dir) if f.endswith('.pkl') and f != 'training_summary.csv']\n",
    "    \n",
    "    for model_file in model_files:\n",
    "        try:\n",
    "            model_path = os.path.join(models_dir, model_file)\n",
    "            \n",
    "            with open(model_path, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "            \n",
    "            center_name = model_data['center_name']\n",
    "            task_type = model_data['task_type']\n",
    "            \n",
    "            if center_name not in saved_models:\n",
    "                saved_models[center_name] = {}\n",
    "            \n",
    "            saved_models[center_name][task_type] = model_data\n",
    "            \n",
    "            print(f\"모델 로드: {center_name} {task_type} ({model_data['model_name']})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"모델 로드 실패 ({model_file}): {e}\")\n",
    "    \n",
    "    return saved_models\n",
    "\n",
    "def predict_for_center(center_name, processed_data, center_models):\n",
    "    \"\"\"센터별 모델로 예측\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for task_type, model_data in center_models.items():\n",
    "        try:\n",
    "            pipeline = model_data['pipeline']\n",
    "            feature_names = model_data['feature_names']\n",
    "            model_name = model_data['model_name']\n",
    "            target_col = model_data['target_column']\n",
    "            \n",
    "            # 예측 데이터 준비\n",
    "            X_future = prepare_prediction_features(processed_data, feature_names)\n",
    "            \n",
    "            if X_future is None or len(X_future) == 0:\n",
    "                continue\n",
    "            \n",
    "            # 예측 수행\n",
    "            y_pred = pipeline.predict(X_future)\n",
    "            \n",
    "            print(f\"  {task_type} 예측: {len(y_pred)}개\")\n",
    "            \n",
    "            # 결과 저장\n",
    "            for i in range(len(X_future)):\n",
    "                pred_result = {\n",
    "                    'date': processed_data.iloc[i]['날짜'],\n",
    "                    'center': center_name,\n",
    "                    'task_type': task_type,\n",
    "                    'model_name': model_name,\n",
    "                    'target_column': target_col,\n",
    "                    'predicted_value': int(y_pred[i]) if task_type == 'classification' else float(y_pred[i])\n",
    "                }\n",
    "                predictions.append(pred_result)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  {task_type} 예측 실패: {e}\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def print_prediction_summary(results_df):\n",
    "    \"\"\"예측 결과 요약 출력\"\"\"\n",
    "    print(f\"\\n--- 예측 결과 요약 ---\")\n",
    "    \n",
    "    for center in results_df['center'].unique():\n",
    "        center_data = results_df[results_df['center'] == center]\n",
    "        \n",
    "        print(f\"\\n{center.upper()} 센터:\")\n",
    "        \n",
    "        reg_data = center_data[center_data['task_type'] == 'regression']\n",
    "        clf_data = center_data[center_data['task_type'] == 'classification']\n",
    "        \n",
    "        if len(reg_data) > 0:\n",
    "            print(f\"  회귀 예측: {len(reg_data)}개\")\n",
    "            print(f\"  평균 예측값: {reg_data['predicted_value'].mean():.1f}\")\n",
    "            print(f\"  예측 범위: {reg_data['predicted_value'].min():.1f} ~ {reg_data['predicted_value'].max():.1f}\")\n",
    "        \n",
    "        if len(clf_data) > 0:\n",
    "            print(f\"  분류 예측: {len(clf_data)}개\")\n",
    "            grade_dist = clf_data['predicted_value'].value_counts().sort_index()\n",
    "            print(f\"  예측 등급 분포: {dict(grade_dist)}\")\n",
    "\n",
    "# ================================================================================================\n",
    "# 8. 사용 가이드 및 실행 예시\n",
    "# ================================================================================================\n",
    "def example_usage():\n",
    "    \"\"\"사용 예시 가이드\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"하수처리장 예측 파이프라인 사용 가이드\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n1단계: 모델 학습 및 저장 (한 번만 실행)\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"saved_models, summary = train_and_save_best_models(cutoff_date='2025-05-20')\")\n",
    "    print()\n",
    "    print(\"생성되는 파일:\")\n",
    "    print(\"- ./trained_models/ 폴더\")\n",
    "    print(\"  ├── nanji_RandomForest_Reg_regression_20250101_123456.pkl\")\n",
    "    print(\"  ├── nanji_LogisticRegression_Clf_classification_20250101_123456.pkl\")\n",
    "    print(\"  ├── ... (최대 8개 모델)\")\n",
    "    print(\"  ├── training_summary.csv\")\n",
    "    print(\"  └── feature_importance_*.png (시각화 이미지들)\")\n",
    "    \n",
    "    print(\"\\n2단계: 새로운 데이터로 예측 (반복 실행 가능)\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"# CSV 파일로 예측\")\n",
    "    print(\"results = predict_with_saved_models('새로운_데이터.csv')\")\n",
    "    print()\n",
    "    print(\"# DataFrame으로 예측\")\n",
    "    print(\"new_df = pd.read_csv('새로운_데이터.csv')\")\n",
    "    print(\"results = predict_with_saved_models(new_df)\")\n",
    "    print()\n",
    "    print(\"# 과거 데이터와 함께 예측 (더 정확한 시차 변수)\")\n",
    "    print(\"historical_data = load_original_data()\")\n",
    "    print(\"results = predict_with_saved_models('새로운_데이터.csv',\")\n",
    "    print(\"                                   historical_data_for_features=historical_data)\")\n",
    "    \n",
    "    print(\"\\n새로운 데이터 형식 예시:\")\n",
    "    print(\"=\"*30)\n",
    "    print(\"날짜,일_평균기온(°C),일_일강수량(mm),평균습도(%),계절,불쾌지수등급\")\n",
    "    print(\"2025-06-01,25.3,0,65,여름,약간 불쾌\")\n",
    "    print(\"2025-06-02,26.1,2.5,70,여름,불쾌\")\n",
    "    print(\"2025-06-03,24.8,15.2,75,여름,매우 불쾌\")\n",
    "    \n",
    "    print(\"\\n예측 결과 형식:\")\n",
    "    print(\"=\"*20)\n",
    "    print(\"date,center,task_type,model_name,predicted_value\")\n",
    "    print(\"2025-06-01,nanji,regression,RandomForest_Reg,1250.5\")\n",
    "    print(\"2025-06-01,nanji,classification,LogisticRegression_Clf,2\")\n",
    "    \n",
    "    print(\"\\n주의사항:\")\n",
    "    print(\"=\"*15)\n",
    "    print(\"- 모델 학습은 한 번만 실행하면 됩니다\")\n",
    "    print(\"- 예측은 새로운 데이터가 있을 때마다 실행 가능\")\n",
    "    print(\"- 데이터 경로가 올바른지 확인하세요\")\n",
    "    print(\"- 새 데이터의 컬럼 구조는 기존 학습 데이터와 유사해야 합니다\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()\n",
    "\n",
    "# ================================================================================================\n",
    "# 실행 준비 완료\n",
    "# ================================================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"완전한 하수처리장 예측 파이프라인 준비 완료!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n다음 명령어로 시작하세요:\")\n",
    "print(\"\\n# 1단계: 모델 학습\")\n",
    "print(\"saved_models, summary = train_and_save_best_models()\")\n",
    "print(\"\\n# 2단계: 예측\")\n",
    "print(\"results = predict_with_saved_models('새로운_데이터.csv')\")\n",
    "print(\"\\n사용 가이드: example_usage()\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb7a3036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "1단계: 모델 학습 및 저장\n",
      "================================================================================\n",
      "\n",
      "[NANJI 센터 모델 학습]\n",
      "  학습 데이터: 3058행\n",
      "\n",
      "======================================================================\n",
      "센터: nanji - Stratified vs 시계열 분할 비교\n",
      "======================================================================\n",
      "데이터 크기: 3058행, 44컬럼\n",
      "등급 분포: {0.0: 458, 1.0: 1682, 2.0: 611, 3.0: 307}\n",
      "클래스 불균형 비율: 5.5:1 (최대:1682, 최소:307)\n",
      "\n",
      "==================================================\n",
      "분할 방법: TEMPORAL\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (temporal) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  17%|█▋        | 1/6 [00:00<00:04,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.557, MAE=49542, MAPE=7.3%\n",
      "  LinearRegression  : R²=0.513, MAE=62439, MAPE=9.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  50%|█████     | 3/6 [00:02<00:02,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.395, MAE=58665, MAPE=9.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  67%|██████▋   | 4/6 [00:02<00:01,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.480, MAE=56356, MAPE=8.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "회귀(temporal):  83%|████████▎ | 5/6 [00:04<00:01,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.492, MAE=54005, MAPE=8.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.540, MAE=48369, MAPE=7.1%\n",
      "\n",
      "--- 분류 모델 평가 (temporal) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 453, 1: 1309, 2: 435, 3: 249}\n",
      "테스트 세트 등급 분포: {0: 5, 1: 373, 2: 176, 3: 58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  17%|█▋        | 1/6 [00:00<00:01,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.748, F1=0.481, 극값F1=0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  33%|███▎      | 2/6 [00:06<00:15,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.665, F1=0.468, 극값F1=0.386\n",
      "  LogisticRegression_Clf: ACC=0.314, F1=0.308, 극값F1=0.199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "분류(temporal):  67%|██████▋   | 4/6 [00:08<00:03,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.722, F1=0.567, 극값F1=0.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "분류(temporal):  83%|████████▎ | 5/6 [00:15<00:03,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.694, F1=0.527, 극값F1=0.432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.672, F1=0.508, 극값F1=0.404\n",
      "\n",
      "==================================================\n",
      "분할 방법: STRATIFIED\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (random_shuffle) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  17%|█▋        | 1/6 [00:00<00:03,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.612, MAE=47300, MAPE=7.0%\n",
      "  LinearRegression  : R²=0.592, MAE=50530, MAPE=7.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  50%|█████     | 3/6 [00:02<00:02,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.620, MAE=46878, MAPE=6.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  67%|██████▋   | 4/6 [00:02<00:01,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.637, MAE=45859, MAPE=6.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "회귀(random_shuffle):  83%|████████▎ | 5/6 [00:04<00:01,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.605, MAE=48431, MAPE=7.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.648, MAE=44875, MAPE=6.6%\n",
      "\n",
      "--- 분류 모델 평가 (stratified) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 366, 1: 1345, 2: 489, 3: 246}\n",
      "테스트 세트 등급 분포: {0: 92, 1: 337, 2: 122, 3: 61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  17%|█▋        | 1/6 [00:00<00:01,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.725, F1=0.664, 극값F1=0.660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  33%|███▎      | 2/6 [00:06<00:15,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.725, F1=0.658, 극값F1=0.648\n",
      "  LogisticRegression_Clf: ACC=0.613, F1=0.578, 극값F1=0.597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "분류(stratified):  67%|██████▋   | 4/6 [00:08<00:04,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.745, F1=0.667, 극값F1=0.645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "분류(stratified):  83%|████████▎ | 5/6 [00:15<00:03,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.735, F1=0.665, 극값F1=0.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.675, F1=0.630, 극값F1=0.632\n",
      "  최고 회귀 모델: CatBoost_Reg (R²=0.648)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    저장됨: nanji_CatBoost_Reg_regression_20250827_123501.pkl\n",
      "    Top 5 피처: 강수량_7일_누적, 하천, 강수량_1일_누적, 세탁업, 체감온도(°C)\n",
      "    Feature Importance 저장: feature_importance_nanji_CatBoost_Reg_regression_20250827_123501.png\n",
      "  최고 분류 모델: XGBoost_Clf (F1=0.667)\n",
      "    저장됨: nanji_XGBoost_Clf_classification_20250827_123503.pkl\n",
      "    Top 5 피처: 체감온도(°C), 일_최저기온(°C), 강수량_7일_누적, 강수량_2일_누적, 목욕장업\n",
      "    Feature Importance 저장: feature_importance_nanji_XGBoost_Clf_classification_20250827_123503.png\n",
      "\n",
      "[JUNGNANG 센터 모델 학습]\n",
      "  학습 데이터: 3058행\n",
      "\n",
      "======================================================================\n",
      "센터: jungnang - Stratified vs 시계열 분할 비교\n",
      "======================================================================\n",
      "데이터 크기: 3058행, 44컬럼\n",
      "등급 분포: {0.0: 460, 1.0: 1680, 2.0: 611, 3.0: 307}\n",
      "클래스 불균형 비율: 5.5:1 (최대:1680, 최소:307)\n",
      "\n",
      "==================================================\n",
      "분할 방법: TEMPORAL\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (temporal) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  17%|█▋        | 1/6 [00:00<00:04,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.304, MAE=91088, MAPE=7.1%\n",
      "  LinearRegression  : R²=0.055, MAE=131262, MAPE=10.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  50%|█████     | 3/6 [00:02<00:02,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.137, MAE=105765, MAPE=8.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  67%|██████▋   | 4/6 [00:03<00:01,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.269, MAE=96690, MAPE=7.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "회귀(temporal):  83%|████████▎ | 5/6 [00:05<00:01,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.254, MAE=96761, MAPE=7.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.259, MAE=99822, MAPE=7.9%\n",
      "\n",
      "--- 분류 모델 평가 (temporal) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 405, 1: 1321, 2: 458, 3: 262}\n",
      "테스트 세트 등급 분포: {0: 55, 1: 359, 2: 153, 3: 45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  17%|█▋        | 1/6 [00:00<00:01,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.627, F1=0.425, 극값F1=0.311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  33%|███▎      | 2/6 [00:07<00:16,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.547, F1=0.380, 극값F1=0.303\n",
      "  LogisticRegression_Clf: ACC=0.359, F1=0.296, 극값F1=0.312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "분류(temporal):  67%|██████▋   | 4/6 [00:09<00:04,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.541, F1=0.442, 극값F1=0.342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "분류(temporal):  83%|████████▎ | 5/6 [00:17<00:04,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.531, F1=0.468, 극값F1=0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.554, F1=0.411, 극값F1=0.325\n",
      "\n",
      "==================================================\n",
      "분할 방법: STRATIFIED\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (random_shuffle) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  17%|█▋        | 1/6 [00:00<00:03,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.576, MAE=76240, MAPE=5.5%\n",
      "  LinearRegression  : R²=0.522, MAE=89786, MAPE=6.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  50%|█████     | 3/6 [00:02<00:02,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.572, MAE=78344, MAPE=5.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  67%|██████▋   | 4/6 [00:02<00:01,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.575, MAE=76245, MAPE=5.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "회귀(random_shuffle):  83%|████████▎ | 5/6 [00:04<00:01,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.562, MAE=78893, MAPE=5.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.602, MAE=75195, MAPE=5.5%\n",
      "\n",
      "--- 분류 모델 평가 (stratified) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 368, 1: 1344, 2: 489, 3: 245}\n",
      "테스트 세트 등급 분포: {0: 92, 1: 336, 2: 122, 3: 62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  17%|█▋        | 1/6 [00:00<00:01,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.711, F1=0.629, 극값F1=0.606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  33%|███▎      | 2/6 [00:07<00:17,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.730, F1=0.640, 극값F1=0.603\n",
      "  LogisticRegression_Clf: ACC=0.560, F1=0.548, 극값F1=0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "분류(stratified):  67%|██████▋   | 4/6 [00:09<00:04,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.745, F1=0.661, 극값F1=0.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "분류(stratified):  83%|████████▎ | 5/6 [00:15<00:03,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.739, F1=0.647, 극값F1=0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.691, F1=0.653, 극값F1=0.654\n",
      "  최고 회귀 모델: CatBoost_Reg (R²=0.602)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    저장됨: jungnang_CatBoost_Reg_regression_20250827_123551.pkl\n",
      "    Top 5 피처: 강수량_7일_누적, 세탁업, 체력단련장업, 목욕장업, 하천\n",
      "    Feature Importance 저장: feature_importance_jungnang_CatBoost_Reg_regression_20250827_123551.png\n",
      "  최고 분류 모델: XGBoost_Clf (F1=0.661)\n",
      "    저장됨: jungnang_XGBoost_Clf_classification_20250827_123553.pkl\n",
      "    Top 5 피처: 수영장업, 체력단련장업, 강수량_7일_누적, 목욕장업, 일_최저기온(°C)\n",
      "    Feature Importance 저장: feature_importance_jungnang_XGBoost_Clf_classification_20250827_123553.png\n",
      "\n",
      "[SEONAM 센터 모델 학습]\n",
      "  학습 데이터: 3058행\n",
      "\n",
      "======================================================================\n",
      "센터: seonam - Stratified vs 시계열 분할 비교\n",
      "======================================================================\n",
      "데이터 크기: 3058행, 43컬럼\n",
      "등급 분포: {0.0: 459, 1.0: 1680, 2.0: 612, 3.0: 307}\n",
      "클래스 불균형 비율: 5.5:1 (최대:1680, 최소:307)\n",
      "\n",
      "==================================================\n",
      "분할 방법: TEMPORAL\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (temporal) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  17%|█▋        | 1/6 [00:00<00:04,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.037, MAE=157633, MAPE=10.9%\n",
      "  LinearRegression  : R²=0.058, MAE=156666, MAPE=10.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  50%|█████     | 3/6 [00:02<00:02,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.270, MAE=124493, MAPE=8.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  67%|██████▋   | 4/6 [00:03<00:01,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.244, MAE=127234, MAPE=8.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "회귀(temporal):  83%|████████▎ | 5/6 [00:04<00:01,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.280, MAE=120014, MAPE=8.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.327, MAE=124794, MAPE=8.5%\n",
      "\n",
      "--- 분류 모델 평가 (temporal) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 136, 1: 1493, 2: 544, 3: 273}\n",
      "테스트 세트 등급 분포: {0: 323, 1: 187, 2: 68, 3: 34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  17%|█▋        | 1/6 [00:00<00:01,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.304, F1=0.323, 극값F1=0.124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  33%|███▎      | 2/6 [00:07<00:16,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.477, F1=0.432, 극값F1=0.593\n",
      "  LogisticRegression_Clf: ACC=0.271, F1=0.306, 극값F1=0.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "분류(temporal):  67%|██████▋   | 4/6 [00:08<00:04,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.317, F1=0.307, 극값F1=0.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "분류(temporal):  83%|████████▎ | 5/6 [00:16<00:03,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.301, F1=0.265, 극값F1=0.057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.252, F1=0.265, 극값F1=0.149\n",
      "\n",
      "==================================================\n",
      "분할 방법: STRATIFIED\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (random_shuffle) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  17%|█▋        | 1/6 [00:00<00:04,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.562, MAE=91557, MAPE=5.4%\n",
      "  LinearRegression  : R²=0.528, MAE=100152, MAPE=6.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  50%|█████     | 3/6 [00:02<00:02,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.604, MAE=86879, MAPE=5.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  67%|██████▋   | 4/6 [00:03<00:01,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.590, MAE=87201, MAPE=5.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "회귀(random_shuffle):  83%|████████▎ | 5/6 [00:04<00:01,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.578, MAE=89065, MAPE=5.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.594, MAE=86488, MAPE=5.1%\n",
      "\n",
      "--- 분류 모델 평가 (stratified) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 367, 1: 1344, 2: 489, 3: 246}\n",
      "테스트 세트 등급 분포: {0: 92, 1: 336, 2: 123, 3: 61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  17%|█▋        | 1/6 [00:00<00:01,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.727, F1=0.652, 극값F1=0.669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  33%|███▎      | 2/6 [00:07<00:16,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.735, F1=0.655, 극값F1=0.662\n",
      "  LogisticRegression_Clf: ACC=0.595, F1=0.562, 극값F1=0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "분류(stratified):  67%|██████▋   | 4/6 [00:09<00:04,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.745, F1=0.658, 극값F1=0.659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "분류(stratified):  83%|████████▎ | 5/6 [00:16<00:03,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.745, F1=0.652, 극값F1=0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.699, F1=0.644, 극값F1=0.673\n",
      "  최고 회귀 모델: GradientBoosting_Reg (R²=0.604)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    저장됨: seonam_GradientBoosting_Reg_regression_20250827_123642.pkl\n",
      "    Top 5 피처: 일_일강수량(mm), 강수량_1일_누적, 강수량_7일_누적, 일_최저기온(°C), 세탁업\n",
      "    Feature Importance 저장: feature_importance_seonam_GradientBoosting_Reg_regression_20250827_123642.png\n",
      "  최고 분류 모델: XGBoost_Clf (F1=0.658)\n",
      "    저장됨: seonam_XGBoost_Clf_classification_20250827_123644.pkl\n",
      "    Top 5 피처: 일_일강수량(mm), 강수량_1일_누적, 체력단련장업, 폭우_여부, 목욕장업\n",
      "    Feature Importance 저장: feature_importance_seonam_XGBoost_Clf_classification_20250827_123644.png\n",
      "\n",
      "[TANCHEON 센터 모델 학습]\n",
      "  학습 데이터: 3058행\n",
      "\n",
      "======================================================================\n",
      "센터: tancheon - Stratified vs 시계열 분할 비교\n",
      "======================================================================\n",
      "데이터 크기: 3058행, 42컬럼\n",
      "등급 분포: {0.0: 459, 1.0: 1680, 2.0: 612, 3.0: 307}\n",
      "클래스 불균형 비율: 5.5:1 (최대:1680, 최소:307)\n",
      "\n",
      "==================================================\n",
      "분할 방법: TEMPORAL\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (temporal) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  17%|█▋        | 1/6 [00:00<00:03,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.275, MAE=62889, MAPE=8.3%\n",
      "  LinearRegression  : R²=0.287, MAE=66726, MAPE=8.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  50%|█████     | 3/6 [00:02<00:02,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.027, MAE=75273, MAPE=10.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  67%|██████▋   | 4/6 [00:02<00:01,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.109, MAE=72585, MAPE=9.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "회귀(temporal):  83%|████████▎ | 5/6 [00:04<00:01,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.202, MAE=66029, MAPE=8.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.197, MAE=68829, MAPE=9.2%\n",
      "\n",
      "--- 분류 모델 평가 (temporal) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 345, 1: 1348, 2: 496, 3: 257}\n",
      "테스트 세트 등급 분포: {0: 114, 1: 332, 2: 116, 3: 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  17%|█▋        | 1/6 [00:00<00:01,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.557, F1=0.410, 극값F1=0.246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  33%|███▎      | 2/6 [00:06<00:15,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.364, F1=0.361, 극값F1=0.371\n",
      "  LogisticRegression_Clf: ACC=0.268, F1=0.259, 극값F1=0.444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "분류(temporal):  67%|██████▋   | 4/6 [00:08<00:03,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.477, F1=0.405, 극값F1=0.322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "분류(temporal):  83%|████████▎ | 5/6 [00:15<00:03,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.454, F1=0.382, 극값F1=0.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.358, F1=0.354, 극값F1=0.369\n",
      "\n",
      "==================================================\n",
      "분할 방법: STRATIFIED\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (random_shuffle) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  17%|█▋        | 1/6 [00:00<00:03,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.500, MAE=48022, MAPE=6.0%\n",
      "  LinearRegression  : R²=0.377, MAE=57635, MAPE=7.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  50%|█████     | 3/6 [00:02<00:02,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.510, MAE=47588, MAPE=6.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  67%|██████▋   | 4/6 [00:02<00:01,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.514, MAE=46528, MAPE=5.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "회귀(random_shuffle):  83%|████████▎ | 5/6 [00:04<00:01,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.482, MAE=48315, MAPE=6.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.515, MAE=47014, MAPE=5.9%\n",
      "\n",
      "--- 분류 모델 평가 (stratified) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 367, 1: 1344, 2: 489, 3: 246}\n",
      "테스트 세트 등급 분포: {0: 92, 1: 336, 2: 123, 3: 61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  17%|█▋        | 1/6 [00:00<00:01,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.641, F1=0.538, 극값F1=0.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  33%|███▎      | 2/6 [00:06<00:15,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.636, F1=0.524, 극값F1=0.485\n",
      "  LogisticRegression_Clf: ACC=0.449, F1=0.432, 극값F1=0.459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "분류(stratified):  67%|██████▋   | 4/6 [00:08<00:04,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.658, F1=0.547, 극값F1=0.488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "분류(stratified):  83%|████████▎ | 5/6 [00:15<00:03,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.647, F1=0.545, 극값F1=0.515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.614, F1=0.559, 극값F1=0.577\n",
      "  최고 회귀 모델: CatBoost_Reg (R²=0.515)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    저장됨: tancheon_CatBoost_Reg_regression_20250827_123729.pkl\n",
      "    Top 5 피처: 세탁업, 체력단련장업, 강수량_7일_누적, 하천, 일_일강수량(mm)\n",
      "    Feature Importance 저장: feature_importance_tancheon_CatBoost_Reg_regression_20250827_123729.png\n",
      "  최고 분류 모델: CatBoost_Clf (F1=0.559)\n",
      "    저장됨: tancheon_CatBoost_Clf_classification_20250827_123730.pkl\n",
      "    Top 5 피처: 세탁업, 강수량_7일_누적, 생활인구, 월, 체력단련장업\n",
      "    Feature Importance 저장: feature_importance_tancheon_CatBoost_Clf_classification_20250827_123730.png\n",
      "\n",
      "=============================================================\n",
      "모델 학습 완료!\n",
      "저장 위치: ./trained_models\n",
      "학습된 모델: 8개\n",
      "학습 요약: ./trained_models/training_summary.csv\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "# 1. 위 코드 전체 실행 (함수들 로드)\n",
    "\n",
    "# 2. 모델 학습 (한 번만)\n",
    "saved_models, summary = train_and_save_best_models()\n",
    "\n",
    "# 3. 예측 (필요할 때마다)\n",
    "# results = predict_with_saved_models('새_데이터.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e233ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22e1ecec",
   "metadata": {},
   "source": [
    "# 다시 버전 13\n",
    "\n",
    "**1단계 결과 (모델 학습):**\n",
    "\n",
    "**파일:**\n",
    "- `./trained_models/nanji_RandomForest_Reg_regression_20250101_123456.pkl` (난지 회귀 모델)\n",
    "- `./trained_models/nanji_LogisticRegression_Clf_classification_20250101_123456.pkl` (난지 분류 모델)\n",
    "- `./trained_models/jungnang_XGBoost_Reg_regression_20250101_123456.pkl` (중랑 회귀 모델)\n",
    "- `./trained_models/jungnang_RandomForest_Clf_classification_20250101_123456.pkl` (중랑 분류 모델)\n",
    "- ... (서남, 탄천도 각각 2개씩 = 총 8개 모델)\n",
    "- `./trained_models/training_summary.csv` (학습 결과 요약)\n",
    "- `./results/feature_importance_nanji_RandomForest_Reg_regression_*.png` (총 8개 시각화)\n",
    "\n",
    "**콘솔 출력:**\n",
    "```\n",
    "난지 센터 최고 회귀 모델: RandomForest_Reg (R²=0.851)\n",
    "난지 센터 최고 분류 모델: LogisticRegression_Clf (F1=0.743)\n",
    "중랑 센터 최고 회귀 모델: XGBoost_Reg (R²=0.823)\n",
    "...\n",
    "```\n",
    "\n",
    "**2단계 결과 (예측):**\n",
    "\n",
    "**파일:**\n",
    "- `./results/predictions_20250101_143022.csv`\n",
    "\n",
    "**predictions CSV 내용:**\n",
    "```csv\n",
    "date,center,task_type,model_name,target_column,predicted_value\n",
    "2025-06-01,nanji,regression,RandomForest_Reg,합계_1일후,1247.3\n",
    "2025-06-01,nanji,classification,LogisticRegression_Clf,등급_1일후,2\n",
    "2025-06-01,jungnang,regression,XGBoost_Reg,합계_1일후,982.1\n",
    "2025-06-01,jungnang,classification,RandomForest_Clf,등급_1일후,1\n",
    "2025-06-01,seonam,regression,LightGBM_Reg,합계_1일후,1456.8\n",
    "2025-06-01,seonam,classification,XGBoost_Clf,등급_1일후,3\n",
    "2025-06-01,tancheon,regression,CatBoost_Reg,합계_1일후,678.9\n",
    "2025-06-01,tancheon,classification,GradientBoosting_Clf,등급_1일후,1\n",
    "```\n",
    "\n",
    "**콘솔 출력:**\n",
    "```\n",
    "NANJI 센터:\n",
    "  회귀 예측: 10개\n",
    "  평균 예측값: 1234.5\n",
    "  예측 범위: 980.2 ~ 1456.7\n",
    "  분류 예측: 10개\n",
    "  예측 등급 분포: {0: 2, 1: 4, 2: 3, 3: 1}\n",
    "```\n",
    "\n",
    "**핵심:**\n",
    "- **predicted_value**: 실제 예측값 (회귀는 하수처리량, 분류는 0~3 등급)\n",
    "- **4개 센터 × 2개 태스크 × N일** 만큼 예측 결과 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cc8c418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "하수처리장 예측 파이프라인 초기화 완료\n",
      "XGBoost: ✓\n",
      "LightGBM: ✓\n",
      "CatBoost: ✓\n",
      "SHAP: ✓\n",
      "================================================================================\n",
      "================================================================================\n",
      "하수처리장 예측 파이프라인 사용 가이드\n",
      "================================================================================\n",
      "\n",
      "1단계: 모델 학습 및 저장 (한 번만 실행)\n",
      "==================================================\n",
      "saved_models, summary = train_and_save_best_models(cutoff_date='2025-05-20')\n",
      "\n",
      "생성되는 파일:\n",
      "- ./trained_models/ 폴더\n",
      "  ├── nanji_RandomForest_Reg_regression_20250101_123456.pkl\n",
      "  ├── nanji_LogisticRegression_Clf_classification_20250101_123456.pkl\n",
      "  ├── ... (최대 8개 모델)\n",
      "  ├── training_summary.csv\n",
      "  └── feature_importance_*.png (시각화 이미지들)\n",
      "\n",
      "2단계: 새로운 데이터로 예측 (반복 실행 가능)\n",
      "==================================================\n",
      "# CSV 파일로 예측\n",
      "results = predict_with_saved_models('새로운_데이터.csv')\n",
      "\n",
      "# DataFrame으로 예측\n",
      "new_df = pd.read_csv('새로운_데이터.csv')\n",
      "results = predict_with_saved_models(new_df)\n",
      "\n",
      "# 과거 데이터와 함께 예측 (더 정확한 시차 변수)\n",
      "historical_data = load_original_data()\n",
      "results = predict_with_saved_models('새로운_데이터.csv',\n",
      "                                   historical_data_for_features=historical_data)\n",
      "\n",
      "새로운 데이터 형식 예시:\n",
      "==============================\n",
      "날짜,일_평균기온(°C),일_일강수량(mm),평균습도(%),계절,불쾌지수등급\n",
      "2025-06-01,25.3,0,65,여름,약간 불쾌\n",
      "2025-06-02,26.1,2.5,70,여름,불쾌\n",
      "2025-06-03,24.8,15.2,75,여름,매우 불쾌\n",
      "\n",
      "예측 결과 형식:\n",
      "====================\n",
      "date,center,task_type,model_name,predicted_value\n",
      "2025-06-01,nanji,regression,RandomForest_Reg,1250.5\n",
      "2025-06-01,nanji,classification,LogisticRegression_Clf,2\n",
      "\n",
      "주의사항:\n",
      "===============\n",
      "- 모델 학습은 한 번만 실행하면 됩니다\n",
      "- 예측은 새로운 데이터가 있을 때마다 실행 가능\n",
      "- 데이터 경로가 올바른지 확인하세요\n",
      "- 새 데이터의 컬럼 구조는 기존 학습 데이터와 유사해야 합니다\n",
      "\n",
      "================================================================================\n",
      "완전한 하수처리장 예측 파이프라인 준비 완료!\n",
      "================================================================================\n",
      "\n",
      "다음 명령어로 시작하세요:\n",
      "\n",
      "# 1단계: 모델 학습\n",
      "saved_models, summary = train_and_save_best_models()\n",
      "\n",
      "# 2단계: 예측\n",
      "results = predict_with_saved_models('새로운_데이터.csv')\n",
      "\n",
      "사용 가이드: example_usage()\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================================\n",
    "# 완전한 하수처리장 예측 파이프라인 (학습-저장-예측)\n",
    "# ================================================================================================\n",
    "\n",
    "# 필수 라이브러리 import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import time, os, json, warnings\n",
    "import joblib\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# 선택적 라이브러리 import\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGB = True\n",
    "except ImportError:\n",
    "    HAS_LGB = False\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    HAS_CATBOOST = True\n",
    "except ImportError:\n",
    "    HAS_CATBOOST = False\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    HAS_SHAP = True\n",
    "except ImportError:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "# 한글 폰트 설정\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "try:\n",
    "    plt.rcParams['font.family'] = 'AppleGothic' # 맥\n",
    "except Exception:\n",
    "    plt.rcParams['font.family'] ='Malgun Gothic' # 윈도우\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"하수처리장 예측 파이프라인 초기화 완료\")\n",
    "print(f\"XGBoost: {'✓' if HAS_XGB else '✗'}\")\n",
    "print(f\"LightGBM: {'✓' if HAS_LGB else '✗'}\")\n",
    "print(f\"CatBoost: {'✓' if HAS_CATBOOST else '✗'}\")\n",
    "print(f\"SHAP: {'✓' if HAS_SHAP else '✗'}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ================================================================================================\n",
    "# 1. 모델 정의 함수들\n",
    "# ================================================================================================\n",
    "def build_regression_models():\n",
    "    \"\"\"회귀 모델들\"\"\"\n",
    "    models = {}\n",
    "    models[\"RandomForest_Reg\"] = RandomForestRegressor(\n",
    "        n_estimators=300, min_samples_leaf=2, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    models[\"LinearRegression\"] = LinearRegression()\n",
    "    models[\"GradientBoosting_Reg\"] = GradientBoostingRegressor(\n",
    "        n_estimators=200, learning_rate=0.1, random_state=42\n",
    "    )\n",
    "    if HAS_XGB:\n",
    "        models[\"XGBoost_Reg\"] = xgb.XGBRegressor(\n",
    "            n_estimators=400, max_depth=5, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            random_state=42, n_jobs=-1, verbosity=0\n",
    "        )\n",
    "    if HAS_LGB:\n",
    "        models[\"LightGBM_Reg\"] = lgb.LGBMRegressor(\n",
    "            n_estimators=500, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            random_state=42, n_jobs=-1, verbosity=-1\n",
    "        )\n",
    "    if HAS_CATBOOST:\n",
    "        models[\"CatBoost_Reg\"] = cb.CatBoostRegressor(\n",
    "            iterations=500, learning_rate=0.05, depth=6,\n",
    "            random_state=42, verbose=False\n",
    "        )\n",
    "    return models\n",
    "\n",
    "def build_classification_models():\n",
    "    \"\"\"분류 모델들 (4등급)\"\"\"\n",
    "    models = {}\n",
    "    models[\"RandomForest_Clf\"] = RandomForestClassifier(\n",
    "        n_estimators=300, min_samples_leaf=2, random_state=42, \n",
    "        n_jobs=-1, class_weight='balanced'\n",
    "    ) \n",
    "    models[\"GradientBoosting_Clf\"] = GradientBoostingClassifier(\n",
    "        n_estimators=200, learning_rate=0.1, random_state=42\n",
    "    )\n",
    "    models[\"LogisticRegression_Clf\"] = LogisticRegression(\n",
    "        multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=1000,\n",
    "        random_state=42, class_weight='balanced'\n",
    "    )\n",
    "    if HAS_XGB:\n",
    "        models[\"XGBoost_Clf\"] = xgb.XGBClassifier(\n",
    "            n_estimators=400, max_depth=5, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            objective=\"multi:softprob\", num_class=4,\n",
    "            tree_method=\"hist\", random_state=42, n_jobs=-1, verbosity=0\n",
    "        )\n",
    "    if HAS_LGB:\n",
    "        models[\"LightGBM_Clf\"] = lgb.LGBMClassifier(\n",
    "            n_estimators=500, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            objective=\"multiclass\", num_class=4,\n",
    "            random_state=42, n_jobs=-1, verbosity=-1, is_unbalance=True\n",
    "        )\n",
    "    if HAS_CATBOOST:\n",
    "        models[\"CatBoost_Clf\"] = cb.CatBoostClassifier(\n",
    "            iterations=500, learning_rate=0.05, depth=6,\n",
    "            random_state=42, verbose=False, auto_class_weights='Balanced'\n",
    "        )\n",
    "    return models\n",
    "\n",
    "# ================================================================================================\n",
    "# 2. 데이터 처리 함수들\n",
    "# ================================================================================================\n",
    "def make_pipeline_unified(model, model_name, model_type):\n",
    "    \"\"\"통합 전처리 파이프라인\"\"\"\n",
    "    if model_name in [\"LinearRegression\", \"LogisticRegression_Clf\"]:\n",
    "        pre = Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ])\n",
    "    else:\n",
    "        pre = Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        ])\n",
    "    return Pipeline(steps=[(\"pre\", pre), (\"model\", model)])\n",
    "\n",
    "def prepare_data_stratified(df, target_col, model_type, test_size=0.2, split_method='stratified'):\n",
    "    \"\"\"데이터 준비 - Stratified vs 시계열 분할\"\"\"\n",
    "    work = df.sort_values('날짜').reset_index(drop=True).copy()\n",
    "    dates = pd.to_datetime(work['날짜'])\n",
    "\n",
    "    not_use_col = [\n",
    "        '날짜',\n",
    "        '1처리장','2처리장','정화조','중계펌프장','합계','시설현대화',\n",
    "        '3처리장','4처리장','합계', '합계_1일후','합계_2일후',\n",
    "        '등급','등급_1일후','등급_2일후'\n",
    "    ]\n",
    "    \n",
    "    drop_cols = [c for c in (set(not_use_col) | {target_col}) if c in work.columns]\n",
    "    X_raw = work.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    \n",
    "    for c in X_raw.columns:\n",
    "        X_raw[c] = pd.to_numeric(X_raw[c], errors=\"coerce\")\n",
    "\n",
    "    if model_type == \"regression\":\n",
    "        y = pd.to_numeric(work[target_col], errors=\"coerce\")\n",
    "    else:\n",
    "        y = work[target_col].astype(\"int64\")\n",
    "\n",
    "    valid_idx = (~X_raw.isnull().all(axis=1)) & (~pd.isnull(y))\n",
    "    X_raw = X_raw[valid_idx].reset_index(drop=True)\n",
    "    y = y[valid_idx].reset_index(drop=True)\n",
    "    dates = dates[valid_idx].reset_index(drop=True)\n",
    "    \n",
    "    if split_method == 'stratified':\n",
    "        if model_type == \"classification\":\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "            train_idx, test_idx = next(sss.split(X_raw, y))\n",
    "        else:\n",
    "            train_idx, test_idx = train_test_split(\n",
    "                range(len(X_raw)), test_size=test_size, random_state=42\n",
    "            )\n",
    "            \n",
    "        X_train, X_test = X_raw.iloc[train_idx].copy(), X_raw.iloc[test_idx].copy()\n",
    "        y_train, y_test = y.iloc[train_idx].copy(), y.iloc[test_idx].copy()\n",
    "        dates_train, dates_test = dates.iloc[train_idx].copy(), dates.iloc[test_idx].copy()\n",
    "        \n",
    "    else:  # temporal split\n",
    "        n = len(X_raw)\n",
    "        split = int(n * (1 - test_size))\n",
    "        X_train, X_test = X_raw.iloc[:split].copy(), X_raw.iloc[split:].copy()\n",
    "        y_train, y_test = y.iloc[:split].copy(), y.iloc[split:].copy()\n",
    "        dates_train, dates_test = dates.iloc[:split].copy(), dates.iloc[split:].copy()\n",
    "\n",
    "    feature_names = list(X_raw.columns)\n",
    "    return X_train, X_test, y_train, y_test, feature_names, dates_train, dates_test\n",
    "\n",
    "def make_features(df, cutoff_date=None):\n",
    "    \"\"\"파생변수 생성 함수 - Data Leakage 방지 버전\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['날짜'] = pd.to_datetime(df['날짜'])\n",
    "    df = df.sort_values('날짜').reset_index(drop=True)\n",
    "\n",
    "    df['월'] = df['날짜'].dt.month\n",
    "    df['요일'] = df['날짜'].dt.weekday\n",
    "\n",
    "    season_map = {'봄': 0, '여름': 1, '가을': 2, '겨울': 3}\n",
    "    discomfort_map = {'쾌적': 0, '약간 불쾌': 1, '불쾌': 2, '매우 불쾌': 3, '극심한 불쾌': 4}\n",
    "    df['계절'] = df['계절'].map(season_map).astype('Int64')\n",
    "    df['불쾌지수등급'] = df['불쾌지수등급'].map(discomfort_map).astype('Int64')\n",
    "\n",
    "    # 강수량 시차 피처\n",
    "    df['강수량_1일전'] = df['일_일강수량(mm)'].shift(1)\n",
    "    df['강수량_2일전'] = df['일_일강수량(mm)'].shift(2)\n",
    "    df['강수량_1일_누적'] = df['일_일강수량(mm)'].rolling(1, min_periods=1).sum()\n",
    "    df['강수량_2일_누적'] = df['일_일강수량(mm)'].rolling(2, min_periods=1).sum()\n",
    "    df['강수량_3일_누적'] = df['일_일강수량(mm)'].rolling(3, min_periods=1).sum()\n",
    "    df['강수량_5일_누적'] = df['일_일강수량(mm)'].rolling(5, min_periods=1).sum()\n",
    "    df['강수량_7일_누적'] = df['일_일강수량(mm)'].rolling(7, min_periods=1).sum()\n",
    "\n",
    "    df['일교차'] = df['일_최고기온(°C)'] - df['일_최저기온(°C)']\n",
    "    df['폭우_여부'] = (df['일_일강수량(mm)'] >= 80).astype(int)\n",
    "    \n",
    "    # 체감온도 계산 (간단 버전)\n",
    "    T = pd.to_numeric(df.get('일_평균기온(°C)', np.nan), errors='coerce')\n",
    "    V_ms = pd.to_numeric(df.get('일_평균풍속(m/s)', np.nan), errors='coerce')\n",
    "    RH = pd.to_numeric(df.get('평균습도(%)', np.nan), errors='coerce')\n",
    "    \n",
    "    e = (RH/100.0) * 6.105 * np.exp(17.27*T/(237.7 + T))\n",
    "    df['체감온도(°C)'] = T + 0.33*e - 0.70*V_ms - 4.00\n",
    "    \n",
    "    # 분류용 등급 계산\n",
    "    q = df['합계'].dropna().quantile([0.15, 0.70, 0.90])\n",
    "    q15, q70, q90 = float(q.loc[0.15]), float(q.loc[0.70]), float(q.loc[0.90])\n",
    "\n",
    "    def categorize(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        if x < q15:\n",
    "            return 0\n",
    "        elif x < q70:\n",
    "            return 1\n",
    "        elif x < q90:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    df['등급'] = df['합계'].apply(categorize)\n",
    "    \n",
    "    # 타겟 변수 생성 (Data Leakage 방지)\n",
    "    if cutoff_date is not None:\n",
    "        cutoff = pd.to_datetime(cutoff_date)\n",
    "        \n",
    "        df['합계_1일후'] = np.nan\n",
    "        df['합계_2일후'] = np.nan\n",
    "        df['등급_1일후'] = np.nan\n",
    "        df['등급_2일후'] = np.nan\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            current_date = df.loc[i, '날짜']\n",
    "            \n",
    "            if i + 1 < len(df) and current_date <= cutoff:\n",
    "                next_date = df.loc[i+1, '날짜']\n",
    "                if next_date <= cutoff:\n",
    "                    df.loc[i, '합계_1일후'] = df.loc[i+1, '합계']\n",
    "                    df.loc[i, '등급_1일후'] = df.loc[i+1, '등급']\n",
    "            \n",
    "            if i + 2 < len(df) and current_date <= cutoff:\n",
    "                next2_date = df.loc[i+2, '날짜']\n",
    "                if next2_date <= cutoff:\n",
    "                    df.loc[i, '합계_2일후'] = df.loc[i+2, '합계']\n",
    "                    df.loc[i, '등급_2일후'] = df.loc[i+2, '등급']\n",
    "    else:\n",
    "        df['합계_1일후'] = df['합계'].shift(-1)\n",
    "        df['합계_2일후'] = df['합계'].shift(-2)\n",
    "        df['등급_1일후'] = df['등급'].shift(-1).astype('Int64')\n",
    "        df['등급_2일후'] = df['등급'].shift(-2).astype('Int64')\n",
    "\n",
    "    df.attrs['cutoffs'] = {\"q15\": q15, \"q70\": q70, \"q90\": q90}\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    df = df[df[\"날짜\"] < \"2025-06-01\"]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def make_features_for_prediction(historical_df, future_df):\n",
    "    \"\"\"새로운 데이터에 대한 파생변수 생성 (과거 데이터 활용)\"\"\"\n",
    "    combined_df = pd.concat([historical_df, future_df], ignore_index=True)\n",
    "    combined_df['날짜'] = pd.to_datetime(combined_df['날짜'])\n",
    "    combined_df = combined_df.sort_values('날짜').reset_index(drop=True)\n",
    "    \n",
    "    combined_df['월'] = combined_df['날짜'].dt.month\n",
    "    combined_df['요일'] = combined_df['날짜'].dt.weekday\n",
    "    \n",
    "    season_map = {'봄': 0, '여름': 1, '가을': 2, '겨울': 3}\n",
    "    discomfort_map = {'쾌적': 0, '약간 불쾌': 1, '불쾌': 2, '매우 불쾌': 3, '극심한 불쾌': 4}\n",
    "    combined_df['계절'] = combined_df['계절'].map(season_map).astype('Int64')\n",
    "    combined_df['불쾌지수등급'] = combined_df['불쾌지수등급'].map(discomfort_map).astype('Int64')\n",
    "    \n",
    "    # 시차 변수들\n",
    "    combined_df['강수량_1일전'] = combined_df['일_일강수량(mm)'].shift(1)\n",
    "    combined_df['강수량_2일전'] = combined_df['일_일강수량(mm)'].shift(2)\n",
    "    combined_df['강수량_1일_누적'] = combined_df['일_일강수량(mm)'].rolling(1, min_periods=1).sum()\n",
    "    combined_df['강수량_2일_누적'] = combined_df['일_일강수량(mm)'].rolling(2, min_periods=1).sum()\n",
    "    combined_df['강수량_3일_누적'] = combined_df['일_일강수량(mm)'].rolling(3, min_periods=1).sum()\n",
    "    combined_df['강수량_5일_누적'] = combined_df['일_일강수량(mm)'].rolling(5, min_periods=1).sum()\n",
    "    combined_df['강수량_7일_누적'] = combined_df['일_일강수량(mm)'].rolling(7, min_periods=1).sum()\n",
    "    \n",
    "    combined_df['일교차'] = combined_df['일_최고기온(°C)'] - combined_df['일_최저기온(°C)']\n",
    "    combined_df['폭우_여부'] = (combined_df['일_일강수량(mm)'] >= 80).astype(int)\n",
    "    \n",
    "    # 체감온도 계산\n",
    "    T = pd.to_numeric(combined_df.get('일_평균기온(°C)', np.nan), errors='coerce')\n",
    "    V_ms = pd.to_numeric(combined_df.get('일_평균풍속(m/s)', np.nan), errors='coerce')\n",
    "    RH = pd.to_numeric(combined_df.get('평균습도(%)', np.nan), errors='coerce')\n",
    "    \n",
    "    e = (RH/100.0) * 6.105 * np.exp(17.27*T/(237.7 + T))\n",
    "    combined_df['체감온도(°C)'] = T + 0.33*e - 0.70*V_ms - 4.00\n",
    "    \n",
    "    # 새 데이터 부분만 반환\n",
    "    historical_len = len(historical_df)\n",
    "    return combined_df.iloc[historical_len:].reset_index(drop=True)\n",
    "\n",
    "def make_simple_features(data):\n",
    "    \"\"\"간단한 피처 생성 (시차 변수 제외)\"\"\"\n",
    "    df = data.copy()\n",
    "    df['날짜'] = pd.to_datetime(df['날짜'])\n",
    "    df = df.sort_values('날짜').reset_index(drop=True)\n",
    "    \n",
    "    # 기본 피처들\n",
    "    df['월'] = df['날짜'].dt.month\n",
    "    df['요일'] = df['날짜'].dt.weekday\n",
    "    \n",
    "    # 계절/불쾌지수 매핑\n",
    "    season_map = {'봄': 0, '여름': 1, '가을': 2, '겨울': 3}\n",
    "    discomfort_map = {'쾌적': 0, '약간 불쾌': 1, '불쾌': 2, '매우 불쾌': 3, '극심한 불쾌': 4}\n",
    "    \n",
    "    if '계절' in df.columns:\n",
    "        df['계절'] = df['계절'].map(season_map).astype('Int64')\n",
    "    if '불쾌지수등급' in df.columns:\n",
    "        df['불쾌지수등급'] = df['불쾌지수등급'].map(discomfort_map).astype('Int64')\n",
    "    \n",
    "    # 기본 계산 피처들\n",
    "    if '일_최고기온(°C)' in df.columns and '일_최저기온(°C)' in df.columns:\n",
    "        df['일교차'] = df['일_최고기온(°C)'] - df['일_최저기온(°C)']\n",
    "    \n",
    "    if '일_일강수량(mm)' in df.columns:\n",
    "        df['폭우_여부'] = (df['일_일강수량(mm)'] >= 80).astype(int)\n",
    "    \n",
    "    # 체감온도 계산\n",
    "    T = pd.to_numeric(df.get('일_평균기온(°C)', np.nan), errors='coerce')\n",
    "    V_ms = pd.to_numeric(df.get('일_평균풍속(m/s)', np.nan), errors='coerce')\n",
    "    RH = pd.to_numeric(df.get('평균습도(%)', np.nan), errors='coerce')\n",
    "    \n",
    "    e = (RH/100.0) * 6.105 * np.exp(17.27*T/(237.7 + T))\n",
    "    df['체감온도(°C)'] = T + 0.33*e - 0.70*V_ms - 4.00\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ================================================================================================\n",
    "# 3. 평가 함수들\n",
    "# ================================================================================================\n",
    "def evaluate_regression_model(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"회귀 모델 평가\"\"\"\n",
    "    try:\n",
    "        pipe = make_pipeline_unified(model, model_name, \"regression\")\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = pipe.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-8))) * 100\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'regression',\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'mape': mape,\n",
    "            'success': True\n",
    "        }, pipe, y_pred\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'regression',\n",
    "            'mae': np.nan,\n",
    "            'rmse': np.nan,\n",
    "            'r2': np.nan,\n",
    "            'mape': np.nan,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }, None, None\n",
    "\n",
    "def evaluate_classification_model(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"분류 모델 평가\"\"\"\n",
    "    try:\n",
    "        pipe = make_pipeline_unified(model, model_name, \"classification\")\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = pipe.predict(X_test)\n",
    "        \n",
    "        if isinstance(y_pred, np.ndarray) and y_pred.ndim > 1:\n",
    "            y_pred = y_pred.ravel()\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1_macro = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "        f1_weighted = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "        \n",
    "        extreme_classes = [0, 3]\n",
    "        y_true_extreme = pd.Series(y_test).isin(extreme_classes).astype(int)\n",
    "        y_pred_extreme = pd.Series(y_pred).isin(extreme_classes).astype(int)\n",
    "        extreme_f1 = f1_score(y_true_extreme, y_pred_extreme, zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'classification',\n",
    "            'accuracy': acc,\n",
    "            'macro_f1': f1_macro,\n",
    "            'weighted_f1': f1_weighted,\n",
    "            'extreme_f1': extreme_f1,\n",
    "            'success': True\n",
    "        }, pipe, y_pred\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'classification',\n",
    "            'accuracy': np.nan,\n",
    "            'macro_f1': np.nan,\n",
    "            'weighted_f1': np.nan,\n",
    "            'extreme_f1': np.nan,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }, None, None\n",
    "\n",
    "def comprehensive_evaluation_comparison(center_name, df):\n",
    "    \"\"\"Stratified vs 시계열 분할 비교 평가\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"센터: {center_name} - Stratified vs 시계열 분할 비교\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"데이터 크기: {len(df)}행, {len(df.columns)}컬럼\")\n",
    "    \n",
    "    if '등급_1일후' in df.columns:\n",
    "        grade_dist = df['등급_1일후'].value_counts().sort_index()\n",
    "        print(f\"등급 분포: {dict(grade_dist)}\")\n",
    "        \n",
    "        min_class = grade_dist.min()\n",
    "        max_class = grade_dist.max()\n",
    "        imbalance_ratio = max_class / min_class\n",
    "        print(f\"클래스 불균형 비율: {imbalance_ratio:.1f}:1 (최대:{max_class}, 최소:{min_class})\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for split_method in ['temporal', 'stratified']:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"분할 방법: {split_method.upper()}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # 회귀 모델 평가\n",
    "        reg_method_name = \"random_shuffle\" if split_method == \"stratified\" else split_method\n",
    "        print(f\"\\n--- 회귀 모델 평가 ({reg_method_name}) ---\")\n",
    "        \n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test, feature_names, dates_train, dates_test = prepare_data_stratified(\n",
    "                df, target_col=\"합계_1일후\", model_type=\"regression\", test_size=0.2, split_method=split_method\n",
    "            )\n",
    "            \n",
    "            print(f\"회귀용 데이터: 학습 {len(X_train)}행, 테스트 {len(X_test)}행\")\n",
    "            \n",
    "            regression_models = build_regression_models()\n",
    "            \n",
    "            for model_name, model in tqdm(regression_models.items(), desc=f\"회귀({reg_method_name})\", leave=False):\n",
    "                result, pipe, y_pred = evaluate_regression_model(model, model_name, X_train, X_test, y_train, y_test)\n",
    "                result['center'] = center_name\n",
    "                result['split_method'] = split_method\n",
    "                results.append(result)\n",
    "                \n",
    "                if result['success']:\n",
    "                    print(f\"  {model_name:18s}: R²={result['r2']:.3f}, MAE={result['mae']:.0f}, MAPE={result['mape']:.1f}%\")\n",
    "                else:\n",
    "                    print(f\"  {model_name:18s}: 실패 - {result.get('error', '')[:50]}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"회귀 모델 평가 실패 ({reg_method_name}): {e}\")\n",
    "        \n",
    "        # 분류 모델 평가\n",
    "        print(f\"\\n--- 분류 모델 평가 ({split_method}) ---\")\n",
    "        \n",
    "        try:\n",
    "            X_train_clf, X_test_clf, y_train_clf, y_test_clf, feature_names_clf, _, _ = prepare_data_stratified(\n",
    "                df, target_col=\"등급_1일후\", model_type=\"classification\", test_size=0.2, split_method=split_method\n",
    "            )\n",
    "            \n",
    "            print(f\"분류용 데이터: 학습 {len(X_train_clf)}행, 테스트 {len(X_test_clf)}행\")\n",
    "            \n",
    "            test_dist = pd.Series(y_test_clf).value_counts().sort_index()\n",
    "            train_dist = pd.Series(y_train_clf).value_counts().sort_index()\n",
    "            print(f\"학습 세트 등급 분포: {dict(train_dist)}\")\n",
    "            print(f\"테스트 세트 등급 분포: {dict(test_dist)}\")\n",
    "            \n",
    "            classification_models = build_classification_models()\n",
    "            \n",
    "            for model_name, model in tqdm(classification_models.items(), desc=f\"분류({split_method})\", leave=False):\n",
    "                result, pipe, y_pred = evaluate_classification_model(model, model_name, X_train_clf, X_test_clf, y_train_clf, y_test_clf)\n",
    "                result['center'] = center_name\n",
    "                result['split_method'] = split_method\n",
    "                results.append(result)\n",
    "                \n",
    "                if result['success']:\n",
    "                    print(f\"  {model_name:18s}: ACC={result['accuracy']:.3f}, F1={result['macro_f1']:.3f}, 극값F1={result['extreme_f1']:.3f}\")\n",
    "                else:\n",
    "                    print(f\"  {model_name:18s}: 실패 - {result.get('error', '')[:50]}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"분류 모델 평가 실패 ({split_method}): {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ================================================================================================\n",
    "# 4. Feature Importance & SHAP 분석 함수들 (선택사항)\n",
    "# ================================================================================================\n",
    "def extract_feature_importance(model, model_name, feature_names):\n",
    "    \"\"\"모델별 Feature Importance 추출\"\"\"\n",
    "    try:\n",
    "        mdl = model.named_steps['model']\n",
    "        if hasattr(mdl, 'feature_importances_'):\n",
    "            importance = mdl.feature_importances_\n",
    "        elif hasattr(mdl, 'coef_'):\n",
    "            coef = mdl.coef_\n",
    "            if isinstance(coef, np.ndarray) and coef.ndim == 2:\n",
    "                importance = np.mean(np.abs(coef), axis=0)\n",
    "            else:\n",
    "                importance = np.abs(coef)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        if len(importance) != len(feature_names):\n",
    "            print(f\"[경고] importance 길이({len(importance)}) != feature_names({len(feature_names)})\")\n",
    "            m = min(len(importance), len(feature_names))\n",
    "            importance = np.asarray(importance)[:m]\n",
    "            feature_names = list(feature_names)[:m]\n",
    "\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        return importance_df\n",
    "    except Exception as e:\n",
    "        print(f\"Feature importance 추출 실패 ({model_name}): {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_feature_importance(importance_df, model_name, top_n=15):\n",
    "    \"\"\"Feature Importance 시각화\"\"\"\n",
    "    if importance_df is None or len(importance_df) == 0:\n",
    "        return None\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    top_features = importance_df.head(top_n)\n",
    "    \n",
    "    ax.barh(range(len(top_features)), top_features['importance'], color='skyblue')\n",
    "    ax.set_yticks(range(len(top_features)))\n",
    "    ax.set_yticklabels(top_features['feature'])\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title(f'{model_name} - Top {top_n} Feature Importance')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    for i, v in enumerate(top_features['importance']):\n",
    "        ax.text(v + 0.001, i, f'{v:.3f}', va='center')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analyze_model_with_shap(model, X_test, feature_names, model_name, max_samples=100):\n",
    "    \"\"\"SHAP 분석\"\"\"\n",
    "    if not HAS_SHAP:\n",
    "        print(\"SHAP 라이브러리가 설치되지 않았습니다.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if len(X_test) > max_samples:\n",
    "            sample_idx = np.random.choice(len(X_test), max_samples, replace=False)\n",
    "            X_sample = X_test.iloc[sample_idx]\n",
    "        else:\n",
    "            X_sample = X_test\n",
    "        \n",
    "        X_processed = model.named_steps['pre'].transform(X_sample)\n",
    "        \n",
    "        if 'RandomForest' in model_name or 'GradientBoosting' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        elif 'XGBoost' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        elif 'LightGBM' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        elif 'CatBoost' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        else:\n",
    "            explainer = shap.LinearExplainer(model.named_steps['model'], X_processed)\n",
    "        \n",
    "        shap_values = explainer.shap_values(X_processed)\n",
    "        \n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[0]\n",
    "        \n",
    "        return shap_values, X_processed, explainer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"SHAP 분석 실패 ({model_name}): {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_shap_summary(shap_values, X_processed, feature_names, model_name):\n",
    "    \"\"\"SHAP Summary Plot\"\"\"\n",
    "    if shap_values is None or not HAS_SHAP:\n",
    "        return []\n",
    "\n",
    "    figs = []\n",
    "    try:\n",
    "        # Bar plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values, X_processed,\n",
    "                          feature_names=feature_names,\n",
    "                          plot_type=\"bar\", show=False)\n",
    "        ax = plt.gca()\n",
    "        ax.set_title(f'{model_name} - SHAP Feature Importance')\n",
    "        figs.append(plt.gcf())\n",
    "\n",
    "        # Beeswarm plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values, X_processed,\n",
    "                          feature_names=feature_names,\n",
    "                          show=False)\n",
    "        ax = plt.gca()\n",
    "        ax.set_title(f'{model_name} - SHAP Summary Plot')\n",
    "        figs.append(plt.gcf())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"SHAP 시각화 실패 ({model_name}): {e}\")\n",
    "\n",
    "    return figs\n",
    "\n",
    "# ================================================================================================\n",
    "# 5. 유틸리티 함수들\n",
    "# ================================================================================================\n",
    "def load_original_data():\n",
    "    \"\"\"원본 데이터 로드\"\"\"\n",
    "    nanji_raw = pd.read_csv('../data/processed/center_season/nanji/난지_merged.csv', encoding='utf-8-sig')\n",
    "    jungnang_raw = pd.read_csv('../data/processed/center_season/jungnang/중랑_merged.csv', encoding='utf-8-sig')\n",
    "    seonam_raw = pd.read_csv('../data/processed/center_season/seonam/서남_merged.csv', encoding='utf-8-sig')\n",
    "    tancheon_raw = pd.read_csv('../data/processed/center_season/tancheon/탄천_merged.csv', encoding='utf-8-sig')\n",
    "    \n",
    "    return {\n",
    "        \"nanji\": nanji_raw,\n",
    "        \"jungnang\": jungnang_raw,\n",
    "        \"seonam\": seonam_raw,\n",
    "        \"tancheon\": tancheon_raw\n",
    "    }\n",
    "\n",
    "def prepare_prediction_features(future_data, expected_features):\n",
    "    \"\"\"예측용 피처 준비\"\"\"\n",
    "    not_use_col = [\n",
    "        '날짜', '1처리장','2처리장','정화조','중계펌프장','합계','시설현대화',\n",
    "        '3처리장','4처리장','합계', '합계_1일후','합계_2일후',\n",
    "        '등급','등급_1일후','등급_2일후'\n",
    "    ]\n",
    "    \n",
    "    available_cols = [col for col in future_data.columns if col not in not_use_col]\n",
    "    X_future = future_data[available_cols].copy()\n",
    "    \n",
    "    for c in X_future.columns:\n",
    "        X_future[c] = pd.to_numeric(X_future[c], errors=\"coerce\")\n",
    "    \n",
    "    missing_features = set(expected_features) - set(X_future.columns)\n",
    "    if missing_features:\n",
    "        for feature in missing_features:\n",
    "            X_future[feature] = 0\n",
    "    \n",
    "    X_future = X_future[expected_features].copy()\n",
    "    return X_future\n",
    "\n",
    "# ================================================================================================\n",
    "# 6. 1단계: 모델 학습 및 저장 파이프라인\n",
    "# ================================================================================================\n",
    "def train_and_save_best_models(cutoff_date='2025-05-20', save_dir='./trained_models', results_dir='./results'):\n",
    "    \"\"\"각 센터별 최고 성능 모델 학습 및 저장\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"1단계: 모델 학습 및 저장\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 저장 디렉토리 생성\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # 데이터 로드\n",
    "    centers_data = load_original_data()\n",
    "    cutoff = pd.to_datetime(cutoff_date)\n",
    "    \n",
    "    saved_models = {}\n",
    "    training_summary = []\n",
    "    \n",
    "    for center_name, df_raw in centers_data.items():\n",
    "        print(f\"\\n[{center_name.upper()} 센터 모델 학습]\")\n",
    "        \n",
    "        # 학습 데이터 준비\n",
    "        df_raw['날짜'] = pd.to_datetime(df_raw['날짜'])\n",
    "        raw_train_data = df_raw[df_raw['날짜'] <= cutoff].copy()\n",
    "        \n",
    "        if len(raw_train_data) < 50:\n",
    "            print(f\"  학습 데이터 부족\")\n",
    "            continue\n",
    "        \n",
    "        # 피처 엔지니어링\n",
    "        train_data = make_features(raw_train_data, cutoff_date=cutoff_date)\n",
    "        print(f\"  학습 데이터: {len(train_data)}행\")\n",
    "        \n",
    "        # 모델 평가\n",
    "        training_results = comprehensive_evaluation_comparison(center_name, train_data)\n",
    "        \n",
    "        # 최고 성능 모델 선택 및 학습\n",
    "        best_models = select_and_save_best_models(center_name, train_data, training_results, save_dir, results_dir)\n",
    "        \n",
    "        if best_models:\n",
    "            saved_models[center_name] = best_models\n",
    "            \n",
    "            # 학습 요약 정보 저장\n",
    "            for task_type, model_info in best_models.items():\n",
    "                training_summary.append({\n",
    "                    'center': center_name,\n",
    "                    'task_type': task_type,\n",
    "                    'model_name': model_info['model_name'],\n",
    "                    'performance': model_info['performance'],\n",
    "                    'saved_path': model_info['saved_path'],\n",
    "                    'training_date': datetime.now().isoformat()\n",
    "                })\n",
    "    \n",
    "    # 학습 요약 저장\n",
    "    summary_path = os.path.join(save_dir, 'training_summary.csv')\n",
    "    summary_df = pd.DataFrame(training_summary)\n",
    "    summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n={'='*60}\")\n",
    "    print(f\"모델 학습 완료!\")\n",
    "    print(f\"저장 위치: {save_dir}\")\n",
    "    print(f\"학습된 모델: {len(training_summary)}개\")\n",
    "    print(f\"학습 요약: {summary_path}\")\n",
    "    print(f\"={'='*60}\")\n",
    "    \n",
    "    return saved_models, training_summary\n",
    "\n",
    "def select_and_save_best_models(center_name, train_data, training_results, save_dir, results_dir):\n",
    "    \"\"\"센터별 최고 성능 모델 선택 및 저장\"\"\"\n",
    "    results_df = pd.DataFrame(training_results)\n",
    "    successful_results = results_df[results_df['success'] == True]\n",
    "    \n",
    "    if len(successful_results) == 0:\n",
    "        return None\n",
    "    \n",
    "    saved_models = {}\n",
    "    \n",
    "    # 회귀 최고 성능 모델\n",
    "    reg_results = successful_results[successful_results['type'] == 'regression']\n",
    "    if len(reg_results) > 0:\n",
    "        best_reg = reg_results.loc[reg_results['r2'].idxmax()]\n",
    "        print(f\"  최고 회귀 모델: {best_reg['model']} (R²={best_reg['r2']:.3f})\")\n",
    "        \n",
    "        # 모델 학습 및 저장\n",
    "        model_data = train_and_save_single_model(\n",
    "            center_name, train_data, best_reg, 'regression', save_dir, results_dir\n",
    "        )\n",
    "        \n",
    "        if model_data:\n",
    "            saved_models['regression'] = model_data\n",
    "    \n",
    "    # 분류 최고 성능 모델\n",
    "    clf_results = successful_results[successful_results['type'] == 'classification']\n",
    "    if len(clf_results) > 0:\n",
    "        best_clf = clf_results.loc[clf_results['macro_f1'].idxmax()]\n",
    "        print(f\"  최고 분류 모델: {best_clf['model']} (F1={best_clf['macro_f1']:.3f})\")\n",
    "        \n",
    "        # 모델 학습 및 저장\n",
    "        model_data = train_and_save_single_model(\n",
    "            center_name, train_data, best_clf, 'classification', save_dir, results_dir\n",
    "        )\n",
    "        \n",
    "        if model_data:\n",
    "            saved_models['classification'] = model_data\n",
    "    \n",
    "    return saved_models\n",
    "\n",
    "def train_and_save_single_model(center_name, train_data, best_result, task_type, save_dir, results_dir):\n",
    "    \"\"\"개별 모델 학습 및 저장\"\"\"\n",
    "    try:\n",
    "        model_name = best_result['model']\n",
    "        split_method = best_result['split_method']\n",
    "        target_col = \"합계_1일후\" if task_type == \"regression\" else \"등급_1일후\"\n",
    "        \n",
    "        # 전체 데이터로 학습\n",
    "        X_train, X_test, y_train, y_test, feature_names, _, _ = prepare_data_stratified(\n",
    "            train_data, target_col=target_col, model_type=task_type, \n",
    "            test_size=0.05, split_method=split_method\n",
    "        )\n",
    "        \n",
    "        X_all = pd.concat([X_train, X_test], ignore_index=True)\n",
    "        y_all = pd.concat([y_train, y_test], ignore_index=True)\n",
    "        \n",
    "        # 모델 구축 및 학습\n",
    "        if task_type == \"regression\":\n",
    "            models = build_regression_models()\n",
    "        else:\n",
    "            models = build_classification_models()\n",
    "        \n",
    "        model = models[model_name]\n",
    "        pipeline = make_pipeline_unified(model, model_name, task_type)\n",
    "        pipeline.fit(X_all, y_all)\n",
    "        \n",
    "        # 모델 저장\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{center_name}_{model_name}_{task_type}_{timestamp}.pkl\"\n",
    "        model_path = os.path.join(save_dir, filename)\n",
    "        \n",
    "        model_data = {\n",
    "            'pipeline': pipeline,\n",
    "            'feature_names': feature_names,\n",
    "            'model_name': model_name,\n",
    "            'center_name': center_name,\n",
    "            'task_type': task_type,\n",
    "            'performance': dict(best_result),\n",
    "            'split_method': split_method,\n",
    "            'training_date': datetime.now().isoformat(),\n",
    "            'target_column': target_col\n",
    "        }\n",
    "        \n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"    저장됨: {filename}\")\n",
    "        \n",
    "        # Feature Importance 분석 및 저장 (선택사항)\n",
    "        try:\n",
    "            importance_df = extract_feature_importance(pipeline, model_name, feature_names)\n",
    "            if importance_df is not None:\n",
    "                print(f\"    Top 5 피처: {', '.join(importance_df.head(5)['feature'].tolist())}\")\n",
    "                \n",
    "                # Feature Importance 시각화 저장\n",
    "                fig = plot_feature_importance(importance_df, f\"{center_name}_{model_name}\")\n",
    "                if fig:\n",
    "                    img_path = os.path.join(results_dir, f\"feature_importance_{center_name}_{model_name}_{task_type}_{timestamp}.png\")\n",
    "                    fig.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "                    plt.close(fig)\n",
    "                    print(f\"    Feature Importance 저장: {os.path.basename(img_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Feature Importance 분석 실패: {e}\")\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'performance': dict(best_result),\n",
    "            'saved_path': model_path,\n",
    "            'feature_names': feature_names,\n",
    "            'target_column': target_col\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    모델 저장 실패: {e}\")\n",
    "        return None\n",
    "    \n",
    "# ================================================================================================\n",
    "# 모델 성능 비교 시각화 추가 코드 (버전 13에 추가)\n",
    "# ================================================================================================\n",
    "\n",
    "def create_performance_comparison_plots(training_summary, results_dir='./results'):\n",
    "    \"\"\"모델 성능 비교 시각화 생성\"\"\"\n",
    "    \n",
    "    print(\"\\n모델 성능 비교 그래프 생성 중...\")\n",
    "    \n",
    "    if not training_summary or len(training_summary) == 0:\n",
    "        print(\"비교할 성능 데이터가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    # DataFrame 변환\n",
    "    if isinstance(training_summary, list):\n",
    "        summary_df = pd.DataFrame(training_summary)\n",
    "    else:\n",
    "        summary_df = training_summary.copy()\n",
    "    \n",
    "    # 결과 저장 디렉토리 확인\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. 센터별 회귀 성능 비교\n",
    "    plot_regression_performance_comparison(summary_df, results_dir, timestamp)\n",
    "    \n",
    "    # 2. 센터별 분류 성능 비교  \n",
    "    plot_classification_performance_comparison(summary_df, results_dir, timestamp)\n",
    "    \n",
    "    # 3. 모델별 성능 비교 (전체 센터 통합)\n",
    "    plot_model_performance_comparison(summary_df, results_dir, timestamp)\n",
    "    \n",
    "    # 4. 센터별 최고 성능 요약\n",
    "    plot_best_performance_summary(summary_df, results_dir, timestamp)\n",
    "    \n",
    "    print(f\"성능 비교 그래프 저장 완료: {results_dir}\")\n",
    "\n",
    "def plot_regression_performance_comparison(summary_df, results_dir, timestamp):\n",
    "    \"\"\"회귀 모델 성능 비교 (R², MAE)\"\"\"\n",
    "    \n",
    "    reg_data = summary_df[summary_df['task_type'] == 'regression'].copy()\n",
    "    if len(reg_data) == 0:\n",
    "        return\n",
    "    \n",
    "    # 성능 지표 추출\n",
    "    reg_data['r2'] = reg_data['performance'].apply(lambda x: x.get('r2', 0))\n",
    "    reg_data['mae'] = reg_data['performance'].apply(lambda x: x.get('mae', 0))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('회귀 모델 성능 비교', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # R² 비교 (센터별)\n",
    "    pivot_r2 = reg_data.pivot_table(index='center', columns='model_name', values='r2', fill_value=0)\n",
    "    pivot_r2.plot(kind='bar', ax=axes[0,0], color=plt.cm.Set3.colors)\n",
    "    axes[0,0].set_title('센터별 R² 성능')\n",
    "    axes[0,0].set_ylabel('R² Score')\n",
    "    axes[0,0].set_ylim(0, 1)\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    axes[0,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # MAE 비교 (센터별)  \n",
    "    pivot_mae = reg_data.pivot_table(index='center', columns='model_name', values='mae', fill_value=0)\n",
    "    pivot_mae.plot(kind='bar', ax=axes[0,1], color=plt.cm.Set2.colors)\n",
    "    axes[0,1].set_title('센터별 MAE 성능 (낮을수록 좋음)')\n",
    "    axes[0,1].set_ylabel('MAE')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    axes[0,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 모델별 평균 R²\n",
    "    model_r2_avg = reg_data.groupby('model_name')['r2'].mean().sort_values(ascending=False)\n",
    "    model_r2_avg.plot(kind='bar', ax=axes[1,0], color='skyblue')\n",
    "    axes[1,0].set_title('모델별 평균 R² 성능')\n",
    "    axes[1,0].set_ylabel('평균 R² Score')\n",
    "    axes[1,0].set_ylim(0, 1)\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 값 표시\n",
    "    for i, v in enumerate(model_r2_avg.values):\n",
    "        axes[1,0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 모델별 평균 MAE\n",
    "    model_mae_avg = reg_data.groupby('model_name')['mae'].mean().sort_values(ascending=True)\n",
    "    model_mae_avg.plot(kind='bar', ax=axes[1,1], color='lightcoral')\n",
    "    axes[1,1].set_title('모델별 평균 MAE 성능')\n",
    "    axes[1,1].set_ylabel('평균 MAE')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 값 표시\n",
    "    for i, v in enumerate(model_mae_avg.values):\n",
    "        axes[1,1].text(i, v + max(model_mae_avg)*0.01, f'{v:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"regression_performance_comparison_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  회귀 성능 비교 저장: {os.path.basename(save_path)}\")\n",
    "\n",
    "def plot_classification_performance_comparison(summary_df, results_dir, timestamp):\n",
    "    \"\"\"분류 모델 성능 비교 (Accuracy, F1)\"\"\"\n",
    "    \n",
    "    clf_data = summary_df[summary_df['task_type'] == 'classification'].copy()\n",
    "    if len(clf_data) == 0:\n",
    "        return\n",
    "    \n",
    "    # 성능 지표 추출\n",
    "    clf_data['accuracy'] = clf_data['performance'].apply(lambda x: x.get('accuracy', 0))\n",
    "    clf_data['macro_f1'] = clf_data['performance'].apply(lambda x: x.get('macro_f1', 0))\n",
    "    clf_data['extreme_f1'] = clf_data['performance'].apply(lambda x: x.get('extreme_f1', 0))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('분류 모델 성능 비교', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Accuracy 비교 (센터별)\n",
    "    pivot_acc = clf_data.pivot_table(index='center', columns='model_name', values='accuracy', fill_value=0)\n",
    "    pivot_acc.plot(kind='bar', ax=axes[0,0], color=plt.cm.Set3.colors)\n",
    "    axes[0,0].set_title('센터별 정확도')\n",
    "    axes[0,0].set_ylabel('Accuracy')\n",
    "    axes[0,0].set_ylim(0, 1)\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    axes[0,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Macro F1 비교 (센터별)\n",
    "    pivot_f1 = clf_data.pivot_table(index='center', columns='model_name', values='macro_f1', fill_value=0)\n",
    "    pivot_f1.plot(kind='bar', ax=axes[0,1], color=plt.cm.Set2.colors)\n",
    "    axes[0,1].set_title('센터별 Macro F1')\n",
    "    axes[0,1].set_ylabel('Macro F1 Score')\n",
    "    axes[0,1].set_ylim(0, 1)\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    axes[0,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 모델별 평균 정확도\n",
    "    model_acc_avg = clf_data.groupby('model_name')['accuracy'].mean().sort_values(ascending=False)\n",
    "    model_acc_avg.plot(kind='bar', ax=axes[1,0], color='lightgreen')\n",
    "    axes[1,0].set_title('모델별 평균 정확도')\n",
    "    axes[1,0].set_ylabel('평균 Accuracy')\n",
    "    axes[1,0].set_ylim(0, 1)\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 값 표시\n",
    "    for i, v in enumerate(model_acc_avg.values):\n",
    "        axes[1,0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 극값 F1 비교\n",
    "    pivot_extreme = clf_data.pivot_table(index='center', columns='model_name', values='extreme_f1', fill_value=0)\n",
    "    pivot_extreme.plot(kind='bar', ax=axes[1,1], color=plt.cm.Pastel1.colors)\n",
    "    axes[1,1].set_title('센터별 극값 F1 성능')\n",
    "    axes[1,1].set_ylabel('Extreme F1 Score')\n",
    "    axes[1,1].set_ylim(0, 1)\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    axes[1,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"classification_performance_comparison_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  분류 성능 비교 저장: {os.path.basename(save_path)}\")\n",
    "\n",
    "def plot_model_performance_comparison(summary_df, results_dir, timestamp):\n",
    "    \"\"\"모델별 성능 비교 (전체 센터 통합)\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('모델별 성능 종합 비교', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 회귀 모델들\n",
    "    reg_data = summary_df[summary_df['task_type'] == 'regression'].copy()\n",
    "    if len(reg_data) > 0:\n",
    "        reg_data['r2'] = reg_data['performance'].apply(lambda x: x.get('r2', 0))\n",
    "        reg_data['mae'] = reg_data['performance'].apply(lambda x: x.get('mae', 0))\n",
    "        reg_data['mape'] = reg_data['performance'].apply(lambda x: x.get('mape', 0))\n",
    "        \n",
    "        # 모델별 R² 분포\n",
    "        reg_models = reg_data['model_name'].unique()\n",
    "        r2_data = [reg_data[reg_data['model_name'] == model]['r2'].values for model in reg_models]\n",
    "        axes[0,0].boxplot(r2_data, labels=reg_models)\n",
    "        axes[0,0].set_title('회귀 모델별 R² 분포')\n",
    "        axes[0,0].set_ylabel('R² Score')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 모델별 MAE 분포  \n",
    "        mae_data = [reg_data[reg_data['model_name'] == model]['mae'].values for model in reg_models]\n",
    "        axes[0,1].boxplot(mae_data, labels=reg_models)\n",
    "        axes[0,1].set_title('회귀 모델별 MAE 분포')\n",
    "        axes[0,1].set_ylabel('MAE')\n",
    "        axes[0,1].tick_params(axis='x', rotation=45)\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # R² vs MAE 산점도\n",
    "        axes[0,2].scatter(reg_data['r2'], reg_data['mae'], c=plt.cm.tab10.colors, s=100, alpha=0.7)\n",
    "        for i, model in enumerate(reg_data['model_name']):\n",
    "            axes[0,2].annotate(f\"{reg_data.iloc[i]['center']}_{model[:3]}\", \n",
    "                             (reg_data.iloc[i]['r2'], reg_data.iloc[i]['mae']),\n",
    "                             xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        axes[0,2].set_title('R² vs MAE')\n",
    "        axes[0,2].set_xlabel('R² Score')\n",
    "        axes[0,2].set_ylabel('MAE')\n",
    "        axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 분류 모델들\n",
    "    clf_data = summary_df[summary_df['task_type'] == 'classification'].copy()\n",
    "    if len(clf_data) > 0:\n",
    "        clf_data['accuracy'] = clf_data['performance'].apply(lambda x: x.get('accuracy', 0))\n",
    "        clf_data['macro_f1'] = clf_data['performance'].apply(lambda x: x.get('macro_f1', 0))\n",
    "        clf_data['extreme_f1'] = clf_data['performance'].apply(lambda x: x.get('extreme_f1', 0))\n",
    "        \n",
    "        # 모델별 정확도 분포\n",
    "        clf_models = clf_data['model_name'].unique()\n",
    "        acc_data = [clf_data[clf_data['model_name'] == model]['accuracy'].values for model in clf_models]\n",
    "        axes[1,0].boxplot(acc_data, labels=clf_models)\n",
    "        axes[1,0].set_title('분류 모델별 정확도 분포')\n",
    "        axes[1,0].set_ylabel('Accuracy')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 모델별 F1 분포\n",
    "        f1_data = [clf_data[clf_data['model_name'] == model]['macro_f1'].values for model in clf_models]\n",
    "        axes[1,1].boxplot(f1_data, labels=clf_models)\n",
    "        axes[1,1].set_title('분류 모델별 Macro F1 분포')\n",
    "        axes[1,1].set_ylabel('Macro F1 Score')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy vs F1 산점도\n",
    "        axes[1,2].scatter(clf_data['accuracy'], clf_data['macro_f1'], c=plt.cm.tab10.colors, s=100, alpha=0.7)\n",
    "        for i, model in enumerate(clf_data['model_name']):\n",
    "            axes[1,2].annotate(f\"{clf_data.iloc[i]['center']}_{model[:3]}\", \n",
    "                             (clf_data.iloc[i]['accuracy'], clf_data.iloc[i]['macro_f1']),\n",
    "                             xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        axes[1,2].set_title('정확도 vs Macro F1')\n",
    "        axes[1,2].set_xlabel('Accuracy')\n",
    "        axes[1,2].set_ylabel('Macro F1 Score')\n",
    "        axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"model_performance_comparison_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  모델 종합 비교 저장: {os.path.basename(save_path)}\")\n",
    "\n",
    "def plot_best_performance_summary(summary_df, results_dir, timestamp):\n",
    "    \"\"\"센터별 최고 성능 요약\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle('센터별 최고 성능 모델 요약', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    centers = summary_df['center'].unique()\n",
    "    \n",
    "    # 회귀 최고 성능\n",
    "    reg_best = []\n",
    "    reg_models = []\n",
    "    for center in centers:\n",
    "        center_reg = summary_df[(summary_df['center'] == center) & (summary_df['task_type'] == 'regression')]\n",
    "        if len(center_reg) > 0:\n",
    "            center_reg = center_reg.copy()\n",
    "            center_reg['r2'] = center_reg['performance'].apply(lambda x: x.get('r2', 0))\n",
    "            best_idx = center_reg['r2'].idxmax()\n",
    "            reg_best.append(center_reg.loc[best_idx, 'r2'])\n",
    "            reg_models.append(center_reg.loc[best_idx, 'model_name'][:8])  # 모델명 축약\n",
    "        else:\n",
    "            reg_best.append(0)\n",
    "            reg_models.append('None')\n",
    "    \n",
    "    # 분류 최고 성능  \n",
    "    clf_best = []\n",
    "    clf_models = []\n",
    "    for center in centers:\n",
    "        center_clf = summary_df[(summary_df['center'] == center) & (summary_df['task_type'] == 'classification')]\n",
    "        if len(center_clf) > 0:\n",
    "            center_clf = center_clf.copy()\n",
    "            center_clf['macro_f1'] = center_clf['performance'].apply(lambda x: x.get('macro_f1', 0))\n",
    "            best_idx = center_clf['macro_f1'].idxmax()\n",
    "            clf_best.append(center_clf.loc[best_idx, 'macro_f1'])\n",
    "            clf_models.append(center_clf.loc[best_idx, 'model_name'][:8])  # 모델명 축약\n",
    "        else:\n",
    "            clf_best.append(0)\n",
    "            clf_models.append('None')\n",
    "    \n",
    "    # 회귀 최고 성능 플롯\n",
    "    bars1 = axes[0].bar(centers, reg_best, color='skyblue', alpha=0.7)\n",
    "    axes[0].set_title('센터별 최고 회귀 성능 (R²)')\n",
    "    axes[0].set_ylabel('R² Score')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 값과 모델명 표시\n",
    "    for i, (bar, score, model) in enumerate(zip(bars1, reg_best, reg_models)):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, score + 0.01, \n",
    "                    f'{score:.3f}\\n{model}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 분류 최고 성능 플롯\n",
    "    bars2 = axes[1].bar(centers, clf_best, color='lightcoral', alpha=0.7)\n",
    "    axes[1].set_title('센터별 최고 분류 성능 (Macro F1)')\n",
    "    axes[1].set_ylabel('Macro F1 Score')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 값과 모델명 표시\n",
    "    for i, (bar, score, model) in enumerate(zip(bars2, clf_best, clf_models)):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, score + 0.01, \n",
    "                    f'{score:.3f}\\n{model}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"best_performance_summary_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  최고 성능 요약 저장: {os.path.basename(save_path)}\")\n",
    "\n",
    "# ================================================================================================\n",
    "# train_and_save_best_models 함수 수정 (성능 비교 그래프 생성 추가)\n",
    "# ================================================================================================\n",
    "\n",
    "def train_and_save_best_models_with_plots(cutoff_date='2025-05-20', save_dir='./trained_models', \n",
    "                                        results_dir='./results', create_plots=True):\n",
    "    \"\"\"모델 학습 + 성능 비교 그래프 생성\"\"\"\n",
    "    \n",
    "    # 기존 학습 과정 수행\n",
    "    saved_models, training_summary = train_and_save_best_models(cutoff_date, save_dir, results_dir)\n",
    "    \n",
    "    # 성능 비교 그래프 생성\n",
    "    if create_plots and training_summary:\n",
    "        create_performance_comparison_plots(training_summary, results_dir)\n",
    "    \n",
    "    return saved_models, training_summary\n",
    "\n",
    "# ================================================================================================\n",
    "# 7. 2단계: 새로운 데이터로 예측 파이프라인\n",
    "# ================================================================================================\n",
    "def predict_with_saved_models(new_data_path, models_dir='./trained_models', \n",
    "                             results_dir='./results', historical_data_for_features=None):\n",
    "    \"\"\"저장된 모델들로 새로운 데이터 예측\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"2단계: 새로운 데이터 예측\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 새로운 데이터 로드\n",
    "    if isinstance(new_data_path, str):\n",
    "        new_data = pd.read_csv(new_data_path, encoding='utf-8-sig')\n",
    "        print(f\"새 데이터 로드: {new_data_path} ({len(new_data)}행)\")\n",
    "    else:\n",
    "        new_data = new_data_path.copy()\n",
    "        print(f\"새 데이터 제공됨: {len(new_data)}행\")\n",
    "    \n",
    "    # 저장된 모델 목록 확인\n",
    "    saved_models = load_all_saved_models(models_dir)\n",
    "    \n",
    "    if not saved_models:\n",
    "        print(\"저장된 모델이 없습니다. 먼저 train_and_save_best_models()를 실행하세요.\")\n",
    "        return None\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    # 센터별 예측\n",
    "    for center_name in saved_models.keys():\n",
    "        print(f\"\\n[{center_name.upper()} 센터 예측]\")\n",
    "        \n",
    "        # 해당 센터 데이터 필터링 (현재는 전체 데이터 사용)\n",
    "        center_data = new_data.copy()\n",
    "        \n",
    "        if len(center_data) == 0:\n",
    "            print(f\"  해당 센터 데이터 없음\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  예측 데이터: {len(center_data)}행\")\n",
    "        \n",
    "        # 피처 생성\n",
    "        if historical_data_for_features is not None:\n",
    "            processed_data = make_features_for_prediction(\n",
    "                historical_data_for_features[center_name], center_data\n",
    "            )\n",
    "        else:\n",
    "            processed_data = make_simple_features(center_data)\n",
    "        \n",
    "        # 센터별 모델로 예측\n",
    "        center_predictions = predict_for_center(center_name, processed_data, saved_models[center_name])\n",
    "        all_predictions.extend(center_predictions)\n",
    "    \n",
    "    # 결과 정리\n",
    "    if all_predictions:\n",
    "        results_df = pd.DataFrame(all_predictions)\n",
    "        results_df = results_df.sort_values(['center', 'date', 'task_type'])\n",
    "        \n",
    "        # 결과 저장 디렉토리 생성\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        # 결과 저장\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = os.path.join(results_dir, f\"predictions_{timestamp}.csv\")\n",
    "        results_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"\\n={'='*60}\")\n",
    "        print(f\"예측 완료!\")\n",
    "        print(f\"총 예측 건수: {len(results_df)}\")\n",
    "        print(f\"결과 저장: {output_path}\")\n",
    "        print(f\"={'='*60}\")\n",
    "        \n",
    "        # 예측 결과 요약 출력\n",
    "        print_prediction_summary(results_df)\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    else:\n",
    "        print(\"예측 결과가 없습니다.\")\n",
    "        return None\n",
    "\n",
    "def load_all_saved_models(models_dir):\n",
    "    \"\"\"저장된 모든 모델 로드\"\"\"\n",
    "    saved_models = {}\n",
    "    \n",
    "    if not os.path.exists(models_dir):\n",
    "        return saved_models\n",
    "    \n",
    "    model_files = [f for f in os.listdir(models_dir) if f.endswith('.pkl') and f != 'training_summary.csv']\n",
    "    \n",
    "    for model_file in model_files:\n",
    "        try:\n",
    "            model_path = os.path.join(models_dir, model_file)\n",
    "            \n",
    "            with open(model_path, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "            \n",
    "            center_name = model_data['center_name']\n",
    "            task_type = model_data['task_type']\n",
    "            \n",
    "            if center_name not in saved_models:\n",
    "                saved_models[center_name] = {}\n",
    "            \n",
    "            saved_models[center_name][task_type] = model_data\n",
    "            \n",
    "            print(f\"모델 로드: {center_name} {task_type} ({model_data['model_name']})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"모델 로드 실패 ({model_file}): {e}\")\n",
    "    \n",
    "    return saved_models\n",
    "\n",
    "def predict_for_center(center_name, processed_data, center_models):\n",
    "    \"\"\"센터별 모델로 예측\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for task_type, model_data in center_models.items():\n",
    "        try:\n",
    "            pipeline = model_data['pipeline']\n",
    "            feature_names = model_data['feature_names']\n",
    "            model_name = model_data['model_name']\n",
    "            target_col = model_data['target_column']\n",
    "            \n",
    "            # 예측 데이터 준비\n",
    "            X_future = prepare_prediction_features(processed_data, feature_names)\n",
    "            \n",
    "            if X_future is None or len(X_future) == 0:\n",
    "                continue\n",
    "            \n",
    "            # 예측 수행\n",
    "            y_pred = pipeline.predict(X_future)\n",
    "            \n",
    "            print(f\"  {task_type} 예측: {len(y_pred)}개\")\n",
    "            \n",
    "            # 결과 저장\n",
    "            for i in range(len(X_future)):\n",
    "                pred_result = {\n",
    "                    'date': processed_data.iloc[i]['날짜'],\n",
    "                    'center': center_name,\n",
    "                    'task_type': task_type,\n",
    "                    'model_name': model_name,\n",
    "                    'target_column': target_col,\n",
    "                    'predicted_value': int(y_pred[i]) if task_type == 'classification' else float(y_pred[i])\n",
    "                }\n",
    "                predictions.append(pred_result)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  {task_type} 예측 실패: {e}\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def print_prediction_summary(results_df):\n",
    "    \"\"\"예측 결과 요약 출력\"\"\"\n",
    "    print(f\"\\n--- 예측 결과 요약 ---\")\n",
    "    \n",
    "    for center in results_df['center'].unique():\n",
    "        center_data = results_df[results_df['center'] == center]\n",
    "        \n",
    "        print(f\"\\n{center.upper()} 센터:\")\n",
    "        \n",
    "        reg_data = center_data[center_data['task_type'] == 'regression']\n",
    "        clf_data = center_data[center_data['task_type'] == 'classification']\n",
    "        \n",
    "        if len(reg_data) > 0:\n",
    "            print(f\"  회귀 예측: {len(reg_data)}개\")\n",
    "            print(f\"  평균 예측값: {reg_data['predicted_value'].mean():.1f}\")\n",
    "            print(f\"  예측 범위: {reg_data['predicted_value'].min():.1f} ~ {reg_data['predicted_value'].max():.1f}\")\n",
    "        \n",
    "        if len(clf_data) > 0:\n",
    "            print(f\"  분류 예측: {len(clf_data)}개\")\n",
    "            grade_dist = clf_data['predicted_value'].value_counts().sort_index()\n",
    "            print(f\"  예측 등급 분포: {dict(grade_dist)}\")\n",
    "            \n",
    "# ================================================================================================\n",
    "# SHAP 분석 추가 코드 (버전13 + 성능비교 코드 뒤에 추가)\n",
    "# ================================================================================================\n",
    "\n",
    "def train_and_save_single_model_with_shap(center_name, train_data, best_result, task_type, save_dir, results_dir):\n",
    "    \"\"\"개별 모델 학습 및 저장 + SHAP 분석 포함\"\"\"\n",
    "    try:\n",
    "        model_name = best_result['model']\n",
    "        split_method = best_result['split_method']\n",
    "        target_col = \"합계_1일후\" if task_type == \"regression\" else \"등급_1일후\"\n",
    "        \n",
    "        # 전체 데이터로 학습\n",
    "        X_train, X_test, y_train, y_test, feature_names, _, _ = prepare_data_stratified(\n",
    "            train_data, target_col=target_col, model_type=task_type, \n",
    "            test_size=0.2, split_method=split_method  # SHAP용으로 테스트 데이터 확보\n",
    "        )\n",
    "        \n",
    "        # 전체 학습용 데이터 준비\n",
    "        X_all = pd.concat([X_train, X_test], ignore_index=True)\n",
    "        y_all = pd.concat([y_train, y_test], ignore_index=True)\n",
    "        \n",
    "        # 모델 구축 및 학습\n",
    "        if task_type == \"regression\":\n",
    "            models = build_regression_models()\n",
    "        else:\n",
    "            models = build_classification_models()\n",
    "        \n",
    "        model = models[model_name]\n",
    "        pipeline = make_pipeline_unified(model, model_name, task_type)\n",
    "        pipeline.fit(X_all, y_all)\n",
    "        \n",
    "        # 모델 저장\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{center_name}_{model_name}_{task_type}_{timestamp}.pkl\"\n",
    "        model_path = os.path.join(save_dir, filename)\n",
    "        \n",
    "        model_data = {\n",
    "            'pipeline': pipeline,\n",
    "            'feature_names': feature_names,\n",
    "            'model_name': model_name,\n",
    "            'center_name': center_name,\n",
    "            'task_type': task_type,\n",
    "            'performance': dict(best_result),\n",
    "            'split_method': split_method,\n",
    "            'training_date': datetime.now().isoformat(),\n",
    "            'target_column': target_col\n",
    "        }\n",
    "        \n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"    저장됨: {filename}\")\n",
    "        \n",
    "        # SHAP 분석을 위한 별도 파이프라인 학습 (테스트 데이터용)\n",
    "        analysis_pipeline = make_pipeline_unified(model, model_name, task_type)\n",
    "        analysis_pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Feature Importance 분석 및 저장\n",
    "        try:\n",
    "            importance_df = extract_feature_importance(analysis_pipeline, model_name, feature_names)\n",
    "            if importance_df is not None:\n",
    "                print(f\"    Top 5 피처: {', '.join(importance_df.head(5)['feature'].tolist())}\")\n",
    "                \n",
    "                # Feature Importance 시각화 저장\n",
    "                fig = plot_feature_importance(importance_df, f\"{center_name}_{model_name}\")\n",
    "                if fig:\n",
    "                    img_path = os.path.join(results_dir, f\"feature_importance_{center_name}_{model_name}_{task_type}_{timestamp}.png\")\n",
    "                    fig.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "                    plt.close(fig)\n",
    "                    print(f\"    Feature Importance 저장: {os.path.basename(img_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Feature Importance 분석 실패: {e}\")\n",
    "        \n",
    "        # SHAP 분석 및 저장\n",
    "        if HAS_SHAP:\n",
    "            try:\n",
    "                print(f\"    SHAP 분석 시작...\")\n",
    "                shap_result = analyze_model_with_shap(analysis_pipeline, X_test, feature_names, model_name, max_samples=50)\n",
    "                \n",
    "                if shap_result:\n",
    "                    shap_values, X_processed, explainer = shap_result\n",
    "                    \n",
    "                    # SHAP 시각화\n",
    "                    shap_figs = plot_shap_summary(shap_values, X_processed, feature_names, f\"{center_name}_{model_name}\")\n",
    "                    \n",
    "                    for i, fig in enumerate(shap_figs):\n",
    "                        suffix = \"bar\" if i == 0 else \"beeswarm\"\n",
    "                        shap_img_path = os.path.join(results_dir, f\"shap_{suffix}_{center_name}_{model_name}_{task_type}_{timestamp}.png\")\n",
    "                        fig.savefig(shap_img_path, dpi=300, bbox_inches='tight')\n",
    "                        plt.close(fig)\n",
    "                        print(f\"    SHAP {suffix} 저장: {os.path.basename(shap_img_path)}\")\n",
    "                    \n",
    "                    # SHAP 값 요약 저장 (CSV)\n",
    "                    if isinstance(shap_values, np.ndarray):\n",
    "                        mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
    "                        shap_summary_df = pd.DataFrame({\n",
    "                            'feature': feature_names,\n",
    "                            'mean_abs_shap': mean_abs_shap\n",
    "                        }).sort_values('mean_abs_shap', ascending=False)\n",
    "                        \n",
    "                        shap_csv_path = os.path.join(results_dir, f\"shap_summary_{center_name}_{model_name}_{task_type}_{timestamp}.csv\")\n",
    "                        shap_summary_df.to_csv(shap_csv_path, index=False, encoding='utf-8-sig')\n",
    "                        print(f\"    SHAP 요약 저장: {os.path.basename(shap_csv_path)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    SHAP 분석 실패: {e}\")\n",
    "        else:\n",
    "            print(f\"    SHAP 라이브러리 없음 - SHAP 분석 건너뜀\")\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'performance': dict(best_result),\n",
    "            'saved_path': model_path,\n",
    "            'feature_names': feature_names,\n",
    "            'target_column': target_col\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    모델 저장 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def select_and_save_best_models_with_shap(center_name, train_data, training_results, save_dir, results_dir):\n",
    "    \"\"\"센터별 최고 성능 모델 선택 및 저장 + SHAP 분석\"\"\"\n",
    "    results_df = pd.DataFrame(training_results)\n",
    "    successful_results = results_df[results_df['success'] == True]\n",
    "    \n",
    "    if len(successful_results) == 0:\n",
    "        return None\n",
    "    \n",
    "    saved_models = {}\n",
    "    \n",
    "    # 회귀 최고 성능 모델\n",
    "    reg_results = successful_results[successful_results['type'] == 'regression']\n",
    "    if len(reg_results) > 0:\n",
    "        best_reg = reg_results.loc[reg_results['r2'].idxmax()]\n",
    "        print(f\"  최고 회귀 모델: {best_reg['model']} (R²={best_reg['r2']:.3f})\")\n",
    "        \n",
    "        # 모델 학습 및 저장 (SHAP 포함)\n",
    "        model_data = train_and_save_single_model_with_shap(\n",
    "            center_name, train_data, best_reg, 'regression', save_dir, results_dir\n",
    "        )\n",
    "        \n",
    "        if model_data:\n",
    "            saved_models['regression'] = model_data\n",
    "    \n",
    "    # 분류 최고 성능 모델\n",
    "    clf_results = successful_results[successful_results['type'] == 'classification']\n",
    "    if len(clf_results) > 0:\n",
    "        best_clf = clf_results.loc[clf_results['macro_f1'].idxmax()]\n",
    "        print(f\"  최고 분류 모델: {best_clf['model']} (F1={best_clf['macro_f1']:.3f})\")\n",
    "        \n",
    "        # 모델 학습 및 저장 (SHAP 포함)\n",
    "        model_data = train_and_save_single_model_with_shap(\n",
    "            center_name, train_data, best_clf, 'classification', save_dir, results_dir\n",
    "        )\n",
    "        \n",
    "        if model_data:\n",
    "            saved_models['classification'] = model_data\n",
    "    \n",
    "    return saved_models\n",
    "\n",
    "def train_and_save_best_models_with_plots_and_shap(cutoff_date='2025-05-20', save_dir='./trained_models', \n",
    "                                                  results_dir='./results', create_plots=True):\n",
    "    \"\"\"모델 학습 + SHAP 분석 + 성능 비교 그래프 생성\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"1단계: 모델 학습 및 저장 (SHAP 분석 포함)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 저장 디렉토리 생성\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # 데이터 로드\n",
    "    centers_data = load_original_data()\n",
    "    cutoff = pd.to_datetime(cutoff_date)\n",
    "    \n",
    "    saved_models = {}\n",
    "    training_summary = []\n",
    "    \n",
    "    for center_name, df_raw in centers_data.items():\n",
    "        print(f\"\\n[{center_name.upper()} 센터 모델 학습]\")\n",
    "        \n",
    "        # 학습 데이터 준비\n",
    "        df_raw['날짜'] = pd.to_datetime(df_raw['날짜'])\n",
    "        raw_train_data = df_raw[df_raw['날짜'] <= cutoff].copy()\n",
    "        \n",
    "        if len(raw_train_data) < 50:\n",
    "            print(f\"  학습 데이터 부족\")\n",
    "            continue\n",
    "        \n",
    "        # 피처 엔지니어링\n",
    "        train_data = make_features(raw_train_data, cutoff_date=cutoff_date)\n",
    "        print(f\"  학습 데이터: {len(train_data)}행\")\n",
    "        \n",
    "        # 모델 평가\n",
    "        training_results = comprehensive_evaluation_comparison(center_name, train_data)\n",
    "        \n",
    "        # 최고 성능 모델 선택 및 학습 (SHAP 포함)\n",
    "        best_models = select_and_save_best_models_with_shap(center_name, train_data, training_results, save_dir, results_dir)\n",
    "        \n",
    "        if best_models:\n",
    "            saved_models[center_name] = best_models\n",
    "            \n",
    "            # 학습 요약 정보 저장\n",
    "            for task_type, model_info in best_models.items():\n",
    "                training_summary.append({\n",
    "                    'center': center_name,\n",
    "                    'task_type': task_type,\n",
    "                    'model_name': model_info['model_name'],\n",
    "                    'performance': model_info['performance'],\n",
    "                    'saved_path': model_info['saved_path'],\n",
    "                    'training_date': datetime.now().isoformat()\n",
    "                })\n",
    "    \n",
    "    # 학습 요약 저장\n",
    "    summary_path = os.path.join(save_dir, 'training_summary.csv')\n",
    "    summary_df = pd.DataFrame(training_summary)\n",
    "    summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n={'='*60}\")\n",
    "    print(f\"모델 학습 완료!\")\n",
    "    print(f\"저장 위치: {save_dir}\")\n",
    "    print(f\"학습된 모델: {len(training_summary)}개\")\n",
    "    print(f\"학습 요약: {summary_path}\")\n",
    "    print(f\"={'='*60}\")\n",
    "    \n",
    "    # 성능 비교 그래프 생성\n",
    "    if create_plots and training_summary:\n",
    "        create_performance_comparison_plots(training_summary, results_dir)\n",
    "    \n",
    "    return saved_models, training_summary\n",
    "\n",
    "# ================================================================================================\n",
    "# 최종 사용법\n",
    "# ================================================================================================\n",
    "def final_example_usage():\n",
    "    \"\"\"SHAP 포함 최종 사용 예시\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"SHAP 분석 포함 완전한 파이프라인 사용법\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n1. SHAP + 성능비교 그래프 포함 학습:\")\n",
    "    print(\"saved_models, summary = train_and_save_best_models_with_plots_and_shap()\")\n",
    "    \n",
    "    print(\"\\n2. 예측:\")\n",
    "    print(\"results = predict_with_saved_models('새_데이터.csv')\")\n",
    "    \n",
    "    print(\"\\n최종 산출물 (총 29개 파일):\")\n",
    "    print(\"=\"*40)\n",
    "    print(\"./trained_models/ (9개)\")\n",
    "    print(\"├── 모델 파일 8개 (.pkl)\")\n",
    "    print(\"└── training_summary.csv\")\n",
    "    \n",
    "    print(\"\\n./results/ (20개)\")\n",
    "    print(\"├── Feature Importance 이미지 8개\")\n",
    "    print(\"├── SHAP Bar 이미지 8개\") \n",
    "    print(\"├── SHAP Beeswarm 이미지 8개\")\n",
    "    print(\"├── SHAP 요약 CSV 8개\")\n",
    "    print(\"└── 성능 비교 그래프 4개\")\n",
    "    \n",
    "    print(\"\\n예측 시 추가:\")\n",
    "    print(\"└── predictions_*.csv\")\n",
    "    \n",
    "    print(\"\\n주의사항:\")\n",
    "    print(\"- SHAP 라이브러리 필요: pip install shap\")\n",
    "    print(\"- SHAP 분석으로 시간이 더 오래 걸림\")\n",
    "    print(\"- 메모리 사용량 증가\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_example_usage()\n",
    "\n",
    "# ================================================================================================\n",
    "# 8. 사용 가이드 및 실행 예시\n",
    "# ================================================================================================\n",
    "def example_usage():\n",
    "    \"\"\"사용 예시 가이드\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"하수처리장 예측 파이프라인 사용 가이드\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n1단계: 모델 학습 및 저장 (한 번만 실행)\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"saved_models, summary = train_and_save_best_models(cutoff_date='2025-05-20')\")\n",
    "    print()\n",
    "    print(\"생성되는 파일:\")\n",
    "    print(\"- ./trained_models/ 폴더\")\n",
    "    print(\"  ├── nanji_RandomForest_Reg_regression_20250101_123456.pkl\")\n",
    "    print(\"  ├── nanji_LogisticRegression_Clf_classification_20250101_123456.pkl\")\n",
    "    print(\"  ├── ... (최대 8개 모델)\")\n",
    "    print(\"  ├── training_summary.csv\")\n",
    "    print(\"  └── feature_importance_*.png (시각화 이미지들)\")\n",
    "    \n",
    "    print(\"\\n2단계: 새로운 데이터로 예측 (반복 실행 가능)\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"# CSV 파일로 예측\")\n",
    "    print(\"results = predict_with_saved_models('새로운_데이터.csv')\")\n",
    "    print()\n",
    "    print(\"# DataFrame으로 예측\")\n",
    "    print(\"new_df = pd.read_csv('새로운_데이터.csv')\")\n",
    "    print(\"results = predict_with_saved_models(new_df)\")\n",
    "    print()\n",
    "    print(\"# 과거 데이터와 함께 예측 (더 정확한 시차 변수)\")\n",
    "    print(\"historical_data = load_original_data()\")\n",
    "    print(\"results = predict_with_saved_models('새로운_데이터.csv',\")\n",
    "    print(\"                                   historical_data_for_features=historical_data)\")\n",
    "    \n",
    "    print(\"\\n새로운 데이터 형식 예시:\")\n",
    "    print(\"=\"*30)\n",
    "    print(\"날짜,일_평균기온(°C),일_일강수량(mm),평균습도(%),계절,불쾌지수등급\")\n",
    "    print(\"2025-06-01,25.3,0,65,여름,약간 불쾌\")\n",
    "    print(\"2025-06-02,26.1,2.5,70,여름,불쾌\")\n",
    "    print(\"2025-06-03,24.8,15.2,75,여름,매우 불쾌\")\n",
    "    \n",
    "    print(\"\\n예측 결과 형식:\")\n",
    "    print(\"=\"*20)\n",
    "    print(\"date,center,task_type,model_name,predicted_value\")\n",
    "    print(\"2025-06-01,nanji,regression,RandomForest_Reg,1250.5\")\n",
    "    print(\"2025-06-01,nanji,classification,LogisticRegression_Clf,2\")\n",
    "    \n",
    "    print(\"\\n주의사항:\")\n",
    "    print(\"=\"*15)\n",
    "    print(\"- 모델 학습은 한 번만 실행하면 됩니다\")\n",
    "    print(\"- 예측은 새로운 데이터가 있을 때마다 실행 가능\")\n",
    "    print(\"- 데이터 경로가 올바른지 확인하세요\")\n",
    "    print(\"- 새 데이터의 컬럼 구조는 기존 학습 데이터와 유사해야 합니다\")\n",
    "\n",
    "def updated_example_usage():\n",
    "    \"\"\"업데이트된 사용 예시\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"성능 비교 그래프 포함 파이프라인 사용법\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n1. 성능 비교 그래프 포함 학습:\")\n",
    "    print(\"saved_models, summary = train_and_save_best_models_with_plots()\")\n",
    "    \n",
    "    print(\"\\n2. 성능 비교 그래프만 별도 생성:\")\n",
    "    print(\"# 학습 요약 데이터가 있는 경우\")\n",
    "    print(\"create_performance_comparison_plots(training_summary, results_dir='./results')\")\n",
    "    \n",
    "    print(\"\\n생성되는 성능 비교 그래프:\")\n",
    "    print(\"- regression_performance_comparison_*.png\")\n",
    "    print(\"- classification_performance_comparison_*.png\") \n",
    "    print(\"- model_performance_comparison_*.png\")\n",
    "    print(\"- best_performance_summary_*.png\")\n",
    "    \n",
    "    print(\"\\n그래프 내용:\")\n",
    "    print(\"1. 회귀: 센터별/모델별 R², MAE 비교\")\n",
    "    print(\"2. 분류: 센터별/모델별 정확도, F1 비교\")\n",
    "    print(\"3. 모델 종합: 박스플롯, 산점도 분석\")\n",
    "    print(\"4. 최고 성능: 센터별 베스트 모델 요약\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    updated_example_usage()\n",
    "\n",
    "\n",
    "# ================================================================================================\n",
    "# 실행 준비 완료\n",
    "# ================================================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"완전한 하수처리장 예측 파이프라인 준비 완료!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n다음 명령어로 시작하세요:\")\n",
    "print(\"\\n# 1단계: 모델 학습\")\n",
    "print(\"saved_models, summary = train_and_save_best_models()\")\n",
    "print(\"\\n# 2단계: 예측\")\n",
    "print(\"results = predict_with_saved_models('새로운_데이터.csv')\")\n",
    "print(\"\\n사용 가이드: example_usage()\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44bfe3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "1단계: 모델 학습 및 저장\n",
      "================================================================================\n",
      "\n",
      "[NANJI 센터 모델 학습]\n",
      "  학습 데이터: 3058행\n",
      "\n",
      "======================================================================\n",
      "센터: nanji - Stratified vs 시계열 분할 비교\n",
      "======================================================================\n",
      "데이터 크기: 3058행, 44컬럼\n",
      "등급 분포: {0.0: 458, 1.0: 1682, 2.0: 611, 3.0: 307}\n",
      "클래스 불균형 비율: 5.5:1 (최대:1682, 최소:307)\n",
      "\n",
      "==================================================\n",
      "분할 방법: TEMPORAL\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (temporal) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  17%|█▋        | 1/6 [00:00<00:04,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.557, MAE=49542, MAPE=7.3%\n",
      "  LinearRegression  : R²=0.513, MAE=62439, MAPE=9.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  50%|█████     | 3/6 [00:02<00:02,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.395, MAE=58665, MAPE=9.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  67%|██████▋   | 4/6 [00:02<00:01,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.480, MAE=56356, MAPE=8.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  83%|████████▎ | 5/6 [00:04<00:01,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.492, MAE=54005, MAPE=8.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.540, MAE=48369, MAPE=7.1%\n",
      "\n",
      "--- 분류 모델 평가 (temporal) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 453, 1: 1309, 2: 435, 3: 249}\n",
      "테스트 세트 등급 분포: {0: 5, 1: 373, 2: 176, 3: 58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  17%|█▋        | 1/6 [00:00<00:01,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.748, F1=0.481, 극값F1=0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  33%|███▎      | 2/6 [00:06<00:15,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.665, F1=0.468, 극값F1=0.386\n",
      "  LogisticRegression_Clf: ACC=0.314, F1=0.308, 극값F1=0.199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  67%|██████▋   | 4/6 [00:08<00:03,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.722, F1=0.567, 극값F1=0.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  83%|████████▎ | 5/6 [00:15<00:03,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.694, F1=0.527, 극값F1=0.432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.672, F1=0.508, 극값F1=0.404\n",
      "\n",
      "==================================================\n",
      "분할 방법: STRATIFIED\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (random_shuffle) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  17%|█▋        | 1/6 [00:00<00:04,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.612, MAE=47300, MAPE=7.0%\n",
      "  LinearRegression  : R²=0.592, MAE=50530, MAPE=7.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  50%|█████     | 3/6 [00:02<00:02,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.620, MAE=46878, MAPE=6.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  67%|██████▋   | 4/6 [00:03<00:01,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.637, MAE=45859, MAPE=6.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  83%|████████▎ | 5/6 [00:05<00:01,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.605, MAE=48431, MAPE=7.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.648, MAE=44875, MAPE=6.6%\n",
      "\n",
      "--- 분류 모델 평가 (stratified) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 366, 1: 1345, 2: 489, 3: 246}\n",
      "테스트 세트 등급 분포: {0: 92, 1: 337, 2: 122, 3: 61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  17%|█▋        | 1/6 [00:00<00:01,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.725, F1=0.664, 극값F1=0.660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  33%|███▎      | 2/6 [00:06<00:15,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.725, F1=0.658, 극값F1=0.648\n",
      "  LogisticRegression_Clf: ACC=0.613, F1=0.578, 극값F1=0.597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  67%|██████▋   | 4/6 [00:08<00:04,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.745, F1=0.667, 극값F1=0.645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  83%|████████▎ | 5/6 [00:16<00:03,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.735, F1=0.665, 극값F1=0.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.675, F1=0.630, 극값F1=0.632\n",
      "  최고 회귀 모델: CatBoost_Reg (R²=0.648)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    저장됨: nanji_CatBoost_Reg_regression_20250827_165649.pkl\n",
      "    Top 5 피처: 강수량_7일_누적, 하천, 강수량_1일_누적, 세탁업, 체감온도(°C)\n",
      "    Feature Importance 저장: feature_importance_nanji_CatBoost_Reg_regression_20250827_165649.png\n",
      "  최고 분류 모델: XGBoost_Clf (F1=0.667)\n",
      "    저장됨: nanji_XGBoost_Clf_classification_20250827_165651.pkl\n",
      "    Top 5 피처: 체감온도(°C), 일_최저기온(°C), 강수량_7일_누적, 강수량_2일_누적, 목욕장업\n",
      "    Feature Importance 저장: feature_importance_nanji_XGBoost_Clf_classification_20250827_165651.png\n",
      "\n",
      "[JUNGNANG 센터 모델 학습]\n",
      "  학습 데이터: 3058행\n",
      "\n",
      "======================================================================\n",
      "센터: jungnang - Stratified vs 시계열 분할 비교\n",
      "======================================================================\n",
      "데이터 크기: 3058행, 44컬럼\n",
      "등급 분포: {0.0: 460, 1.0: 1680, 2.0: 611, 3.0: 307}\n",
      "클래스 불균형 비율: 5.5:1 (최대:1680, 최소:307)\n",
      "\n",
      "==================================================\n",
      "분할 방법: TEMPORAL\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (temporal) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  17%|█▋        | 1/6 [00:00<00:04,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.304, MAE=91088, MAPE=7.1%\n",
      "  LinearRegression  : R²=0.055, MAE=131262, MAPE=10.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  50%|█████     | 3/6 [00:02<00:02,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.137, MAE=105765, MAPE=8.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  67%|██████▋   | 4/6 [00:03<00:01,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.269, MAE=96690, MAPE=7.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  83%|████████▎ | 5/6 [00:04<00:01,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.254, MAE=96761, MAPE=7.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.259, MAE=99822, MAPE=7.9%\n",
      "\n",
      "--- 분류 모델 평가 (temporal) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 405, 1: 1321, 2: 458, 3: 262}\n",
      "테스트 세트 등급 분포: {0: 55, 1: 359, 2: 153, 3: 45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  17%|█▋        | 1/6 [00:00<00:01,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.627, F1=0.425, 극값F1=0.311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  33%|███▎      | 2/6 [00:06<00:16,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.547, F1=0.380, 극값F1=0.303\n",
      "  LogisticRegression_Clf: ACC=0.359, F1=0.296, 극값F1=0.312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  67%|██████▋   | 4/6 [00:08<00:04,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.541, F1=0.442, 극값F1=0.342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  83%|████████▎ | 5/6 [00:15<00:03,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.531, F1=0.468, 극값F1=0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.554, F1=0.411, 극값F1=0.325\n",
      "\n",
      "==================================================\n",
      "분할 방법: STRATIFIED\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (random_shuffle) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  17%|█▋        | 1/6 [00:00<00:04,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.576, MAE=76240, MAPE=5.5%\n",
      "  LinearRegression  : R²=0.522, MAE=89786, MAPE=6.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  50%|█████     | 3/6 [00:02<00:02,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.572, MAE=78344, MAPE=5.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  67%|██████▋   | 4/6 [00:03<00:01,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.575, MAE=76245, MAPE=5.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  83%|████████▎ | 5/6 [00:04<00:01,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.562, MAE=78893, MAPE=5.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.602, MAE=75195, MAPE=5.5%\n",
      "\n",
      "--- 분류 모델 평가 (stratified) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 368, 1: 1344, 2: 489, 3: 245}\n",
      "테스트 세트 등급 분포: {0: 92, 1: 336, 2: 122, 3: 62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  17%|█▋        | 1/6 [00:00<00:01,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.711, F1=0.629, 극값F1=0.606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  33%|███▎      | 2/6 [00:07<00:16,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.730, F1=0.640, 극값F1=0.603\n",
      "  LogisticRegression_Clf: ACC=0.560, F1=0.548, 극값F1=0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  67%|██████▋   | 4/6 [00:09<00:04,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.745, F1=0.661, 극값F1=0.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  83%|████████▎ | 5/6 [00:16<00:03,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.739, F1=0.647, 극값F1=0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.691, F1=0.653, 극값F1=0.654\n",
      "  최고 회귀 모델: CatBoost_Reg (R²=0.602)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    저장됨: jungnang_CatBoost_Reg_regression_20250827_165738.pkl\n",
      "    Top 5 피처: 강수량_7일_누적, 세탁업, 체력단련장업, 목욕장업, 하천\n",
      "    Feature Importance 저장: feature_importance_jungnang_CatBoost_Reg_regression_20250827_165738.png\n",
      "  최고 분류 모델: XGBoost_Clf (F1=0.661)\n",
      "    저장됨: jungnang_XGBoost_Clf_classification_20250827_165740.pkl\n",
      "    Top 5 피처: 수영장업, 체력단련장업, 강수량_7일_누적, 목욕장업, 일_최저기온(°C)\n",
      "    Feature Importance 저장: feature_importance_jungnang_XGBoost_Clf_classification_20250827_165740.png\n",
      "\n",
      "[SEONAM 센터 모델 학습]\n",
      "  학습 데이터: 3058행\n",
      "\n",
      "======================================================================\n",
      "센터: seonam - Stratified vs 시계열 분할 비교\n",
      "======================================================================\n",
      "데이터 크기: 3058행, 43컬럼\n",
      "등급 분포: {0.0: 459, 1.0: 1680, 2.0: 612, 3.0: 307}\n",
      "클래스 불균형 비율: 5.5:1 (최대:1680, 최소:307)\n",
      "\n",
      "==================================================\n",
      "분할 방법: TEMPORAL\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (temporal) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  17%|█▋        | 1/6 [00:00<00:04,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.037, MAE=157633, MAPE=10.9%\n",
      "  LinearRegression  : R²=0.058, MAE=156666, MAPE=10.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  50%|█████     | 3/6 [00:02<00:02,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.270, MAE=124493, MAPE=8.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  67%|██████▋   | 4/6 [00:03<00:01,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.244, MAE=127234, MAPE=8.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  83%|████████▎ | 5/6 [00:04<00:01,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.280, MAE=120014, MAPE=8.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.327, MAE=124794, MAPE=8.5%\n",
      "\n",
      "--- 분류 모델 평가 (temporal) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 136, 1: 1493, 2: 544, 3: 273}\n",
      "테스트 세트 등급 분포: {0: 323, 1: 187, 2: 68, 3: 34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  17%|█▋        | 1/6 [00:00<00:01,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.304, F1=0.323, 극값F1=0.124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  33%|███▎      | 2/6 [00:07<00:17,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.477, F1=0.432, 극값F1=0.593\n",
      "  LogisticRegression_Clf: ACC=0.271, F1=0.306, 극값F1=0.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  67%|██████▋   | 4/6 [00:09<00:04,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.317, F1=0.307, 극값F1=0.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  83%|████████▎ | 5/6 [00:16<00:03,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.301, F1=0.265, 극값F1=0.057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.252, F1=0.265, 극값F1=0.149\n",
      "\n",
      "==================================================\n",
      "분할 방법: STRATIFIED\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (random_shuffle) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  17%|█▋        | 1/6 [00:00<00:04,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.562, MAE=91557, MAPE=5.4%\n",
      "  LinearRegression  : R²=0.528, MAE=100152, MAPE=6.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  50%|█████     | 3/6 [00:02<00:02,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.604, MAE=86879, MAPE=5.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  67%|██████▋   | 4/6 [00:03<00:01,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.590, MAE=87201, MAPE=5.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  83%|████████▎ | 5/6 [00:05<00:01,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.578, MAE=89065, MAPE=5.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.594, MAE=86488, MAPE=5.1%\n",
      "\n",
      "--- 분류 모델 평가 (stratified) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 367, 1: 1344, 2: 489, 3: 246}\n",
      "테스트 세트 등급 분포: {0: 92, 1: 336, 2: 123, 3: 61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  17%|█▋        | 1/6 [00:00<00:01,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.727, F1=0.652, 극값F1=0.669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  33%|███▎      | 2/6 [00:07<00:17,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.735, F1=0.655, 극값F1=0.662\n",
      "  LogisticRegression_Clf: ACC=0.595, F1=0.562, 극값F1=0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  67%|██████▋   | 4/6 [00:09<00:04,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.745, F1=0.658, 극값F1=0.659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  83%|████████▎ | 5/6 [00:16<00:03,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.745, F1=0.652, 극값F1=0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.699, F1=0.644, 극값F1=0.673\n",
      "  최고 회귀 모델: GradientBoosting_Reg (R²=0.604)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    저장됨: seonam_GradientBoosting_Reg_regression_20250827_165831.pkl\n",
      "    Top 5 피처: 일_일강수량(mm), 강수량_1일_누적, 강수량_7일_누적, 일_최저기온(°C), 세탁업\n",
      "    Feature Importance 저장: feature_importance_seonam_GradientBoosting_Reg_regression_20250827_165831.png\n",
      "  최고 분류 모델: XGBoost_Clf (F1=0.658)\n",
      "    저장됨: seonam_XGBoost_Clf_classification_20250827_165833.pkl\n",
      "    Top 5 피처: 일_일강수량(mm), 강수량_1일_누적, 체력단련장업, 폭우_여부, 목욕장업\n",
      "    Feature Importance 저장: feature_importance_seonam_XGBoost_Clf_classification_20250827_165833.png\n",
      "\n",
      "[TANCHEON 센터 모델 학습]\n",
      "  학습 데이터: 3058행\n",
      "\n",
      "======================================================================\n",
      "센터: tancheon - Stratified vs 시계열 분할 비교\n",
      "======================================================================\n",
      "데이터 크기: 3058행, 42컬럼\n",
      "등급 분포: {0.0: 459, 1.0: 1680, 2.0: 612, 3.0: 307}\n",
      "클래스 불균형 비율: 5.5:1 (최대:1680, 최소:307)\n",
      "\n",
      "==================================================\n",
      "분할 방법: TEMPORAL\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (temporal) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  17%|█▋        | 1/6 [00:00<00:04,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.275, MAE=62889, MAPE=8.3%\n",
      "  LinearRegression  : R²=0.287, MAE=66726, MAPE=8.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  50%|█████     | 3/6 [00:02<00:02,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.027, MAE=75273, MAPE=10.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  67%|██████▋   | 4/6 [00:03<00:01,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.109, MAE=72585, MAPE=9.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  83%|████████▎ | 5/6 [00:05<00:01,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.202, MAE=66029, MAPE=8.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.197, MAE=68829, MAPE=9.2%\n",
      "\n",
      "--- 분류 모델 평가 (temporal) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 345, 1: 1348, 2: 496, 3: 257}\n",
      "테스트 세트 등급 분포: {0: 114, 1: 332, 2: 116, 3: 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  17%|█▋        | 1/6 [00:00<00:01,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.557, F1=0.410, 극값F1=0.246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  33%|███▎      | 2/6 [00:06<00:15,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.364, F1=0.361, 극값F1=0.371\n",
      "  LogisticRegression_Clf: ACC=0.268, F1=0.259, 극값F1=0.444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  67%|██████▋   | 4/6 [00:08<00:04,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.477, F1=0.405, 극값F1=0.322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  83%|████████▎ | 5/6 [00:15<00:03,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.454, F1=0.382, 극값F1=0.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.358, F1=0.354, 극값F1=0.369\n",
      "\n",
      "==================================================\n",
      "분할 방법: STRATIFIED\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (random_shuffle) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  17%|█▋        | 1/6 [00:00<00:03,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.500, MAE=48022, MAPE=6.0%\n",
      "  LinearRegression  : R²=0.377, MAE=57635, MAPE=7.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  50%|█████     | 3/6 [00:02<00:02,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.510, MAE=47588, MAPE=6.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  67%|██████▋   | 4/6 [00:02<00:01,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.514, MAE=46528, MAPE=5.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  83%|████████▎ | 5/6 [00:04<00:01,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.482, MAE=48315, MAPE=6.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.515, MAE=47014, MAPE=5.9%\n",
      "\n",
      "--- 분류 모델 평가 (stratified) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 367, 1: 1344, 2: 489, 3: 246}\n",
      "테스트 세트 등급 분포: {0: 92, 1: 336, 2: 123, 3: 61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  17%|█▋        | 1/6 [00:00<00:01,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.641, F1=0.538, 극값F1=0.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  33%|███▎      | 2/6 [00:07<00:16,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.636, F1=0.524, 극값F1=0.485\n",
      "  LogisticRegression_Clf: ACC=0.449, F1=0.432, 극값F1=0.459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  67%|██████▋   | 4/6 [00:09<00:04,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.658, F1=0.547, 극값F1=0.488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  83%|████████▎ | 5/6 [00:16<00:03,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.647, F1=0.545, 극값F1=0.515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.614, F1=0.559, 극값F1=0.577\n",
      "  최고 회귀 모델: CatBoost_Reg (R²=0.515)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    저장됨: tancheon_CatBoost_Reg_regression_20250827_165921.pkl\n",
      "    Top 5 피처: 세탁업, 체력단련장업, 강수량_7일_누적, 하천, 일_일강수량(mm)\n",
      "    Feature Importance 저장: feature_importance_tancheon_CatBoost_Reg_regression_20250827_165921.png\n",
      "  최고 분류 모델: CatBoost_Clf (F1=0.559)\n",
      "    저장됨: tancheon_CatBoost_Clf_classification_20250827_165922.pkl\n",
      "    Top 5 피처: 세탁업, 강수량_7일_누적, 생활인구, 월, 체력단련장업\n",
      "    Feature Importance 저장: feature_importance_tancheon_CatBoost_Clf_classification_20250827_165922.png\n",
      "\n",
      "=============================================================\n",
      "모델 학습 완료!\n",
      "저장 위치: ./trained_models\n",
      "학습된 모델: 8개\n",
      "학습 요약: ./trained_models/training_summary.csv\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "saved_models, summary = train_and_save_best_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173dfff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = predict_with_saved_models('새_데이터.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youngwon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
