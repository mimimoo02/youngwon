{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c5c688",
   "metadata": {},
   "source": [
    "# 클로드 버전 13에 수정 한 코드 \n",
    "네, 맞습니다. 모든 결과는 **체계적으로 폴더에 정리되어 저장**됩니다:\n",
    "\n",
    "## 폴더 구조:\n",
    "\n",
    "### `./trained_model_v5/` (학습된 모델들)\n",
    "```\n",
    "trained_model_v5/\n",
    "├── nanji_CatBoost_Reg_regression_20250827_185516.pkl\n",
    "├── nanji_XGBoost_Clf_classification_20250827_185519.pkl  \n",
    "├── jungnang_CatBoost_Reg_regression_20250827_185612.pkl\n",
    "├── jungnang_XGBoost_Clf_classification_20250827_185615.pkl\n",
    "├── (각 센터별 최고 성능 모델들...)\n",
    "└── training_summary.csv (학습 요약)\n",
    "```\n",
    "\n",
    "### `./results_v5/` (분석 결과들)\n",
    "```\n",
    "results_v5/\n",
    "├── feature_importance_nanji_CatBoost_Reg_regression_20250827_185516.png\n",
    "├── shap_bar_nanji_CatBoost_Reg_regression_20250827_185516.png\n",
    "├── shap_beeswarm_nanji_CatBoost_Reg_regression_20250827_185516.png\n",
    "├── shap_summary_nanji_CatBoost_Reg_regression_20250827_185516.csv\n",
    "├── (센터별 SHAP & 피처 중요도 파일들...)\n",
    "├── regression_performance_comparison_20250827_185802.png\n",
    "├── classification_performance_comparison_20250827_185802.png  \n",
    "├── model_performance_comparison_20250827_185802.png\n",
    "├── best_performance_summary_20250827_185802.png\n",
    "└── predictions_20250827_193045.csv (예측 결과, 나중에)\n",
    "```\n",
    "\n",
    "## 자동 폴더 생성:\n",
    "```python\n",
    "os.makedirs(save_dir, exist_ok=True)        # ./trained_model_v5/\n",
    "os.makedirs(results_dir, exist_ok=True)     # ./results_v5/  \n",
    "```\n",
    "\n",
    "**절대 파일들이 띡띢띡 흩어져 저장되지 않습니다.** \n",
    "\n",
    "- **모델 파일들**: 한 곳에 모음\n",
    "- **분석 그래프들**: 한 곳에 모음  \n",
    "- **타임스탬프로 버전 관리**: 덮어쓰기 방지\n",
    "\n",
    "폴더가 없으면 자동으로 만들어지고, 모든 파일은 용도별로 깔끔하게 정리됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a89880bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "하수처리장 예측 파이프라인 초기화 완료\n",
      "XGBoost: ✓\n",
      "LightGBM: ✓\n",
      "CatBoost: ✓\n",
      "SHAP: ✓\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필수 라이브러리 import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import time, os, json, warnings\n",
    "import joblib\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import gc\n",
    "\n",
    "# 선택적 라이브러리 import\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGB = True\n",
    "except ImportError:\n",
    "    HAS_LGB = False\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    HAS_CATBOOST = True\n",
    "except ImportError:\n",
    "    HAS_CATBOOST = False\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    HAS_SHAP = True\n",
    "except ImportError:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "# 한글 폰트 설정\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "try:\n",
    "    plt.rcParams['font.family'] = 'AppleGothic' # 맥\n",
    "except Exception:\n",
    "    plt.rcParams['font.family'] ='Malgun Gothic' # 윈도우\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"하수처리장 예측 파이프라인 초기화 완료\")\n",
    "print(f\"XGBoost: {'✓' if HAS_XGB else '✗'}\")\n",
    "print(f\"LightGBM: {'✓' if HAS_LGB else '✗'}\")\n",
    "print(f\"CatBoost: {'✓' if HAS_CATBOOST else '✗'}\")\n",
    "print(f\"SHAP: {'✓' if HAS_SHAP else '✗'}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 각 SHAP 분석 후 메모리 정리\n",
    "import gc\n",
    "plt.close('all')  # 모든 matplotlib 그래프 닫기\n",
    "gc.collect()      # 가비지 컬렉션 강제 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc400db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================================\n",
    "# 완전한 하수처리장 예측 파이프라인 (학습-저장-예측)\n",
    "# ================================================================================================\n",
    "\n",
    "# ================================================================================================\n",
    "# 1. 모델 정의 함수들\n",
    "# ================================================================================================\n",
    "def build_regression_models():\n",
    "    \"\"\"회귀 모델들\"\"\"\n",
    "    models = {}\n",
    "    models[\"RandomForest_Reg\"] = RandomForestRegressor(\n",
    "        n_estimators=300, min_samples_leaf=2, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    models[\"LinearRegression\"] = LinearRegression()\n",
    "    models[\"GradientBoosting_Reg\"] = GradientBoostingRegressor(\n",
    "        n_estimators=200, learning_rate=0.1, random_state=42\n",
    "    )\n",
    "    if HAS_XGB:\n",
    "        models[\"XGBoost_Reg\"] = xgb.XGBRegressor(\n",
    "            n_estimators=400, max_depth=5, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            random_state=42, n_jobs=-1, verbosity=0\n",
    "        )\n",
    "    if HAS_LGB:\n",
    "        models[\"LightGBM_Reg\"] = lgb.LGBMRegressor(\n",
    "            n_estimators=500, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            random_state=42, n_jobs=-1, verbosity=-1\n",
    "        )\n",
    "    if HAS_CATBOOST:\n",
    "        models[\"CatBoost_Reg\"] = cb.CatBoostRegressor(\n",
    "            iterations=500, learning_rate=0.05, depth=6,\n",
    "            random_state=42, verbose=False\n",
    "        )\n",
    "    return models\n",
    "\n",
    "def build_classification_models():\n",
    "    \"\"\"분류 모델들 (4등급)\"\"\"\n",
    "    models = {}\n",
    "    models[\"RandomForest_Clf\"] = RandomForestClassifier(\n",
    "        n_estimators=300, min_samples_leaf=2, random_state=42, \n",
    "        n_jobs=-1, class_weight='balanced'\n",
    "    ) \n",
    "    models[\"GradientBoosting_Clf\"] = GradientBoostingClassifier(\n",
    "        n_estimators=200, learning_rate=0.1, random_state=42\n",
    "    )\n",
    "    models[\"LogisticRegression_Clf\"] = LogisticRegression(\n",
    "        multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=1000,\n",
    "        random_state=42, class_weight='balanced'\n",
    "    )\n",
    "    if HAS_XGB:\n",
    "        models[\"XGBoost_Clf\"] = xgb.XGBClassifier(\n",
    "            n_estimators=400, max_depth=5, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            objective=\"multi:softprob\", num_class=4,\n",
    "            tree_method=\"hist\", random_state=42, n_jobs=-1, verbosity=0\n",
    "        )\n",
    "    if HAS_LGB:\n",
    "        models[\"LightGBM_Clf\"] = lgb.LGBMClassifier(\n",
    "            n_estimators=500, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            objective=\"multiclass\", num_class=4,\n",
    "            random_state=42, n_jobs=-1, verbosity=-1, is_unbalance=True\n",
    "        )\n",
    "    if HAS_CATBOOST:\n",
    "        models[\"CatBoost_Clf\"] = cb.CatBoostClassifier(\n",
    "            iterations=500, learning_rate=0.05, depth=6,\n",
    "            random_state=42, verbose=False, auto_class_weights='Balanced'\n",
    "        )\n",
    "    return models\n",
    "\n",
    "# ================================================================================================\n",
    "# 2. 데이터 처리 함수들\n",
    "# ================================================================================================\n",
    "def make_pipeline_unified(model, model_name, model_type):\n",
    "    \"\"\"통합 전처리 파이프라인\"\"\"\n",
    "    if model_name in [\"LinearRegression\", \"LogisticRegression_Clf\"]:\n",
    "        pre = Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ])\n",
    "    else:\n",
    "        pre = Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        ])\n",
    "    return Pipeline(steps=[(\"pre\", pre), (\"model\", model)])\n",
    "\n",
    "def prepare_data_stratified(df, target_col, model_type, test_size=0.2, split_method='stratified'):\n",
    "    \"\"\"데이터 준비 - Stratified vs 시계열 분할\"\"\"\n",
    "    work = df.sort_values('날짜').reset_index(drop=True).copy()\n",
    "    dates = pd.to_datetime(work['날짜'])\n",
    "\n",
    "    not_use_col = [\n",
    "        '날짜',\n",
    "        '1처리장','2처리장','정화조','중계펌프장','합계','시설현대화',\n",
    "        '3처리장','4처리장','합계', '합계_1일후','합계_2일후',\n",
    "        '등급','등급_1일후','등급_2일후'\n",
    "    ]\n",
    "    \n",
    "    drop_cols = [c for c in (set(not_use_col) | {target_col}) if c in work.columns]\n",
    "    X_raw = work.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    \n",
    "    for c in X_raw.columns:\n",
    "        X_raw[c] = pd.to_numeric(X_raw[c], errors=\"coerce\")\n",
    "\n",
    "    if model_type == \"regression\":\n",
    "        y = pd.to_numeric(work[target_col], errors=\"coerce\")\n",
    "    else:\n",
    "        y = work[target_col].astype(\"int64\")\n",
    "\n",
    "    valid_idx = (~X_raw.isnull().all(axis=1)) & (~pd.isnull(y))\n",
    "    X_raw = X_raw[valid_idx].reset_index(drop=True)\n",
    "    y = y[valid_idx].reset_index(drop=True)\n",
    "    dates = dates[valid_idx].reset_index(drop=True)\n",
    "    \n",
    "    if split_method == 'stratified':\n",
    "        if model_type == \"classification\":\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "            train_idx, test_idx = next(sss.split(X_raw, y))\n",
    "        else:\n",
    "            train_idx, test_idx = train_test_split(\n",
    "                range(len(X_raw)), test_size=test_size, random_state=42\n",
    "            )\n",
    "            \n",
    "        X_train, X_test = X_raw.iloc[train_idx].copy(), X_raw.iloc[test_idx].copy()\n",
    "        y_train, y_test = y.iloc[train_idx].copy(), y.iloc[test_idx].copy()\n",
    "        dates_train, dates_test = dates.iloc[train_idx].copy(), dates.iloc[test_idx].copy()\n",
    "        \n",
    "    else:  # temporal split\n",
    "        n = len(X_raw)\n",
    "        split = int(n * (1 - test_size))\n",
    "        X_train, X_test = X_raw.iloc[:split].copy(), X_raw.iloc[split:].copy()\n",
    "        y_train, y_test = y.iloc[:split].copy(), y.iloc[split:].copy()\n",
    "        dates_train, dates_test = dates.iloc[:split].copy(), dates.iloc[split:].copy()\n",
    "\n",
    "    feature_names = list(X_raw.columns)\n",
    "    return X_train, X_test, y_train, y_test, feature_names, dates_train, dates_test\n",
    "\n",
    "def make_features(df, cutoff_date=None):\n",
    "    \"\"\"파생변수 생성 함수 - Data Leakage 방지 버전\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['날짜'] = pd.to_datetime(df['날짜'])\n",
    "    df = df.sort_values('날짜').reset_index(drop=True)\n",
    "    \n",
    "    # 디버깅 정보 - 시작 시점\n",
    "    print(f\"\\n=== make_features 디버깅 ===\")\n",
    "    print(f\"입력 데이터: {len(df)}행\")\n",
    "    print(f\"날짜 범위: {df['날짜'].min()} ~ {df['날짜'].max()}\")\n",
    "    if cutoff_date is not None:\n",
    "        cutoff = pd.to_datetime(cutoff_date)\n",
    "        print(f\"cutoff_date: {cutoff_date}\")\n",
    "        print(f\"  cutoff 이전 데이터: {len(df[df['날짜'] <= cutoff])}행\")\n",
    "        print(f\"  cutoff 이후 데이터: {len(df[df['날짜'] > cutoff])}행\")\n",
    "\n",
    "    df['월'] = df['날짜'].dt.month\n",
    "    df['요일'] = df['날짜'].dt.weekday\n",
    "\n",
    "    season_map = {'봄': 0, '여름': 1, '가을': 2, '겨울': 3}\n",
    "    discomfort_map = {'쾌적': 0, '약간 불쾌': 1, '불쾌': 2, '매우 불쾌': 3, '극심한 불쾌': 4}\n",
    "    df['계절'] = df['계절'].map(season_map).astype('Int64')\n",
    "    df['불쾌지수등급'] = df['불쾌지수등급'].map(discomfort_map).astype('Int64')\n",
    "\n",
    "    # 강수량 시차 피처\n",
    "    df['강수량_1일전'] = df['일_일강수량(mm)'].shift(1)\n",
    "    df['강수량_2일전'] = df['일_일강수량(mm)'].shift(2)\n",
    "    df['강수량_1일_누적'] = df['일_일강수량(mm)'].rolling(1, min_periods=1).sum()\n",
    "    df['강수량_2일_누적'] = df['일_일강수량(mm)'].rolling(2, min_periods=1).sum()\n",
    "    df['강수량_3일_누적'] = df['일_일강수량(mm)'].rolling(3, min_periods=1).sum()\n",
    "    df['강수량_5일_누적'] = df['일_일강수량(mm)'].rolling(5, min_periods=1).sum()\n",
    "    df['강수량_7일_누적'] = df['일_일강수량(mm)'].rolling(7, min_periods=1).sum()\n",
    "\n",
    "    df['일교차'] = df['일_최고기온(°C)'] - df['일_최저기온(°C)']\n",
    "    df['폭우_여부'] = (df['일_일강수량(mm)'] >= 80).astype(int)\n",
    "    \n",
    "    # 체감온도 계산 (간단 버전)\n",
    "    T = pd.to_numeric(df.get('일_평균기온(°C)', np.nan), errors='coerce')\n",
    "    V_ms = pd.to_numeric(df.get('일_평균풍속(m/s)', np.nan), errors='coerce')\n",
    "    RH = pd.to_numeric(df.get('평균습도(%)', np.nan), errors='coerce')\n",
    "    \n",
    "    e = (RH/100.0) * 6.105 * np.exp(17.27*T/(237.7 + T))\n",
    "    df['체감온도(°C)'] = T + 0.33*e - 0.70*V_ms - 4.00\n",
    "    \n",
    "    # 분류용 등급 계산\n",
    "    q = df['합계'].dropna().quantile([0.15, 0.70, 0.90])\n",
    "    q15, q70, q90 = float(q.loc[0.15]), float(q.loc[0.70]), float(q.loc[0.90])\n",
    "    print(f\"등급 구분 기준: q15={q15:.0f}, q70={q70:.0f}, q90={q90:.0f}\")\n",
    "\n",
    "    def categorize(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        if x < q15:\n",
    "            return 0\n",
    "        elif x < q70:\n",
    "            return 1\n",
    "        elif x < q90:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    df['등급'] = df['합계'].apply(categorize)\n",
    "    \n",
    "    # 타겟 변수 생성 (Data Leakage 방지)\n",
    "    if cutoff_date is not None:\n",
    "        cutoff = pd.to_datetime(cutoff_date)\n",
    "        print(f\"타겟 변수 생성 (cutoff 적용)\")\n",
    "        \n",
    "        df['합계_1일후'] = np.nan\n",
    "        df['합계_2일후'] = np.nan\n",
    "        df['등급_1일후'] = np.nan\n",
    "        df['등급_2일후'] = np.nan\n",
    "        \n",
    "        # 생성된 타겟 개수 추적\n",
    "        target_1day_count = 0\n",
    "        target_2day_count = 0\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            current_date = df.loc[i, '날짜']\n",
    "            \n",
    "            if i + 1 < len(df) and current_date <= cutoff:\n",
    "                next_date = df.loc[i+1, '날짜']\n",
    "                if next_date <= cutoff:\n",
    "                    df.loc[i, '합계_1일후'] = df.loc[i+1, '합계']\n",
    "                    df.loc[i, '등급_1일후'] = df.loc[i+1, '등급']\n",
    "                    target_1day_count += 1\n",
    "            \n",
    "            if i + 2 < len(df) and current_date <= cutoff:\n",
    "                next2_date = df.loc[i+2, '날짜']\n",
    "                if next2_date <= cutoff:\n",
    "                    df.loc[i, '합계_2일후'] = df.loc[i+2, '합계']\n",
    "                    df.loc[i, '등급_2일후'] = df.loc[i+2, '등급']\n",
    "                    target_2day_count += 1\n",
    "                    \n",
    "        print(f\"  1일후 타겟 생성: {target_1day_count}개\")\n",
    "        print(f\"  2일후 타겟 생성: {target_2day_count}개\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"타겟 변수 생성 (shift 사용)\")\n",
    "        df['합계_1일후'] = df['합계'].shift(-1)\n",
    "        df['합계_2일후'] = df['합계'].shift(-2)\n",
    "        df['등급_1일후'] = df['등급'].shift(-1).astype('Int64')\n",
    "        df['등급_2일후'] = df['등급'].shift(-2).astype('Int64')\n",
    "        print(f\"  1일후 타겟 유효값: {df['합계_1일후'].notna().sum()}개\")\n",
    "        print(f\"  2일후 타겟 유효값: {df['합계_2일후'].notna().sum()}개\")\n",
    "\n",
    "    df.attrs['cutoffs'] = {\"q15\": q15, \"q70\": q70, \"q90\": q90}\n",
    "    \n",
    "    # dropna 전후 비교\n",
    "    before_dropna = len(df)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    after_dropna = len(df)\n",
    "    print(f\"dropna 처리: {before_dropna}행 → {after_dropna}행 ({before_dropna - after_dropna}행 제거)\")\n",
    "    \n",
    "    # 2025-06-01 필터링\n",
    "    before_filter = len(df)\n",
    "    df = df[df[\"날짜\"] < \"2025-06-01\"]\n",
    "    after_filter = len(df)\n",
    "    print(f\"날짜 필터링: {before_filter}행 → {after_filter}행 ({before_filter - after_filter}행 제거)\")\n",
    "    \n",
    "    # 최종 결과\n",
    "    if '등급_1일후' in df.columns:\n",
    "        final_grade_dist = df['등급_1일후'].value_counts().sort_index()\n",
    "        print(f\"최종 등급 분포: {dict(final_grade_dist)}\")\n",
    "    \n",
    "    print(f\"최종 출력: {len(df)}행, {len(df.columns)}컬럼\")\n",
    "    print(f\"최종 날짜 범위: {df['날짜'].min()} ~ {df['날짜'].max()}\")\n",
    "    print(f\"=== make_features 완료 ===\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def make_features_for_prediction(historical_df, future_df):\n",
    "    \"\"\"새로운 데이터에 대한 파생변수 생성 (과거 데이터 활용)\"\"\"\n",
    "    combined_df = pd.concat([historical_df, future_df], ignore_index=True)\n",
    "    combined_df['날짜'] = pd.to_datetime(combined_df['날짜'])\n",
    "    combined_df = combined_df.sort_values('날짜').reset_index(drop=True)\n",
    "    \n",
    "    combined_df['월'] = combined_df['날짜'].dt.month\n",
    "    combined_df['요일'] = combined_df['날짜'].dt.weekday\n",
    "    \n",
    "    season_map = {'봄': 0, '여름': 1, '가을': 2, '겨울': 3}\n",
    "    discomfort_map = {'쾌적': 0, '약간 불쾌': 1, '불쾌': 2, '매우 불쾌': 3, '극심한 불쾌': 4}\n",
    "    combined_df['계절'] = combined_df['계절'].map(season_map).astype('Int64')\n",
    "    combined_df['불쾌지수등급'] = combined_df['불쾌지수등급'].map(discomfort_map).astype('Int64')\n",
    "    \n",
    "    # 시차 변수들\n",
    "    combined_df['강수량_1일전'] = combined_df['일_일강수량(mm)'].shift(1)\n",
    "    combined_df['강수량_2일전'] = combined_df['일_일강수량(mm)'].shift(2)\n",
    "    combined_df['강수량_1일_누적'] = combined_df['일_일강수량(mm)'].rolling(1, min_periods=1).sum()\n",
    "    combined_df['강수량_2일_누적'] = combined_df['일_일강수량(mm)'].rolling(2, min_periods=1).sum()\n",
    "    combined_df['강수량_3일_누적'] = combined_df['일_일강수량(mm)'].rolling(3, min_periods=1).sum()\n",
    "    combined_df['강수량_5일_누적'] = combined_df['일_일강수량(mm)'].rolling(5, min_periods=1).sum()\n",
    "    combined_df['강수량_7일_누적'] = combined_df['일_일강수량(mm)'].rolling(7, min_periods=1).sum()\n",
    "    \n",
    "    combined_df['일교차'] = combined_df['일_최고기온(°C)'] - combined_df['일_최저기온(°C)']\n",
    "    combined_df['폭우_여부'] = (combined_df['일_일강수량(mm)'] >= 80).astype(int)\n",
    "    \n",
    "    # 체감온도 계산\n",
    "    T = pd.to_numeric(combined_df.get('일_평균기온(°C)', np.nan), errors='coerce')\n",
    "    V_ms = pd.to_numeric(combined_df.get('일_평균풍속(m/s)', np.nan), errors='coerce')\n",
    "    RH = pd.to_numeric(combined_df.get('평균습도(%)', np.nan), errors='coerce')\n",
    "    \n",
    "    e = (RH/100.0) * 6.105 * np.exp(17.27*T/(237.7 + T))\n",
    "    combined_df['체감온도(°C)'] = T + 0.33*e - 0.70*V_ms - 4.00\n",
    "    \n",
    "    # 새 데이터 부분만 반환\n",
    "    historical_len = len(historical_df)\n",
    "    return combined_df.iloc[historical_len:].reset_index(drop=True)\n",
    "\n",
    "def make_simple_features(data):\n",
    "    \"\"\"간단한 피처 생성 (시차 변수 제외)\"\"\"\n",
    "    df = data.copy()\n",
    "    df['날짜'] = pd.to_datetime(df['날짜'])\n",
    "    df = df.sort_values('날짜').reset_index(drop=True)\n",
    "    \n",
    "    # 기본 피처들\n",
    "    df['월'] = df['날짜'].dt.month\n",
    "    df['요일'] = df['날짜'].dt.weekday\n",
    "    \n",
    "    # 계절/불쾌지수 매핑\n",
    "    season_map = {'봄': 0, '여름': 1, '가을': 2, '겨울': 3}\n",
    "    discomfort_map = {'쾌적': 0, '약간 불쾌': 1, '불쾌': 2, '매우 불쾌': 3, '극심한 불쾌': 4}\n",
    "    \n",
    "    if '계절' in df.columns:\n",
    "        df['계절'] = df['계절'].map(season_map).astype('Int64')\n",
    "    if '불쾌지수등급' in df.columns:\n",
    "        df['불쾌지수등급'] = df['불쾌지수등급'].map(discomfort_map).astype('Int64')\n",
    "    \n",
    "    # 기본 계산 피처들\n",
    "    if '일_최고기온(°C)' in df.columns and '일_최저기온(°C)' in df.columns:\n",
    "        df['일교차'] = df['일_최고기온(°C)'] - df['일_최저기온(°C)']\n",
    "    \n",
    "    if '일_일강수량(mm)' in df.columns:\n",
    "        df['폭우_여부'] = (df['일_일강수량(mm)'] >= 80).astype(int)\n",
    "    \n",
    "    # 체감온도 계산\n",
    "    T = pd.to_numeric(df.get('일_평균기온(°C)', np.nan), errors='coerce')\n",
    "    V_ms = pd.to_numeric(df.get('일_평균풍속(m/s)', np.nan), errors='coerce')\n",
    "    RH = pd.to_numeric(df.get('평균습도(%)', np.nan), errors='coerce')\n",
    "    \n",
    "    e = (RH/100.0) * 6.105 * np.exp(17.27*T/(237.7 + T))\n",
    "    df['체감온도(°C)'] = T + 0.33*e - 0.70*V_ms - 4.00\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ================================================================================================\n",
    "# 3. 평가 함수들\n",
    "# ================================================================================================\n",
    "def evaluate_regression_model(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"회귀 모델 평가\"\"\"\n",
    "    try:\n",
    "        pipe = make_pipeline_unified(model, model_name, \"regression\")\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = pipe.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-8))) * 100\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'regression',\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'mape': mape,\n",
    "            'success': True\n",
    "        }, pipe, y_pred\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'regression',\n",
    "            'mae': np.nan,\n",
    "            'rmse': np.nan,\n",
    "            'r2': np.nan,\n",
    "            'mape': np.nan,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }, None, None\n",
    "\n",
    "def evaluate_classification_model(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"분류 모델 평가\"\"\"\n",
    "    try:\n",
    "        pipe = make_pipeline_unified(model, model_name, \"classification\")\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = pipe.predict(X_test)\n",
    "        \n",
    "        if isinstance(y_pred, np.ndarray) and y_pred.ndim > 1:\n",
    "            y_pred = y_pred.ravel()\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1_macro = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "        f1_weighted = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "        \n",
    "        extreme_classes = [0, 3]\n",
    "        y_true_extreme = pd.Series(y_test).isin(extreme_classes).astype(int)\n",
    "        y_pred_extreme = pd.Series(y_pred).isin(extreme_classes).astype(int)\n",
    "        extreme_f1 = f1_score(y_true_extreme, y_pred_extreme, zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'classification',\n",
    "            'accuracy': acc,\n",
    "            'macro_f1': f1_macro,\n",
    "            'weighted_f1': f1_weighted,\n",
    "            'extreme_f1': extreme_f1,\n",
    "            'success': True\n",
    "        }, pipe, y_pred\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'classification',\n",
    "            'accuracy': np.nan,\n",
    "            'macro_f1': np.nan,\n",
    "            'weighted_f1': np.nan,\n",
    "            'extreme_f1': np.nan,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }, None, None\n",
    "\n",
    "def comprehensive_evaluation_comparison(center_name, df):\n",
    "    \"\"\"Stratified vs 시계열 분할 비교 평가\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"센터: {center_name} - Stratified vs 시계열 분할 비교\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"데이터 크기: {len(df)}행, {len(df.columns)}컬럼\")\n",
    "    \n",
    "    if '등급_1일후' in df.columns:\n",
    "        grade_dist = df['등급_1일후'].value_counts().sort_index()\n",
    "        print(f\"등급 분포: {dict(grade_dist)}\")\n",
    "        \n",
    "        min_class = grade_dist.min()\n",
    "        max_class = grade_dist.max()\n",
    "        imbalance_ratio = max_class / min_class\n",
    "        print(f\"클래스 불균형 비율: {imbalance_ratio:.1f}:1 (최대:{max_class}, 최소:{min_class})\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for split_method in ['temporal', 'stratified']:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"분할 방법: {split_method.upper()}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # 회귀 모델 평가\n",
    "        reg_method_name = \"random_shuffle\" if split_method == \"stratified\" else split_method\n",
    "        print(f\"\\n--- 회귀 모델 평가 ({reg_method_name}) ---\")\n",
    "        \n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test, feature_names, dates_train, dates_test = prepare_data_stratified(\n",
    "                df, target_col=\"합계_1일후\", model_type=\"regression\", test_size=0.2, split_method=split_method\n",
    "            )\n",
    "            \n",
    "            print(f\"회귀용 데이터: 학습 {len(X_train)}행, 테스트 {len(X_test)}행\")\n",
    "            \n",
    "            regression_models = build_regression_models()\n",
    "            \n",
    "            for model_name, model in tqdm(regression_models.items(), desc=f\"회귀({reg_method_name})\", leave=False):\n",
    "                result, pipe, y_pred = evaluate_regression_model(model, model_name, X_train, X_test, y_train, y_test)\n",
    "                result['center'] = center_name\n",
    "                result['split_method'] = split_method\n",
    "                results.append(result)\n",
    "                \n",
    "                if result['success']:\n",
    "                    print(f\"  {model_name:18s}: R²={result['r2']:.3f}, MAE={result['mae']:.0f}, MAPE={result['mape']:.1f}%\")\n",
    "                else:\n",
    "                    print(f\"  {model_name:18s}: 실패 - {result.get('error', '')[:50]}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"회귀 모델 평가 실패 ({reg_method_name}): {e}\")\n",
    "        \n",
    "        # 분류 모델 평가\n",
    "        print(f\"\\n--- 분류 모델 평가 ({split_method}) ---\")\n",
    "        \n",
    "        try:\n",
    "            X_train_clf, X_test_clf, y_train_clf, y_test_clf, feature_names_clf, _, _ = prepare_data_stratified(\n",
    "                df, target_col=\"등급_1일후\", model_type=\"classification\", test_size=0.2, split_method=split_method\n",
    "            )\n",
    "            \n",
    "            print(f\"분류용 데이터: 학습 {len(X_train_clf)}행, 테스트 {len(X_test_clf)}행\")\n",
    "            \n",
    "            test_dist = pd.Series(y_test_clf).value_counts().sort_index()\n",
    "            train_dist = pd.Series(y_train_clf).value_counts().sort_index()\n",
    "            print(f\"학습 세트 등급 분포: {dict(train_dist)}\")\n",
    "            print(f\"테스트 세트 등급 분포: {dict(test_dist)}\")\n",
    "            \n",
    "            classification_models = build_classification_models()\n",
    "            \n",
    "            for model_name, model in tqdm(classification_models.items(), desc=f\"분류({split_method})\", leave=False):\n",
    "                result, pipe, y_pred = evaluate_classification_model(model, model_name, X_train_clf, X_test_clf, y_train_clf, y_test_clf)\n",
    "                result['center'] = center_name\n",
    "                result['split_method'] = split_method\n",
    "                results.append(result)\n",
    "                \n",
    "                if result['success']:\n",
    "                    print(f\"  {model_name:18s}: ACC={result['accuracy']:.3f}, F1={result['macro_f1']:.3f}, 극값F1={result['extreme_f1']:.3f}\")\n",
    "                else:\n",
    "                    print(f\"  {model_name:18s}: 실패 - {result.get('error', '')[:50]}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"분류 모델 평가 실패 ({split_method}): {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ================================================================================================\n",
    "# 4. Feature Importance & SHAP 분석 함수들 (선택사항)\n",
    "# ================================================================================================\n",
    "def extract_feature_importance(model, model_name, feature_names):\n",
    "    \"\"\"모델별 Feature Importance 추출\"\"\"\n",
    "    try:\n",
    "        mdl = model.named_steps['model']\n",
    "        if hasattr(mdl, 'feature_importances_'):\n",
    "            importance = mdl.feature_importances_\n",
    "        elif hasattr(mdl, 'coef_'):\n",
    "            coef = mdl.coef_\n",
    "            if isinstance(coef, np.ndarray) and coef.ndim == 2:\n",
    "                importance = np.mean(np.abs(coef), axis=0)\n",
    "            else:\n",
    "                importance = np.abs(coef)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        if len(importance) != len(feature_names):\n",
    "            print(f\"[경고] importance 길이({len(importance)}) != feature_names({len(feature_names)})\")\n",
    "            m = min(len(importance), len(feature_names))\n",
    "            importance = np.asarray(importance)[:m]\n",
    "            feature_names = list(feature_names)[:m]\n",
    "\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        return importance_df\n",
    "    except Exception as e:\n",
    "        print(f\"Feature importance 추출 실패 ({model_name}): {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_feature_importance(importance_df, model_name, top_n=15):\n",
    "    \"\"\"Feature Importance 시각화\"\"\"\n",
    "    if importance_df is None or len(importance_df) == 0:\n",
    "        return None\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    top_features = importance_df.head(top_n)\n",
    "    \n",
    "    ax.barh(range(len(top_features)), top_features['importance'], color='skyblue')\n",
    "    ax.set_yticks(range(len(top_features)))\n",
    "    ax.set_yticklabels(top_features['feature'])\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title(f'{model_name} - Top {top_n} Feature Importance')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    for i, v in enumerate(top_features['importance']):\n",
    "        ax.text(v + 0.001, i, f'{v:.3f}', va='center')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analyze_model_with_shap(model, X_test, feature_names, model_name, max_samples=100):\n",
    "    \"\"\"SHAP 분석 - 다중분류 대응 강화\"\"\"\n",
    "    if not HAS_SHAP:\n",
    "        print(\"SHAP 라이브러리가 설치되지 않았습니다.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if len(X_test) > max_samples:\n",
    "            sample_idx = np.random.choice(len(X_test), max_samples, replace=False)\n",
    "            X_sample = X_test.iloc[sample_idx]\n",
    "        else:\n",
    "            X_sample = X_test\n",
    "        \n",
    "        X_processed = model.named_steps['pre'].transform(X_sample)\n",
    "        \n",
    "        if 'RandomForest' in model_name or 'GradientBoosting' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        elif 'XGBoost' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        elif 'LightGBM' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        elif 'CatBoost' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        else:\n",
    "            explainer = shap.LinearExplainer(model.named_steps['model'], X_processed)\n",
    "        \n",
    "        shap_values = explainer.shap_values(X_processed)\n",
    "        \n",
    "        # 원본 SHAP 값 디버깅\n",
    "        print(f\"원본 SHAP 값 타입: {type(shap_values)}\")\n",
    "        if isinstance(shap_values, list):\n",
    "            print(f\"리스트 길이: {len(shap_values)}\")\n",
    "            if len(shap_values) > 0:\n",
    "                print(f\"첫 번째 요소 shape: {shap_values[0].shape if hasattr(shap_values[0], 'shape') else 'no shape'}\")\n",
    "        elif hasattr(shap_values, 'shape'):\n",
    "            print(f\"배열 shape: {shap_values.shape}\")\n",
    "        \n",
    "        # *** 여기서는 처리하지 않고 그대로 반환 ***\n",
    "        return shap_values, X_processed, explainer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"SHAP 분석 실패 ({model_name}): {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_shap_summary(shap_values, X_processed, feature_names, model_name):\n",
    "    \"\"\"SHAP Summary Plot - 완전 수정 버전\"\"\"\n",
    "    if shap_values is None or not HAS_SHAP:\n",
    "        return []\n",
    "\n",
    "    figs = []\n",
    "    try:\n",
    "        # 다중분류 SHAP 값 처리 - 더 세밀한 처리\n",
    "        shap_values_use = None\n",
    "        \n",
    "        if isinstance(shap_values, list):\n",
    "            print(f\"SHAP 값이 리스트, 길이: {len(shap_values)}\")\n",
    "            if len(shap_values) > 0:\n",
    "                # 다중분류의 경우 첫 번째 클래스만 사용\n",
    "                shap_values_use = shap_values[0]\n",
    "                print(f\"첫 번째 클래스 shape: {shap_values_use.shape}\")\n",
    "        elif isinstance(shap_values, np.ndarray):\n",
    "            if shap_values.ndim == 3:\n",
    "                print(f\"3차원 배열: {shap_values.shape}\")\n",
    "                # (samples, features, classes) -> (samples, features)\n",
    "                shap_values_use = shap_values[:, :, 0]\n",
    "                print(f\"첫 번째 클래스 추출 후: {shap_values_use.shape}\")\n",
    "            elif shap_values.ndim == 2:\n",
    "                print(f\"2차원 배열: {shap_values.shape}\")\n",
    "                shap_values_use = shap_values\n",
    "            else:\n",
    "                print(f\"예상치 못한 차원: {shap_values.ndim}\")\n",
    "                shap_values_use = shap_values\n",
    "        else:\n",
    "            print(f\"알 수 없는 타입: {type(shap_values)}\")\n",
    "            shap_values_use = shap_values\n",
    "        \n",
    "        # 최종 확인\n",
    "        if shap_values_use is None:\n",
    "            print(\"SHAP 값 처리 실패\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"최종 사용할 SHAP 값 shape: {shap_values_use.shape}\")\n",
    "        print(f\"X_processed shape: {X_processed.shape}\")\n",
    "        print(f\"feature_names 길이: {len(feature_names)}\")\n",
    "        \n",
    "        # 차원 일치성 확인\n",
    "        if hasattr(shap_values_use, 'shape') and len(shap_values_use.shape) >= 2:\n",
    "            if shap_values_use.shape[1] != len(feature_names):\n",
    "                print(f\"경고: SHAP 피처 수({shap_values_use.shape[1]}) != feature_names 수({len(feature_names)})\")\n",
    "                # 최소 길이로 맞춤\n",
    "                min_features = min(shap_values_use.shape[1], len(feature_names))\n",
    "                shap_values_use = shap_values_use[:, :min_features]\n",
    "                feature_names = feature_names[:min_features]\n",
    "                print(f\"조정 후 - SHAP: {shap_values_use.shape}, features: {len(feature_names)}\")\n",
    "        \n",
    "        # Bar plot 시도\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            shap.summary_plot(shap_values_use, X_processed[:len(shap_values_use)],\n",
    "                              feature_names=feature_names,\n",
    "                              plot_type=\"bar\", show=False)\n",
    "            plt.title(f'{model_name} - SHAP Feature Importance')\n",
    "            figs.append(plt.gcf())\n",
    "            print(\"    SHAP bar plot 성공\")\n",
    "        except Exception as e:\n",
    "            print(f\"    SHAP bar plot 실패: {e}\")\n",
    "        \n",
    "        # Beeswarm plot 시도 (더 까다로움)\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            shap.summary_plot(shap_values_use, X_processed[:len(shap_values_use)],\n",
    "                              feature_names=feature_names,\n",
    "                              show=False)\n",
    "            plt.title(f'{model_name} - SHAP Summary Plot')\n",
    "            figs.append(plt.gcf())\n",
    "            print(\"    SHAP beeswarm plot 성공\")\n",
    "        except Exception as e:\n",
    "            print(f\"    SHAP beeswarm plot 실패: {e}\")\n",
    "            # beeswarm이 실패해도 bar plot이 성공했다면 계속 진행\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"SHAP 시각화 전체 실패 ({model_name}): {e}\")\n",
    "\n",
    "    plt.close('all') # 혹시 남은 figure들 다 정리\n",
    "    return figs\n",
    "# ================================================================================================\n",
    "# 5. 유틸리티 함수들\n",
    "# ================================================================================================\n",
    "def load_original_data():\n",
    "    \"\"\"원본 데이터 로드\"\"\"\n",
    "    nanji_raw = pd.read_csv('../data/processed/center_season/nanji/난지_merged.csv', encoding='utf-8-sig')\n",
    "    jungnang_raw = pd.read_csv('../data/processed/center_season/jungnang/중랑_merged.csv', encoding='utf-8-sig')\n",
    "    seonam_raw = pd.read_csv('../data/processed/center_season/seonam/서남_merged.csv', encoding='utf-8-sig')\n",
    "    tancheon_raw = pd.read_csv('../data/processed/center_season/tancheon/탄천_merged.csv', encoding='utf-8-sig')\n",
    "    \n",
    "    return {\n",
    "        \"nanji\": nanji_raw,\n",
    "        \"jungnang\": jungnang_raw,\n",
    "        \"seonam\": seonam_raw,\n",
    "        \"tancheon\": tancheon_raw\n",
    "    }\n",
    "\n",
    "def prepare_prediction_features(future_data, expected_features):\n",
    "    \"\"\"예측용 피처 준비\"\"\"\n",
    "    not_use_col = [\n",
    "        '날짜', '1처리장','2처리장','정화조','중계펌프장','합계','시설현대화',\n",
    "        '3처리장','4처리장','합계', '합계_1일후','합계_2일후',\n",
    "        '등급','등급_1일후','등급_2일후'\n",
    "    ]\n",
    "    \n",
    "    available_cols = [col for col in future_data.columns if col not in not_use_col]\n",
    "    X_future = future_data[available_cols].copy()\n",
    "    \n",
    "    for c in X_future.columns:\n",
    "        X_future[c] = pd.to_numeric(X_future[c], errors=\"coerce\")\n",
    "    \n",
    "    missing_features = set(expected_features) - set(X_future.columns)\n",
    "    if missing_features:\n",
    "        for feature in missing_features:\n",
    "            X_future[feature] = 0\n",
    "    \n",
    "    X_future = X_future[expected_features].copy()\n",
    "    return X_future\n",
    "\n",
    "# ================================================================================================\n",
    "# 모델 성능 비교 시각화 추가 코드 (버전 13에 추가)\n",
    "# ================================================================================================\n",
    "def plot_data_characteristics_comparison(centers_data, results_dir, timestamp):\n",
    "    \"\"\"센터별 데이터 특성 비교\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('센터별 데이터 특성 비교', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    center_stats = []\n",
    "    \n",
    "    for center_name, df in centers_data.items():\n",
    "        df['날짜'] = pd.to_datetime(df['날짜'])\n",
    "        df = df.sort_values('날짜').reset_index(drop=True)\n",
    "        \n",
    "        # 기본 통계\n",
    "        stats = {\n",
    "            'center': center_name,\n",
    "            'total_records': len(df),\n",
    "            'date_range': (df['날짜'].max() - df['날짜'].min()).days,\n",
    "            'avg_flow': df['합계'].mean() if '합계' in df.columns else 0,\n",
    "            'std_flow': df['합계'].std() if '합계' in df.columns else 0,\n",
    "            'missing_ratio': df.isnull().sum().sum() / (len(df) * len(df.columns))\n",
    "        }\n",
    "        center_stats.append(stats)\n",
    "    \n",
    "    center_stats_df = pd.DataFrame(center_stats)\n",
    "    \n",
    "    # 1. 데이터 양 비교\n",
    "    axes[0,0].bar(center_stats_df['center'], center_stats_df['total_records'], color='skyblue')\n",
    "    axes[0,0].set_title('센터별 데이터 양')\n",
    "    axes[0,0].set_ylabel('레코드 수')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    for i, v in enumerate(center_stats_df['total_records']):\n",
    "        axes[0,0].text(i, v + 10, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    # 2. 평균 유량 비교\n",
    "    axes[0,1].bar(center_stats_df['center'], center_stats_df['avg_flow'], color='lightgreen')\n",
    "    axes[0,1].set_title('센터별 평균 유량')\n",
    "    axes[0,1].set_ylabel('평균 유량')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    for i, v in enumerate(center_stats_df['avg_flow']):\n",
    "        axes[0,1].text(i, v + max(center_stats_df['avg_flow'])*0.01, f'{v:.0f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. 유량 변동성 비교\n",
    "    axes[1,0].bar(center_stats_df['center'], center_stats_df['std_flow'], color='coral')\n",
    "    axes[1,0].set_title('센터별 유량 변동성 (표준편차)')\n",
    "    axes[1,0].set_ylabel('표준편차')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    for i, v in enumerate(center_stats_df['std_flow']):\n",
    "        axes[1,0].text(i, v + max(center_stats_df['std_flow'])*0.01, f'{v:.0f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. 데이터 완성도\n",
    "    missing_pct = center_stats_df['missing_ratio'] * 100\n",
    "    axes[1,1].bar(center_stats_df['center'], 100 - missing_pct, color='gold')\n",
    "    axes[1,1].set_title('센터별 데이터 완성도')\n",
    "    axes[1,1].set_ylabel('완성도 (%)')\n",
    "    axes[1,1].set_ylim(0, 100)\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    for i, v in enumerate(100 - missing_pct):\n",
    "        axes[1,1].text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"data_characteristics_comparison_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  데이터 특성 비교 저장: {os.path.basename(save_path)}\")\n",
    "\n",
    "def plot_model_complexity_vs_performance(training_summary, results_dir, timestamp):\n",
    "    \"\"\"모델 복잡도 vs 성능 관계\"\"\"\n",
    "    \n",
    "    # 모델 복잡도 정의 (상대적)\n",
    "    complexity_map = {\n",
    "        'LinearRegression': 1,\n",
    "        'LogisticRegression_Clf': 1,\n",
    "        'RandomForest_Reg': 3,\n",
    "        'RandomForest_Clf': 3,\n",
    "        'GradientBoosting_Reg': 4,\n",
    "        'GradientBoosting_Clf': 4,\n",
    "        'XGBoost_Reg': 4,\n",
    "        'XGBoost_Clf': 4,\n",
    "        'LightGBM_Reg': 4,\n",
    "        'LightGBM_Clf': 4,\n",
    "        'CatBoost_Reg': 5,\n",
    "        'CatBoost_Clf': 5\n",
    "    }\n",
    "    \n",
    "    if isinstance(training_summary, list):\n",
    "        summary_df = pd.DataFrame(training_summary)\n",
    "    else:\n",
    "        summary_df = training_summary.copy()\n",
    "    \n",
    "    summary_df['complexity'] = summary_df['model_name'].map(complexity_map)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle('모델 복잡도 vs 성능 관계', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 회귀 모델\n",
    "    reg_data = summary_df[summary_df['task_type'] == 'regression'].copy()\n",
    "    if len(reg_data) > 0:\n",
    "        reg_data['r2'] = reg_data['performance'].apply(lambda x: x.get('r2', 0))\n",
    "        \n",
    "        scatter = axes[0].scatter(reg_data['complexity'], reg_data['r2'], \n",
    "                                 c=reg_data.index, cmap='viridis', s=100, alpha=0.7)\n",
    "        \n",
    "        for i, row in reg_data.iterrows():\n",
    "            axes[0].annotate(f\"{row['center'][:3]}\", \n",
    "                           (row['complexity'], row['r2']),\n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        axes[0].set_xlabel('모델 복잡도 (1=단순, 5=복잡)')\n",
    "        axes[0].set_ylabel('R² Score')\n",
    "        axes[0].set_title('회귀: 복잡도 vs 성능')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].set_xticks(range(1, 6))\n",
    "    \n",
    "    # 분류 모델\n",
    "    clf_data = summary_df[summary_df['task_type'] == 'classification'].copy()\n",
    "    if len(clf_data) > 0:\n",
    "        clf_data['macro_f1'] = clf_data['performance'].apply(lambda x: x.get('macro_f1', 0))\n",
    "        \n",
    "        scatter = axes[1].scatter(clf_data['complexity'], clf_data['macro_f1'], \n",
    "                                 c=clf_data.index, cmap='plasma', s=100, alpha=0.7)\n",
    "        \n",
    "        for i, row in clf_data.iterrows():\n",
    "            axes[1].annotate(f\"{row['center'][:3]}\", \n",
    "                           (row['complexity'], row['macro_f1']),\n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        axes[1].set_xlabel('모델 복잡도 (1=단순, 5=복잡)')\n",
    "        axes[1].set_ylabel('Macro F1 Score')\n",
    "        axes[1].set_title('분류: 복잡도 vs 성능')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].set_xticks(range(1, 6))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"model_complexity_vs_performance_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  복잡도 vs 성능 저장: {os.path.basename(save_path)}\")\n",
    "\n",
    "def plot_prediction_difficulty_analysis(training_summary, results_dir, timestamp):\n",
    "    \"\"\"센터별 예측 난이도 분석\"\"\"\n",
    "    \n",
    "    if isinstance(training_summary, list):\n",
    "        summary_df = pd.DataFrame(training_summary)\n",
    "    else:\n",
    "        summary_df = training_summary.copy()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('센터별 예측 난이도 분석', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    centers = summary_df['center'].unique()\n",
    "    \n",
    "    # 1. 센터별 최고 R² 성능\n",
    "    reg_best_scores = []\n",
    "    for center in centers:\n",
    "        center_reg = summary_df[(summary_df['center'] == center) & \n",
    "                               (summary_df['task_type'] == 'regression')]\n",
    "        if len(center_reg) > 0:\n",
    "            center_reg = center_reg.copy()\n",
    "            center_reg['r2'] = center_reg['performance'].apply(lambda x: x.get('r2', 0))\n",
    "            reg_best_scores.append(center_reg['r2'].max())\n",
    "        else:\n",
    "            reg_best_scores.append(0)\n",
    "    \n",
    "    bars1 = axes[0,0].bar(centers, reg_best_scores, color='skyblue')\n",
    "    axes[0,0].set_title('센터별 최고 R² 성능 (높을수록 예측 용이)')\n",
    "    axes[0,0].set_ylabel('최고 R² Score')\n",
    "    axes[0,0].set_ylim(0, 1)\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, score in zip(bars1, reg_best_scores):\n",
    "        axes[0,0].text(bar.get_x() + bar.get_width()/2, score + 0.01, \n",
    "                      f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. 센터별 성능 분산 (일관성)\n",
    "    reg_score_vars = []\n",
    "    for center in centers:\n",
    "        center_reg = summary_df[(summary_df['center'] == center) & \n",
    "                               (summary_df['task_type'] == 'regression')]\n",
    "        if len(center_reg) > 0:\n",
    "            center_reg = center_reg.copy()\n",
    "            center_reg['r2'] = center_reg['performance'].apply(lambda x: x.get('r2', 0))\n",
    "            reg_score_vars.append(center_reg['r2'].std())\n",
    "        else:\n",
    "            reg_score_vars.append(0)\n",
    "    \n",
    "    bars2 = axes[0,1].bar(centers, reg_score_vars, color='lightcoral')\n",
    "    axes[0,1].set_title('센터별 성능 분산 (낮을수록 일관성 높음)')\n",
    "    axes[0,1].set_ylabel('R² Score 표준편차')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, var in zip(bars2, reg_score_vars):\n",
    "        axes[0,1].text(bar.get_x() + bar.get_width()/2, var + max(reg_score_vars)*0.01, \n",
    "                      f'{var:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. 센터별 최고 F1 성능\n",
    "    clf_best_scores = []\n",
    "    for center in centers:\n",
    "        center_clf = summary_df[(summary_df['center'] == center) & \n",
    "                               (summary_df['task_type'] == 'classification')]\n",
    "        if len(center_clf) > 0:\n",
    "            center_clf = center_clf.copy()\n",
    "            center_clf['macro_f1'] = center_clf['performance'].apply(lambda x: x.get('macro_f1', 0))\n",
    "            clf_best_scores.append(center_clf['macro_f1'].max())\n",
    "        else:\n",
    "            clf_best_scores.append(0)\n",
    "    \n",
    "    bars3 = axes[1,0].bar(centers, clf_best_scores, color='lightgreen')\n",
    "    axes[1,0].set_title('센터별 최고 F1 성능 (높을수록 분류 용이)')\n",
    "    axes[1,0].set_ylabel('최고 Macro F1 Score')\n",
    "    axes[1,0].set_ylim(0, 1)\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, score in zip(bars3, clf_best_scores):\n",
    "        axes[1,0].text(bar.get_x() + bar.get_width()/2, score + 0.01, \n",
    "                      f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. 종합 예측 난이도 점수 (R² + F1 평균)\n",
    "    difficulty_scores = [(r2 + f1) / 2 for r2, f1 in zip(reg_best_scores, clf_best_scores)]\n",
    "    \n",
    "    # 색상 맵핑 (점수가 높을수록 초록, 낮을수록 빨강)\n",
    "    colors = plt.cm.RdYlGn([score for score in difficulty_scores])\n",
    "    \n",
    "    bars4 = axes[1,1].bar(centers, difficulty_scores, color=colors)\n",
    "    axes[1,1].set_title('종합 예측 용이도 점수')\n",
    "    axes[1,1].set_ylabel('종합 점수 (R² + F1) / 2')\n",
    "    axes[1,1].set_ylim(0, 1)\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, score in zip(bars4, difficulty_scores):\n",
    "        axes[1,1].text(bar.get_x() + bar.get_width()/2, score + 0.01, \n",
    "                      f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"prediction_difficulty_analysis_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  예측 난이도 분석 저장: {os.path.basename(save_path)}\")\n",
    "\n",
    "def plot_split_method_detailed_comparison(training_summary, results_dir, timestamp):\n",
    "    \"\"\"Split Method 상세 비교 분석\"\"\"\n",
    "    \n",
    "    if isinstance(training_summary, list):\n",
    "        summary_df = pd.DataFrame(training_summary)\n",
    "    else:\n",
    "        summary_df = training_summary.copy()\n",
    "    \n",
    "    # split_method 정보가 있는지 확인\n",
    "    if 'split_method' not in summary_df.columns:\n",
    "        print(\"Split method 정보가 없어 해당 그래프를 생성할 수 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('Stratified vs Temporal Split 상세 비교', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 회귀 데이터\n",
    "    reg_data = summary_df[summary_df['task_type'] == 'regression'].copy()\n",
    "    if len(reg_data) > 0:\n",
    "        reg_data['r2'] = reg_data['performance'].apply(lambda x: x.get('r2', 0))\n",
    "        reg_data['mae'] = reg_data['performance'].apply(lambda x: x.get('mae', 0))\n",
    "        \n",
    "        # 센터별 split method 성능 비교\n",
    "        reg_pivot = reg_data.pivot_table(index='center', columns='split_method', values='r2')\n",
    "        reg_pivot.plot(kind='bar', ax=axes[0,0])\n",
    "        axes[0,0].set_title('센터별 R² 성능: Split Method 비교')\n",
    "        axes[0,0].set_ylabel('R² Score')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        axes[0,0].legend(title='Split Method')\n",
    "        \n",
    "        # Split method별 성능 분포\n",
    "        split_methods = reg_data['split_method'].unique()\n",
    "        r2_by_split = [reg_data[reg_data['split_method'] == method]['r2'].values \n",
    "                       for method in split_methods]\n",
    "        \n",
    "        axes[0,1].boxplot(r2_by_split, labels=split_methods)\n",
    "        axes[0,1].set_title('Split Method별 R² 분포')\n",
    "        axes[0,1].set_ylabel('R² Score')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 성능 차이 분석\n",
    "        if len(split_methods) == 2:\n",
    "            method1, method2 = split_methods\n",
    "            perf_diff = []\n",
    "            centers = reg_data['center'].unique()\n",
    "            \n",
    "            for center in centers:\n",
    "                center_data = reg_data[reg_data['center'] == center]\n",
    "                perf1 = center_data[center_data['split_method'] == method1]['r2']\n",
    "                perf2 = center_data[center_data['split_method'] == method2]['r2']\n",
    "                \n",
    "                if len(perf1) > 0 and len(perf2) > 0:\n",
    "                    perf_diff.append(perf1.iloc[0] - perf2.iloc[0])\n",
    "            \n",
    "            if perf_diff:\n",
    "                axes[0,2].bar(centers[:len(perf_diff)], perf_diff, \n",
    "                             color=['green' if x > 0 else 'red' for x in perf_diff])\n",
    "                axes[0,2].set_title(f'{method1} - {method2} 성능 차이')\n",
    "                axes[0,2].set_ylabel('R² 차이')\n",
    "                axes[0,2].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "                axes[0,2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 분류 데이터\n",
    "    clf_data = summary_df[summary_df['task_type'] == 'classification'].copy()\n",
    "    if len(clf_data) > 0:\n",
    "        clf_data['macro_f1'] = clf_data['performance'].apply(lambda x: x.get('macro_f1', 0))\n",
    "        clf_data['accuracy'] = clf_data['performance'].apply(lambda x: x.get('accuracy', 0))\n",
    "        \n",
    "        # 센터별 split method 성능 비교\n",
    "        clf_pivot = clf_data.pivot_table(index='center', columns='split_method', values='macro_f1')\n",
    "        clf_pivot.plot(kind='bar', ax=axes[1,0])\n",
    "        axes[1,0].set_title('센터별 F1 성능: Split Method 비교')\n",
    "        axes[1,0].set_ylabel('Macro F1 Score')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        axes[1,0].legend(title='Split Method')\n",
    "        \n",
    "        # Split method별 성능 분포\n",
    "        split_methods = clf_data['split_method'].unique()\n",
    "        f1_by_split = [clf_data[clf_data['split_method'] == method]['macro_f1'].values \n",
    "                       for method in split_methods]\n",
    "        \n",
    "        axes[1,1].boxplot(f1_by_split, labels=split_methods)\n",
    "        axes[1,1].set_title('Split Method별 F1 분포')\n",
    "        axes[1,1].set_ylabel('Macro F1 Score')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 성능 차이 분석\n",
    "        if len(split_methods) == 2:\n",
    "            method1, method2 = split_methods\n",
    "            perf_diff = []\n",
    "            centers = clf_data['center'].unique()\n",
    "            \n",
    "            for center in centers:\n",
    "                center_data = clf_data[clf_data['center'] == center]\n",
    "                perf1 = center_data[center_data['split_method'] == method1]['macro_f1']\n",
    "                perf2 = center_data[center_data['split_method'] == method2]['macro_f1']\n",
    "                \n",
    "                if len(perf1) > 0 and len(perf2) > 0:\n",
    "                    perf_diff.append(perf1.iloc[0] - perf2.iloc[0])\n",
    "            \n",
    "            if perf_diff:\n",
    "                axes[1,2].bar(centers[:len(perf_diff)], perf_diff, \n",
    "                             color=['green' if x > 0 else 'red' for x in perf_diff])\n",
    "                axes[1,2].set_title(f'{method1} - {method2} F1 차이')\n",
    "                axes[1,2].set_ylabel('F1 차이')\n",
    "                axes[1,2].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "                axes[1,2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"split_method_detailed_comparison_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  Split Method 상세 비교 저장: {os.path.basename(save_path)}\")\n",
    "\n",
    "def plot_split_method_detailed_comparison_fixed(all_training_results, results_dir, timestamp):\n",
    "    \"\"\"Split Method 상세 비교 분석 - 수정된 버전\"\"\"\n",
    "    \n",
    "    # all_training_results를 직접 DataFrame으로 변환\n",
    "    if isinstance(all_training_results, list):\n",
    "        summary_df = pd.DataFrame(all_training_results)\n",
    "    else:\n",
    "        summary_df = all_training_results.copy()\n",
    "    \n",
    "    # 성공한 결과만 필터링\n",
    "    successful_results = summary_df[summary_df['success'] == True].copy()\n",
    "    \n",
    "    if len(successful_results) == 0 or 'split_method' not in successful_results.columns:\n",
    "        print(\"Split method 정보가 없거나 성공한 결과가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('Stratified vs Temporal Split 상세 비교', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 회귀 데이터 처리\n",
    "    reg_data = successful_results[successful_results['type'] == 'regression'].copy()\n",
    "    if len(reg_data) > 0:\n",
    "        # 센터별 split method 성능 비교\n",
    "        reg_pivot = reg_data.pivot_table(index='center', columns='split_method', values='r2')\n",
    "        reg_pivot.plot(kind='bar', ax=axes[0,0])\n",
    "        axes[0,0].set_title('센터별 R² 성능: Split Method 비교')\n",
    "        axes[0,0].set_ylabel('R² Score')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        axes[0,0].legend(title='Split Method')\n",
    "        \n",
    "        # Split method별 성능 분포\n",
    "        split_methods = reg_data['split_method'].unique()\n",
    "        r2_by_split = [reg_data[reg_data['split_method'] == method]['r2'].values \n",
    "                       for method in split_methods]\n",
    "        \n",
    "        axes[0,1].boxplot(r2_by_split, labels=split_methods)\n",
    "        axes[0,1].set_title('Split Method별 R² 분포')\n",
    "        axes[0,1].set_ylabel('R² Score')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 성능 차이 분석\n",
    "        if len(split_methods) == 2:\n",
    "            method1, method2 = split_methods\n",
    "            perf_diff = []\n",
    "            centers = reg_data['center'].unique()\n",
    "            \n",
    "            for center in centers:\n",
    "                center_data = reg_data[reg_data['center'] == center]\n",
    "                perf1 = center_data[center_data['split_method'] == method1]['r2']\n",
    "                perf2 = center_data[center_data['split_method'] == method2]['r2']\n",
    "                \n",
    "                if len(perf1) > 0 and len(perf2) > 0:\n",
    "                    perf_diff.append(perf1.mean() - perf2.mean())  # 평균 차이 사용\n",
    "            \n",
    "            if perf_diff:\n",
    "                axes[0,2].bar(centers[:len(perf_diff)], perf_diff, \n",
    "                             color=['green' if x > 0 else 'red' for x in perf_diff])\n",
    "                axes[0,2].set_title(f'{method1} - {method2} R² 차이')\n",
    "                axes[0,2].set_ylabel('R² 차이')\n",
    "                axes[0,2].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "                axes[0,2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 분류 데이터 처리 (동일한 방식)\n",
    "    clf_data = successful_results[successful_results['type'] == 'classification'].copy()\n",
    "    if len(clf_data) > 0:\n",
    "        # 분류 성능 비교 로직... (동일)\n",
    "        clf_pivot = clf_data.pivot_table(index='center', columns='split_method', values='macro_f1')\n",
    "        clf_pivot.plot(kind='bar', ax=axes[1,0])\n",
    "        axes[1,0].set_title('센터별 F1 성능: Split Method 비교')\n",
    "        axes[1,0].set_ylabel('Macro F1 Score')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        axes[1,0].legend(title='Split Method')\n",
    "        \n",
    "        # 나머지 분류 그래프들...\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"split_method_detailed_comparison_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  Split Method 상세 비교 저장: {os.path.basename(save_path)}\")\n",
    "\n",
    "def create_extended_performance_comparison_plots(training_summary, centers_data=None, results_dir='./results_v5'):\n",
    "    \"\"\"확장된 성능 비교 시각화 생성\"\"\"\n",
    "    \n",
    "    print(\"\\n확장된 성능 비교 그래프 생성 중...\")\n",
    "    \n",
    "    if not training_summary or len(training_summary) == 0:\n",
    "        print(\"비교할 성능 데이터가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    # DataFrame 변환\n",
    "    if isinstance(training_summary, list):\n",
    "        summary_df = pd.DataFrame(training_summary)\n",
    "    else:\n",
    "        summary_df = training_summary.copy()\n",
    "    \n",
    "    # 결과 저장 디렉토리 확인\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    created_plots = []\n",
    "    \n",
    "    # 기존 4개 그래프\n",
    "    try:\n",
    "        plot_regression_performance_comparison(summary_df, results_dir, timestamp)\n",
    "        plot_classification_performance_comparison(summary_df, results_dir, timestamp)\n",
    "        plot_model_performance_comparison(summary_df, results_dir, timestamp)\n",
    "        plot_best_performance_summary(summary_df, results_dir, timestamp)\n",
    "        created_plots.extend(['regression', 'classification', 'model_overall', 'best_summary'])\n",
    "        print(\"  기존 4개 그래프 생성 완료\")\n",
    "    except Exception as e:\n",
    "        print(f\"  기존 그래프 생성 중 오류: {e}\")\n",
    "    \n",
    "    # 추가 그래프들\n",
    "    try:\n",
    "        if centers_data:\n",
    "            plot_data_characteristics_comparison(centers_data, results_dir, timestamp)\n",
    "            created_plots.append('data_characteristics')\n",
    "        plot_model_complexity_vs_performance(training_summary, results_dir, timestamp)\n",
    "        created_plots.append('complexity_vs_performance')\n",
    "        plot_prediction_difficulty_analysis(training_summary, results_dir, timestamp)\n",
    "        created_plots.append('prediction_difficulty')\n",
    "        # 수정된 부분: all_training_results 사용\n",
    "        if all_training_results:\n",
    "            plot_split_method_detailed_comparison_fixed(all_training_results, results_dir, timestamp)\n",
    "            created_plots.append('split_method_detailed')\n",
    "        print(\"  추가 4개 그래프 생성 완료\")\n",
    "    except Exception as e:\n",
    "        print(f\"  추가 그래프 생성 중 오류: {e}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    print(f\"\\n총 {len(created_plots)}개 그래프가 생성됨: {results_dir}\")\n",
    "    \n",
    "    # 메모리 정리\n",
    "    plt.close('all')\n",
    "    gc.collect()\n",
    "    print(\"확장 그래프 생성 메모리 정리 완료\")\n",
    "    \n",
    "    return created_plots\n",
    "\n",
    "def create_performance_comparison_plots(training_summary, results_dir='./results_v5'):\n",
    "    \"\"\"모델 성능 비교 시각화 생성\"\"\"\n",
    "    \n",
    "    print(\"\\n모델 성능 비교 그래프 생성 중...\")\n",
    "    \n",
    "    if not training_summary or len(training_summary) == 0:\n",
    "        print(\"비교할 성능 데이터가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    # DataFrame 변환\n",
    "    if isinstance(training_summary, list):\n",
    "        summary_df = pd.DataFrame(training_summary)\n",
    "    else:\n",
    "        summary_df = training_summary.copy()\n",
    "    \n",
    "    # 결과 저장 디렉토리 확인\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. 센터별 회귀 성능 비교\n",
    "    plot_regression_performance_comparison(summary_df, results_dir, timestamp)\n",
    "    \n",
    "    # 2. 센터별 분류 성능 비교  \n",
    "    plot_classification_performance_comparison(summary_df, results_dir, timestamp)\n",
    "    \n",
    "    # 3. 모델별 성능 비교 (전체 센터 통합)\n",
    "    plot_model_performance_comparison(summary_df, results_dir, timestamp)\n",
    "    \n",
    "    # 4. 센터별 최고 성능 요약\n",
    "    plot_best_performance_summary(summary_df, results_dir, timestamp)\n",
    "    \n",
    "    print(f\"성능 비교 그래프 저장 완료: {results_dir}\")\n",
    "    # *** 여기에 메모리 정리 추가 ***\n",
    "    plt.close('all')\n",
    "    gc.collect()\n",
    "    print(\"성능 그래프 메모리 정리 완료\")\n",
    "\n",
    "def plot_regression_performance_comparison(summary_df, results_dir, timestamp):\n",
    "    \"\"\"회귀 모델 성능 비교 (R², MAE)\"\"\"\n",
    "    \n",
    "    reg_data = summary_df[summary_df['task_type'] == 'regression'].copy()\n",
    "    if len(reg_data) == 0:\n",
    "        return\n",
    "    \n",
    "    # 성능 지표 추출\n",
    "    reg_data['r2'] = reg_data['performance'].apply(lambda x: x.get('r2', 0))\n",
    "    reg_data['mae'] = reg_data['performance'].apply(lambda x: x.get('mae', 0))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('회귀 모델 성능 비교', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # R² 비교 (센터별)\n",
    "    pivot_r2 = reg_data.pivot_table(index='center', columns='model_name', values='r2', fill_value=0)\n",
    "    pivot_r2.plot(kind='bar', ax=axes[0,0], color=plt.cm.Set3.colors)\n",
    "    axes[0,0].set_title('센터별 R² 성능')\n",
    "    axes[0,0].set_ylabel('R² Score')\n",
    "    axes[0,0].set_ylim(0, 1)\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    axes[0,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # MAE 비교 (센터별)  \n",
    "    pivot_mae = reg_data.pivot_table(index='center', columns='model_name', values='mae', fill_value=0)\n",
    "    pivot_mae.plot(kind='bar', ax=axes[0,1], color=plt.cm.Set2.colors)\n",
    "    axes[0,1].set_title('센터별 MAE 성능 (낮을수록 좋음)')\n",
    "    axes[0,1].set_ylabel('MAE')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    axes[0,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 모델별 평균 R²\n",
    "    model_r2_avg = reg_data.groupby('model_name')['r2'].mean().sort_values(ascending=False)\n",
    "    model_r2_avg.plot(kind='bar', ax=axes[1,0], color='skyblue')\n",
    "    axes[1,0].set_title('모델별 평균 R² 성능')\n",
    "    axes[1,0].set_ylabel('평균 R² Score')\n",
    "    axes[1,0].set_ylim(0, 1)\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 값 표시\n",
    "    for i, v in enumerate(model_r2_avg.values):\n",
    "        axes[1,0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 모델별 평균 MAE\n",
    "    model_mae_avg = reg_data.groupby('model_name')['mae'].mean().sort_values(ascending=True)\n",
    "    model_mae_avg.plot(kind='bar', ax=axes[1,1], color='lightcoral')\n",
    "    axes[1,1].set_title('모델별 평균 MAE 성능')\n",
    "    axes[1,1].set_ylabel('평균 MAE')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 값 표시\n",
    "    for i, v in enumerate(model_mae_avg.values):\n",
    "        axes[1,1].text(i, v + max(model_mae_avg)*0.01, f'{v:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"regression_performance_comparison_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  회귀 성능 비교 저장: {os.path.basename(save_path)}\")\n",
    "\n",
    "def plot_classification_performance_comparison(summary_df, results_dir, timestamp):\n",
    "    \"\"\"분류 모델 성능 비교 (Accuracy, F1)\"\"\"\n",
    "    \n",
    "    clf_data = summary_df[summary_df['task_type'] == 'classification'].copy()\n",
    "    if len(clf_data) == 0:\n",
    "        return\n",
    "    \n",
    "    # 성능 지표 추출\n",
    "    clf_data['accuracy'] = clf_data['performance'].apply(lambda x: x.get('accuracy', 0))\n",
    "    clf_data['macro_f1'] = clf_data['performance'].apply(lambda x: x.get('macro_f1', 0))\n",
    "    clf_data['extreme_f1'] = clf_data['performance'].apply(lambda x: x.get('extreme_f1', 0))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('분류 모델 성능 비교', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Accuracy 비교 (센터별)\n",
    "    pivot_acc = clf_data.pivot_table(index='center', columns='model_name', values='accuracy', fill_value=0)\n",
    "    pivot_acc.plot(kind='bar', ax=axes[0,0], color=plt.cm.Set3.colors)\n",
    "    axes[0,0].set_title('센터별 정확도')\n",
    "    axes[0,0].set_ylabel('Accuracy')\n",
    "    axes[0,0].set_ylim(0, 1)\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    axes[0,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Macro F1 비교 (센터별)\n",
    "    pivot_f1 = clf_data.pivot_table(index='center', columns='model_name', values='macro_f1', fill_value=0)\n",
    "    pivot_f1.plot(kind='bar', ax=axes[0,1], color=plt.cm.Set2.colors)\n",
    "    axes[0,1].set_title('센터별 Macro F1')\n",
    "    axes[0,1].set_ylabel('Macro F1 Score')\n",
    "    axes[0,1].set_ylim(0, 1)\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    axes[0,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 모델별 평균 정확도\n",
    "    model_acc_avg = clf_data.groupby('model_name')['accuracy'].mean().sort_values(ascending=False)\n",
    "    model_acc_avg.plot(kind='bar', ax=axes[1,0], color='lightgreen')\n",
    "    axes[1,0].set_title('모델별 평균 정확도')\n",
    "    axes[1,0].set_ylabel('평균 Accuracy')\n",
    "    axes[1,0].set_ylim(0, 1)\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 값 표시\n",
    "    for i, v in enumerate(model_acc_avg.values):\n",
    "        axes[1,0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 극값 F1 비교\n",
    "    pivot_extreme = clf_data.pivot_table(index='center', columns='model_name', values='extreme_f1', fill_value=0)\n",
    "    pivot_extreme.plot(kind='bar', ax=axes[1,1], color=plt.cm.Pastel1.colors)\n",
    "    axes[1,1].set_title('센터별 극값 F1 성능')\n",
    "    axes[1,1].set_ylabel('Extreme F1 Score')\n",
    "    axes[1,1].set_ylim(0, 1)\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    axes[1,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"classification_performance_comparison_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  분류 성능 비교 저장: {os.path.basename(save_path)}\")\n",
    "\n",
    "def plot_model_performance_comparison(summary_df, results_dir, timestamp):\n",
    "    \"\"\"모델별 성능 비교 (전체 센터 통합)\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('모델별 성능 종합 비교', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 회귀 모델들\n",
    "    reg_data = summary_df[summary_df['task_type'] == 'regression'].copy()\n",
    "    if len(reg_data) > 0:\n",
    "        reg_data['r2'] = reg_data['performance'].apply(lambda x: x.get('r2', 0))\n",
    "        reg_data['mae'] = reg_data['performance'].apply(lambda x: x.get('mae', 0))\n",
    "        reg_data['mape'] = reg_data['performance'].apply(lambda x: x.get('mape', 0))\n",
    "        \n",
    "        # 모델별 R² 분포\n",
    "        reg_models = reg_data['model_name'].unique()\n",
    "        r2_data = [reg_data[reg_data['model_name'] == model]['r2'].values for model in reg_models]\n",
    "        axes[0,0].boxplot(r2_data, labels=reg_models)\n",
    "        axes[0,0].set_title('회귀 모델별 R² 분포')\n",
    "        axes[0,0].set_ylabel('R² Score')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 모델별 MAE 분포  \n",
    "        mae_data = [reg_data[reg_data['model_name'] == model]['mae'].values for model in reg_models]\n",
    "        axes[0,1].boxplot(mae_data, labels=reg_models)\n",
    "        axes[0,1].set_title('회귀 모델별 MAE 분포')\n",
    "        axes[0,1].set_ylabel('MAE')\n",
    "        axes[0,1].tick_params(axis='x', rotation=45)\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # R² vs MAE 산점도\n",
    "        # axes[0,2].scatter(reg_data['r2'], reg_data['mae'], c=plt.cm.tab10.colors, s=100, alpha=0.7)\n",
    "        # R² vs MAE 산점도\n",
    "        # 데이터 개수만큼만 색상 리스트를 잘라서 사용\n",
    "        num_points = len(reg_data)\n",
    "        axes[0,2].scatter(reg_data['r2'], reg_data['mae'], c=plt.cm.tab10.colors[:num_points], s=100, alpha=0.7)\n",
    "        for i, model in enumerate(reg_data['model_name']):\n",
    "            axes[0,2].annotate(f\"{reg_data.iloc[i]['center']}_{model[:3]}\", \n",
    "                             (reg_data.iloc[i]['r2'], reg_data.iloc[i]['mae']),\n",
    "                             xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        axes[0,2].set_title('R² vs MAE')\n",
    "        axes[0,2].set_xlabel('R² Score')\n",
    "        axes[0,2].set_ylabel('MAE')\n",
    "        axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 분류 모델들\n",
    "    clf_data = summary_df[summary_df['task_type'] == 'classification'].copy()\n",
    "    if len(clf_data) > 0:\n",
    "        clf_data['accuracy'] = clf_data['performance'].apply(lambda x: x.get('accuracy', 0))\n",
    "        clf_data['macro_f1'] = clf_data['performance'].apply(lambda x: x.get('macro_f1', 0))\n",
    "        clf_data['extreme_f1'] = clf_data['performance'].apply(lambda x: x.get('extreme_f1', 0))\n",
    "        \n",
    "        # 모델별 정확도 분포\n",
    "        clf_models = clf_data['model_name'].unique()\n",
    "        acc_data = [clf_data[clf_data['model_name'] == model]['accuracy'].values for model in clf_models]\n",
    "        axes[1,0].boxplot(acc_data, labels=clf_models)\n",
    "        axes[1,0].set_title('분류 모델별 정확도 분포')\n",
    "        axes[1,0].set_ylabel('Accuracy')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 모델별 F1 분포\n",
    "        f1_data = [clf_data[clf_data['model_name'] == model]['macro_f1'].values for model in clf_models]\n",
    "        axes[1,1].boxplot(f1_data, labels=clf_models)\n",
    "        axes[1,1].set_title('분류 모델별 Macro F1 분포')\n",
    "        axes[1,1].set_ylabel('Macro F1 Score')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy vs F1 산점도\n",
    "        # axes[1,2].scatter(clf_data['accuracy'], clf_data['macro_f1'], c=plt.cm.tab10.colors, s=100, alpha=0.7)\n",
    "        num_points_clf = len(clf_data)\n",
    "        axes[1,2].scatter(clf_data['accuracy'], clf_data['macro_f1'], c=plt.cm.tab10.colors[:num_points_clf], s=100, alpha=0.7)\n",
    "        for i, model in enumerate(clf_data['model_name']):\n",
    "            axes[1,2].annotate(f\"{clf_data.iloc[i]['center']}_{model[:3]}\", \n",
    "                             (clf_data.iloc[i]['accuracy'], clf_data.iloc[i]['macro_f1']),\n",
    "                             xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        axes[1,2].set_title('정확도 vs Macro F1')\n",
    "        axes[1,2].set_xlabel('Accuracy')\n",
    "        axes[1,2].set_ylabel('Macro F1 Score')\n",
    "        axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"model_performance_comparison_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  모델 종합 비교 저장: {os.path.basename(save_path)}\")\n",
    "\n",
    "def plot_best_performance_summary(summary_df, results_dir, timestamp):\n",
    "    \"\"\"센터별 최고 성능 요약\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle('센터별 최고 성능 모델 요약', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    centers = summary_df['center'].unique()\n",
    "    \n",
    "    # 회귀 최고 성능\n",
    "    reg_best = []\n",
    "    reg_models = []\n",
    "    for center in centers:\n",
    "        center_reg = summary_df[(summary_df['center'] == center) & (summary_df['task_type'] == 'regression')]\n",
    "        if len(center_reg) > 0:\n",
    "            center_reg = center_reg.copy()\n",
    "            center_reg['r2'] = center_reg['performance'].apply(lambda x: x.get('r2', 0))\n",
    "            best_idx = center_reg['r2'].idxmax()\n",
    "            reg_best.append(center_reg.loc[best_idx, 'r2'])\n",
    "            reg_models.append(center_reg.loc[best_idx, 'model_name'][:8])  # 모델명 축약\n",
    "        else:\n",
    "            reg_best.append(0)\n",
    "            reg_models.append('None')\n",
    "    \n",
    "    # 분류 최고 성능  \n",
    "    clf_best = []\n",
    "    clf_models = []\n",
    "    for center in centers:\n",
    "        center_clf = summary_df[(summary_df['center'] == center) & (summary_df['task_type'] == 'classification')]\n",
    "        if len(center_clf) > 0:\n",
    "            center_clf = center_clf.copy()\n",
    "            center_clf['macro_f1'] = center_clf['performance'].apply(lambda x: x.get('macro_f1', 0))\n",
    "            best_idx = center_clf['macro_f1'].idxmax()\n",
    "            clf_best.append(center_clf.loc[best_idx, 'macro_f1'])\n",
    "            clf_models.append(center_clf.loc[best_idx, 'model_name'][:8])  # 모델명 축약\n",
    "        else:\n",
    "            clf_best.append(0)\n",
    "            clf_models.append('None')\n",
    "    \n",
    "    # 회귀 최고 성능 플롯\n",
    "    bars1 = axes[0].bar(centers, reg_best, color='skyblue', alpha=0.7)\n",
    "    axes[0].set_title('센터별 최고 회귀 성능 (R²)')\n",
    "    axes[0].set_ylabel('R² Score')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 값과 모델명 표시\n",
    "    for i, (bar, score, model) in enumerate(zip(bars1, reg_best, reg_models)):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, score + 0.01, \n",
    "                    f'{score:.3f}\\n{model}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 분류 최고 성능 플롯\n",
    "    bars2 = axes[1].bar(centers, clf_best, color='lightcoral', alpha=0.7)\n",
    "    axes[1].set_title('센터별 최고 분류 성능 (Macro F1)')\n",
    "    axes[1].set_ylabel('Macro F1 Score')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 값과 모델명 표시\n",
    "    for i, (bar, score, model) in enumerate(zip(bars2, clf_best, clf_models)):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, score + 0.01, \n",
    "                    f'{score:.3f}\\n{model}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"best_performance_summary_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  최고 성능 요약 저장: {os.path.basename(save_path)}\")\n",
    "# ================================================================================================\n",
    "# 7. 2단계: 새로운 데이터로 예측 파이프라인\n",
    "# ================================================================================================\n",
    "def predict_with_saved_models(new_data_path, models_dir='./trained_models_v5', \n",
    "                             results_dir='./results_v5', historical_data_for_features=None):\n",
    "    \"\"\"저장된 모델들로 새로운 데이터 예측\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"2단계: 새로운 데이터 예측\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 새로운 데이터 로드\n",
    "    if isinstance(new_data_path, str):\n",
    "        new_data = pd.read_csv(new_data_path, encoding='utf-8-sig')\n",
    "        print(f\"새 데이터 로드: {new_data_path} ({len(new_data)}행)\")\n",
    "    else:\n",
    "        new_data = new_data_path.copy()\n",
    "        print(f\"새 데이터 제공됨: {len(new_data)}행\")\n",
    "    \n",
    "    # 저장된 모델 목록 확인\n",
    "    saved_models = load_all_saved_models(models_dir)\n",
    "    \n",
    "    if not saved_models:\n",
    "        print(\"저장된 모델이 없습니다. 먼저 train_and_save_best_models()를 실행하세요.\")\n",
    "        return None\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    # 센터별 예측\n",
    "    for center_name in saved_models.keys():\n",
    "        print(f\"\\n[{center_name.upper()} 센터 예측]\")\n",
    "        \n",
    "        # 해당 센터 데이터 필터링 (현재는 전체 데이터 사용)\n",
    "        center_data = new_data.copy()\n",
    "        \n",
    "        if len(center_data) == 0:\n",
    "            print(f\"  해당 센터 데이터 없음\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  예측 데이터: {len(center_data)}행\")\n",
    "        \n",
    "        # 피처 생성\n",
    "        if historical_data_for_features is not None:\n",
    "            processed_data = make_features_for_prediction(\n",
    "                historical_data_for_features[center_name], center_data\n",
    "            )\n",
    "        else:\n",
    "            processed_data = make_simple_features(center_data)\n",
    "        \n",
    "        # 센터별 모델로 예측\n",
    "        center_predictions = predict_for_center(center_name, processed_data, saved_models[center_name])\n",
    "        all_predictions.extend(center_predictions)\n",
    "    \n",
    "    # 결과 정리\n",
    "    if all_predictions:\n",
    "        results_df = pd.DataFrame(all_predictions)\n",
    "        results_df = results_df.sort_values(['center', 'date', 'task_type'])\n",
    "        \n",
    "        # 결과 저장 디렉토리 생성\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        # 결과 저장\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = os.path.join(results_dir, f\"predictions_{timestamp}.csv\")\n",
    "        results_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"\\n={'='*60}\")\n",
    "        print(f\"예측 완료!\")\n",
    "        print(f\"총 예측 건수: {len(results_df)}\")\n",
    "        print(f\"결과 저장: {output_path}\")\n",
    "        print(f\"={'='*60}\")\n",
    "        \n",
    "        # 예측 결과 요약 출력\n",
    "        print_prediction_summary(results_df)\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    else:\n",
    "        print(\"예측 결과가 없습니다.\")\n",
    "        return None\n",
    "\n",
    "def load_all_saved_models(models_dir):\n",
    "    \"\"\"저장된 모든 모델 로드\"\"\"\n",
    "    saved_models = {}\n",
    "    \n",
    "    if not os.path.exists(models_dir):\n",
    "        return saved_models\n",
    "    \n",
    "    model_files = [f for f in os.listdir(models_dir) if f.endswith('.pkl') and f != 'training_summary.csv']\n",
    "    \n",
    "    for model_file in model_files:\n",
    "        try:\n",
    "            model_path = os.path.join(models_dir, model_file)\n",
    "            \n",
    "            with open(model_path, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "            \n",
    "            center_name = model_data['center_name']\n",
    "            task_type = model_data['task_type']\n",
    "            \n",
    "            if center_name not in saved_models:\n",
    "                saved_models[center_name] = {}\n",
    "            \n",
    "            saved_models[center_name][task_type] = model_data\n",
    "            \n",
    "            print(f\"모델 로드: {center_name} {task_type} ({model_data['model_name']})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"모델 로드 실패 ({model_file}): {e}\")\n",
    "    \n",
    "    return saved_models\n",
    "\n",
    "def predict_for_center(center_name, processed_data, center_models):\n",
    "    \"\"\"센터별 모델로 예측\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for task_type, model_data in center_models.items():\n",
    "        try:\n",
    "            pipeline = model_data['pipeline']\n",
    "            feature_names = model_data['feature_names']\n",
    "            model_name = model_data['model_name']\n",
    "            target_col = model_data['target_column']\n",
    "            \n",
    "            # 예측 데이터 준비\n",
    "            X_future = prepare_prediction_features(processed_data, feature_names)\n",
    "            \n",
    "            if X_future is None or len(X_future) == 0:\n",
    "                continue\n",
    "            \n",
    "            # 예측 수행\n",
    "            y_pred = pipeline.predict(X_future)\n",
    "            \n",
    "            print(f\"  {task_type} 예측: {len(y_pred)}개\")\n",
    "            \n",
    "            # 결과 저장\n",
    "            for i in range(len(X_future)):\n",
    "                pred_result = {\n",
    "                    'date': processed_data.iloc[i]['날짜'],\n",
    "                    'center': center_name,\n",
    "                    'task_type': task_type,\n",
    "                    'model_name': model_name,\n",
    "                    'target_column': target_col,\n",
    "                    'predicted_value': int(y_pred[i]) if task_type == 'classification' else float(y_pred[i])\n",
    "                }\n",
    "                predictions.append(pred_result)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  {task_type} 예측 실패: {e}\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def print_prediction_summary(results_df):\n",
    "    \"\"\"예측 결과 요약 출력\"\"\"\n",
    "    print(f\"\\n--- 예측 결과 요약 ---\")\n",
    "    \n",
    "    for center in results_df['center'].unique():\n",
    "        center_data = results_df[results_df['center'] == center]\n",
    "        \n",
    "        print(f\"\\n{center.upper()} 센터:\")\n",
    "        \n",
    "        reg_data = center_data[center_data['task_type'] == 'regression']\n",
    "        clf_data = center_data[center_data['task_type'] == 'classification']\n",
    "        \n",
    "        if len(reg_data) > 0:\n",
    "            print(f\"  회귀 예측: {len(reg_data)}개\")\n",
    "            print(f\"  평균 예측값: {reg_data['predicted_value'].mean():.1f}\")\n",
    "            print(f\"  예측 범위: {reg_data['predicted_value'].min():.1f} ~ {reg_data['predicted_value'].max():.1f}\")\n",
    "        \n",
    "        if len(clf_data) > 0:\n",
    "            print(f\"  분류 예측: {len(clf_data)}개\")\n",
    "            grade_dist = clf_data['predicted_value'].value_counts().sort_index()\n",
    "            print(f\"  예측 등급 분포: {dict(grade_dist)}\")\n",
    "            \n",
    "# ================================================================================================\n",
    "# SHAP 분석 추가 코드 (버전13 + 성능비교 코드 뒤에 추가)\n",
    "# ================================================================================================\n",
    "\n",
    "def train_and_save_single_model_with_shap(center_name, train_data, best_result, task_type, save_dir, results_dir):\n",
    "    \"\"\"개별 모델 학습 및 저장 + SHAP 분석 포함\"\"\"\n",
    "    try:\n",
    "        model_name = best_result['model']\n",
    "        split_method = best_result['split_method']\n",
    "        target_col = \"합계_1일후\" if task_type == \"regression\" else \"등급_1일후\"\n",
    "        \n",
    "        # 전체 데이터로 학습\n",
    "        X_train, X_test, y_train, y_test, feature_names, _, _ = prepare_data_stratified(\n",
    "            train_data, target_col=target_col, model_type=task_type, \n",
    "            test_size=0.2, split_method=split_method  # SHAP용으로 테스트 데이터 확보\n",
    "        )\n",
    "        \n",
    "        # 전체 학습용 데이터 준비\n",
    "        X_all = pd.concat([X_train, X_test], ignore_index=True)\n",
    "        y_all = pd.concat([y_train, y_test], ignore_index=True)\n",
    "        \n",
    "        # 모델 구축 및 학습\n",
    "        if task_type == \"regression\":\n",
    "            models = build_regression_models()\n",
    "        else:\n",
    "            models = build_classification_models()\n",
    "        \n",
    "        model = models[model_name]\n",
    "        pipeline = make_pipeline_unified(model, model_name, task_type)\n",
    "        pipeline.fit(X_all, y_all)\n",
    "        \n",
    "        # 모델 저장\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{center_name}_{model_name}_{task_type}_{timestamp}.pkl\"\n",
    "        model_path = os.path.join(save_dir, filename)\n",
    "        \n",
    "        model_data = {\n",
    "            'pipeline': pipeline,\n",
    "            'feature_names': feature_names,\n",
    "            'model_name': model_name,\n",
    "            'center_name': center_name,\n",
    "            'task_type': task_type,\n",
    "            'performance': dict(best_result),\n",
    "            'split_method': split_method,\n",
    "            'training_date': datetime.now().isoformat(),\n",
    "            'target_column': target_col\n",
    "        }\n",
    "        \n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"    저장됨: {filename}\")\n",
    "        \n",
    "        # SHAP 분석을 위한 별도 파이프라인 학습 (테스트 데이터용)\n",
    "        analysis_pipeline = make_pipeline_unified(model, model_name, task_type)\n",
    "        analysis_pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Feature Importance 분석 및 저장\n",
    "        try:\n",
    "            importance_df = extract_feature_importance(analysis_pipeline, model_name, feature_names)\n",
    "            if importance_df is not None:\n",
    "                print(f\"    Top 5 피처: {', '.join(importance_df.head(5)['feature'].tolist())}\")\n",
    "                \n",
    "                # Feature Importance 시각화 저장\n",
    "                fig = plot_feature_importance(importance_df, f\"{center_name}_{model_name}\")\n",
    "                if fig:\n",
    "                    img_path = os.path.join(results_dir, f\"feature_importance_{center_name}_{model_name}_{task_type}_{timestamp}.png\")\n",
    "                    fig.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "                    plt.close(fig)\n",
    "                    print(f\"    Feature Importance 저장: {os.path.basename(img_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Feature Importance 분석 실패: {e}\")\n",
    "        \n",
    "        # SHAP 분석 및 저장\n",
    "        if HAS_SHAP:\n",
    "            try:\n",
    "                print(f\"    SHAP 분석 시작...\")\n",
    "                shap_result = analyze_model_with_shap(analysis_pipeline, X_test, feature_names, model_name, max_samples=50)\n",
    "                \n",
    "                if shap_result:\n",
    "                    shap_values, X_processed, explainer = shap_result\n",
    "                    \n",
    "                    # SHAP 시각화\n",
    "                    shap_figs = plot_shap_summary(shap_values, X_processed, feature_names, f\"{center_name}_{model_name}\")\n",
    "                    \n",
    "                    for i, fig in enumerate(shap_figs):\n",
    "                        suffix = \"bar\" if i == 0 else \"beeswarm\"\n",
    "                        shap_img_path = os.path.join(results_dir, f\"shap_{suffix}_{center_name}_{model_name}_{task_type}_{timestamp}.png\")\n",
    "                        fig.savefig(shap_img_path, dpi=300, bbox_inches='tight')\n",
    "                        plt.close(fig)\n",
    "                        print(f\"    SHAP {suffix} 저장: {os.path.basename(shap_img_path)}\")\n",
    "                    \n",
    "                    # SHAP 값 요약 저장 (CSV)\n",
    "                    if isinstance(shap_values, np.ndarray):\n",
    "                        mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
    "                        shap_summary_df = pd.DataFrame({\n",
    "                            'feature': feature_names,\n",
    "                            'mean_abs_shap': mean_abs_shap\n",
    "                        }).sort_values('mean_abs_shap', ascending=False)\n",
    "                        \n",
    "                        shap_csv_path = os.path.join(results_dir, f\"shap_summary_{center_name}_{model_name}_{task_type}_{timestamp}.csv\")\n",
    "                        shap_summary_df.to_csv(shap_csv_path, index=False, encoding='utf-8-sig')\n",
    "                        print(f\"    SHAP 요약 저장: {os.path.basename(shap_csv_path)}\")\n",
    "                # *** 여기에 메모리 정리 추가 ***\n",
    "                plt.close('all')\n",
    "                del shap_values, X_processed\n",
    "                gc.collect()\n",
    "                print(f\"    메모리 정리 완료\")\n",
    "            except Exception as e:\n",
    "                print(f\"    SHAP 분석 실패: {e}\")\n",
    "        else:\n",
    "            print(f\"    SHAP 라이브러리 없음 - SHAP 분석 건너뜀\")\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'performance': dict(best_result),\n",
    "            'saved_path': model_path,\n",
    "            'feature_names': feature_names,\n",
    "            'target_column': target_col\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    모델 저장 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def select_and_save_best_models_with_shap(center_name, train_data, training_results, save_dir, results_dir):\n",
    "    \"\"\"센터별 최고 성능 모델 선택 및 저장 + SHAP 분석\"\"\"\n",
    "    results_df = pd.DataFrame(training_results)\n",
    "    successful_results = results_df[results_df['success'] == True]\n",
    "    \n",
    "    if len(successful_results) == 0:\n",
    "        return None\n",
    "    \n",
    "    saved_models = {}\n",
    "    \n",
    "    # 회귀 최고 성능 모델\n",
    "    reg_results = successful_results[successful_results['type'] == 'regression']\n",
    "    if len(reg_results) > 0:\n",
    "        best_reg = reg_results.loc[reg_results['r2'].idxmax()]\n",
    "        print(f\"  최고 회귀 모델: {best_reg['model']} (R²={best_reg['r2']:.3f})\")\n",
    "        \n",
    "        # 모델 학습 및 저장 (SHAP 포함)\n",
    "        model_data = train_and_save_single_model_with_shap(\n",
    "            center_name, train_data, best_reg, 'regression', save_dir, results_dir\n",
    "        )\n",
    "        \n",
    "        if model_data:\n",
    "            saved_models['regression'] = model_data\n",
    "    \n",
    "    # 분류 최고 성능 모델\n",
    "    clf_results = successful_results[successful_results['type'] == 'classification']\n",
    "    if len(clf_results) > 0:\n",
    "        best_clf = clf_results.loc[clf_results['macro_f1'].idxmax()]\n",
    "        print(f\"  최고 분류 모델: {best_clf['model']} (F1={best_clf['macro_f1']:.3f})\")\n",
    "        \n",
    "        # 모델 학습 및 저장 (SHAP 포함)\n",
    "        model_data = train_and_save_single_model_with_shap(\n",
    "            center_name, train_data, best_clf, 'classification', save_dir, results_dir\n",
    "        )\n",
    "        \n",
    "        if model_data:\n",
    "            saved_models['classification'] = model_data\n",
    "    \n",
    "    return saved_models\n",
    "\n",
    "def save_comprehensive_results_table(all_results, save_dir, timestamp=None):\n",
    "    \"\"\"모든 모델의 상세 평가 결과를 테이블로 저장\"\"\"\n",
    "    \n",
    "    if not all_results:\n",
    "        print(\"저장할 결과가 없습니다.\")\n",
    "        return None\n",
    "    \n",
    "    if timestamp is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # DataFrame 생성\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # 성공한 결과만 필터링\n",
    "    successful_results = results_df[results_df['success'] == True].copy()\n",
    "    \n",
    "    if len(successful_results) == 0:\n",
    "        print(\"성공한 모델 결과가 없습니다.\")\n",
    "        return None\n",
    "    \n",
    "    # 결과 정리 및 정렬\n",
    "    successful_results = successful_results.sort_values(['center', 'type', 'model'])\n",
    "    \n",
    "    # 회귀와 분류 결과 분리 저장\n",
    "    reg_results = successful_results[successful_results['type'] == 'regression'].copy()\n",
    "    clf_results = successful_results[successful_results['type'] == 'classification'].copy()\n",
    "    \n",
    "    # 회귀 결과 저장\n",
    "    if len(reg_results) > 0:\n",
    "        reg_path = os.path.join(save_dir, f\"regression_results_detailed_{timestamp}.csv\")\n",
    "        reg_results_clean = reg_results[['center', 'model', 'split_method', 'r2', 'mae', 'rmse', 'mape']].copy()\n",
    "        reg_results_clean = reg_results_clean.round(4)  # 소수점 4자리로 반올림\n",
    "        reg_results_clean.to_csv(reg_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"  회귀 모델 결과 저장: {os.path.basename(reg_path)} ({len(reg_results)}개)\")\n",
    "    \n",
    "    # 분류 결과 저장\n",
    "    if len(clf_results) > 0:\n",
    "        clf_path = os.path.join(save_dir, f\"classification_results_detailed_{timestamp}.csv\")\n",
    "        clf_results_clean = clf_results[['center', 'model', 'split_method', 'accuracy', 'macro_f1', 'weighted_f1', 'extreme_f1']].copy()\n",
    "        clf_results_clean = clf_results_clean.round(4)  # 소수점 4자리로 반올림\n",
    "        clf_results_clean.to_csv(clf_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"  분류 모델 결과 저장: {os.path.basename(clf_path)} ({len(clf_results)}개)\")\n",
    "    \n",
    "    # 통합 결과 저장 (모든 모델)\n",
    "    all_path = os.path.join(save_dir, f\"all_models_results_detailed_{timestamp}.csv\")\n",
    "    \n",
    "    # 통합 테이블을 위한 컬럼 통일\n",
    "    unified_results = []\n",
    "    \n",
    "    for _, row in successful_results.iterrows():\n",
    "        if row['type'] == 'regression':\n",
    "            unified_row = {\n",
    "                'center': row['center'],\n",
    "                'task_type': row['type'],\n",
    "                'model': row['model'],\n",
    "                'split_method': row['split_method'],\n",
    "                'primary_metric': round(row['r2'], 4),\n",
    "                'secondary_metric': round(row['mae'], 4),\n",
    "                'tertiary_metric': round(row['rmse'], 4),\n",
    "                'additional_metric': round(row['mape'], 4)\n",
    "            }\n",
    "        else:  # classification\n",
    "            unified_row = {\n",
    "                'center': row['center'],\n",
    "                'task_type': row['type'],\n",
    "                'model': row['model'],\n",
    "                'split_method': row['split_method'],\n",
    "                'primary_metric': round(row['macro_f1'], 4),\n",
    "                'secondary_metric': round(row['accuracy'], 4),\n",
    "                'tertiary_metric': round(row['weighted_f1'], 4),\n",
    "                'additional_metric': round(row['extreme_f1'], 4)\n",
    "            }\n",
    "        unified_results.append(unified_row)\n",
    "    \n",
    "    unified_df = pd.DataFrame(unified_results)\n",
    "    unified_df.to_csv(all_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"  통합 결과 저장: {os.path.basename(all_path)} ({len(unified_df)}개)\")\n",
    "    \n",
    "    # 요약 통계 저장\n",
    "    summary_stats_path = os.path.join(save_dir, f\"results_summary_stats_{timestamp}.csv\")\n",
    "    \n",
    "    summary_stats = []\n",
    "    \n",
    "    # 센터별 요약\n",
    "    for center in successful_results['center'].unique():\n",
    "        center_data = successful_results[successful_results['center'] == center]\n",
    "        \n",
    "        center_reg = center_data[center_data['type'] == 'regression']\n",
    "        center_clf = center_data[center_data['type'] == 'classification']\n",
    "        \n",
    "        summary_row = {\n",
    "            'center': center,\n",
    "            'total_models': len(center_data),\n",
    "            'regression_models': len(center_reg),\n",
    "            'classification_models': len(center_clf),\n",
    "            'best_r2': center_reg['r2'].max() if len(center_reg) > 0 else 0,\n",
    "            'best_f1': center_clf['macro_f1'].max() if len(center_clf) > 0 else 0,\n",
    "            'avg_r2': center_reg['r2'].mean() if len(center_reg) > 0 else 0,\n",
    "            'avg_f1': center_clf['macro_f1'].mean() if len(center_clf) > 0 else 0,\n",
    "        }\n",
    "        summary_stats.append(summary_row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    summary_df = summary_df.round(4)\n",
    "    summary_df.to_csv(summary_stats_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"  요약 통계 저장: {os.path.basename(summary_stats_path)}\")\n",
    "    \n",
    "    return {\n",
    "        'regression_path': reg_path if len(reg_results) > 0 else None,\n",
    "        'classification_path': clf_path if len(clf_results) > 0 else None,\n",
    "        'unified_path': all_path,\n",
    "        'summary_path': summary_stats_path\n",
    "    }\n",
    "\n",
    "# comprehensive_evaluation_comparison 함수 수정 버전 (결과 수집 추가)\n",
    "def comprehensive_evaluation_comparison_with_save(center_name, df, save_results=True, save_dir=None):\n",
    "    \"\"\"Stratified vs 시계열 분할 비교 평가 + 결과 저장\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"센터: {center_name} - Stratified vs 시계열 분할 비교\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"데이터 크기: {len(df)}행, {len(df.columns)}컬럼\")\n",
    "    \n",
    "    if '등급_1일후' in df.columns:\n",
    "        grade_dist = df['등급_1일후'].value_counts().sort_index()\n",
    "        print(f\"등급 분포: {dict(grade_dist)}\")\n",
    "        \n",
    "        min_class = grade_dist.min()\n",
    "        max_class = grade_dist.max()\n",
    "        imbalance_ratio = max_class / min_class\n",
    "        print(f\"클래스 불균형 비율: {imbalance_ratio:.1f}:1 (최대:{max_class}, 최소:{min_class})\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for split_method in ['temporal', 'stratified']:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"분할 방법: {split_method.upper()}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # 회귀 모델 평가\n",
    "        reg_method_name = \"random_shuffle\" if split_method == \"stratified\" else split_method\n",
    "        print(f\"\\n--- 회귀 모델 평가 ({reg_method_name}) ---\")\n",
    "        \n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test, feature_names, dates_train, dates_test = prepare_data_stratified(\n",
    "                df, target_col=\"합계_1일후\", model_type=\"regression\", test_size=0.2, split_method=split_method\n",
    "            )\n",
    "            \n",
    "            print(f\"회귀용 데이터: 학습 {len(X_train)}행, 테스트 {len(X_test)}행\")\n",
    "            \n",
    "            regression_models = build_regression_models()\n",
    "            \n",
    "            for model_name, model in tqdm(regression_models.items(), desc=f\"회귀({reg_method_name})\", leave=False):\n",
    "                result, pipe, y_pred = evaluate_regression_model(model, model_name, X_train, X_test, y_train, y_test)\n",
    "                result['center'] = center_name\n",
    "                result['split_method'] = split_method\n",
    "                results.append(result)\n",
    "                \n",
    "                if result['success']:\n",
    "                    print(f\"  {model_name:18s}: R²={result['r2']:.3f}, MAE={result['mae']:.0f}, MAPE={result['mape']:.1f}%\")\n",
    "                else:\n",
    "                    print(f\"  {model_name:18s}: 실패 - {result.get('error', '')[:50]}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"회귀 모델 평가 실패 ({reg_method_name}): {e}\")\n",
    "        \n",
    "        # 분류 모델 평가\n",
    "        print(f\"\\n--- 분류 모델 평가 ({split_method}) ---\")\n",
    "        \n",
    "        try:\n",
    "            X_train_clf, X_test_clf, y_train_clf, y_test_clf, feature_names_clf, _, _ = prepare_data_stratified(\n",
    "                df, target_col=\"등급_1일후\", model_type=\"classification\", test_size=0.2, split_method=split_method\n",
    "            )\n",
    "            \n",
    "            print(f\"분류용 데이터: 학습 {len(X_train_clf)}행, 테스트 {len(X_test_clf)}행\")\n",
    "            \n",
    "            test_dist = pd.Series(y_test_clf).value_counts().sort_index()\n",
    "            train_dist = pd.Series(y_train_clf).value_counts().sort_index()\n",
    "            print(f\"학습 세트 등급 분포: {dict(train_dist)}\")\n",
    "            print(f\"테스트 세트 등급 분포: {dict(test_dist)}\")\n",
    "            \n",
    "            classification_models = build_classification_models()\n",
    "            \n",
    "            for model_name, model in tqdm(classification_models.items(), desc=f\"분류({split_method})\", leave=False):\n",
    "                result, pipe, y_pred = evaluate_classification_model(model, model_name, X_train_clf, X_test_clf, y_train_clf, y_test_clf)\n",
    "                result['center'] = center_name\n",
    "                result['split_method'] = split_method\n",
    "                results.append(result)\n",
    "                \n",
    "                if result['success']:\n",
    "                    print(f\"  {model_name:18s}: ACC={result['accuracy']:.3f}, F1={result['macro_f1']:.3f}, 극값F1={result['extreme_f1']:.3f}\")\n",
    "                else:\n",
    "                    print(f\"  {model_name:18s}: 실패 - {result.get('error', '')[:50]}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"분류 모델 평가 실패 ({split_method}): {e}\")\n",
    "    \n",
    "    # 결과 저장\n",
    "    if save_results and save_dir:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        center_results_path = os.path.join(save_dir, f\"{center_name}_detailed_results_{timestamp}.csv\")\n",
    "        \n",
    "        # 이 센터의 결과만 저장\n",
    "        center_results_df = pd.DataFrame([r for r in results if r.get('success', False)])\n",
    "        if len(center_results_df) > 0:\n",
    "            center_results_df.to_csv(center_results_path, index=False, encoding='utf-8-sig')\n",
    "            print(f\"\\n{center_name} 센터 상세 결과 저장: {os.path.basename(center_results_path)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def train_and_save_best_models_with_plots_and_shap(cutoff_date='2025-05-20', save_dir='./trained_model_v5', \n",
    "                                                  results_dir='./results_v5', create_plots=True):\n",
    "    \"\"\"모델 학습 + SHAP 분석 + 성능 비교 그래프 생성\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"1단계: 모델 학습 및 저장 (SHAP 분석 포함)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 저장 디렉토리 생성\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # 데이터 로드\n",
    "    centers_data = load_original_data()\n",
    "    cutoff = pd.to_datetime(cutoff_date)\n",
    "    \n",
    "    saved_models = {}\n",
    "    training_summary = []\n",
    "    all_training_results = []  # ← 새로 추가\n",
    "    \n",
    "    for center_name, df_raw in centers_data.items():\n",
    "        print(f\"\\n[{center_name.upper()} 센터 모델 학습]\")\n",
    "        \n",
    "        # 학습 데이터 준비\n",
    "        df_raw['날짜'] = pd.to_datetime(df_raw['날짜'])\n",
    "        raw_train_data = df_raw[df_raw['날짜'] <= cutoff].copy()\n",
    "        \n",
    "        if len(raw_train_data) < 50:\n",
    "            print(f\"  학습 데이터 부족\")\n",
    "            continue\n",
    "        \n",
    "        # 피처 엔지니어링\n",
    "        train_data = make_features(raw_train_data, cutoff_date=cutoff_date)\n",
    "        print(f\"  학습 데이터: {len(train_data)}행\")\n",
    "        \n",
    "        # 모델 평가\n",
    "        training_results = comprehensive_evaluation_comparison(center_name, train_data)\n",
    "        all_training_results.extend(training_results)  # ← 새로 추가\n",
    "        \n",
    "        # 최고 성능 모델 선택 및 학습 (SHAP 포함)\n",
    "        best_models = select_and_save_best_models_with_shap(center_name, train_data, training_results, save_dir, results_dir)\n",
    "        \n",
    "        if best_models:\n",
    "            saved_models[center_name] = best_models\n",
    "            \n",
    "            # 학습 요약 정보 저장\n",
    "            for task_type, model_info in best_models.items():\n",
    "                training_summary.append({\n",
    "                    'center': center_name,\n",
    "                    'task_type': task_type,\n",
    "                    'model_name': model_info['model_name'],\n",
    "                    'performance': model_info['performance'],\n",
    "                    'saved_path': model_info['saved_path'],\n",
    "                    'training_date': datetime.now().isoformat()\n",
    "                })\n",
    "        # *** 여기에 메모리 정리 추가 ***\n",
    "        plt.close('all')\n",
    "        gc.collect()\n",
    "        print(f\"  {center_name} 센터 메모리 정리 완료\")\n",
    "    # 학습 요약 저장\n",
    "    summary_path = os.path.join(save_dir, 'training_summary.csv')\n",
    "    summary_df = pd.DataFrame(training_summary)\n",
    "    summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n={'='*60}\")\n",
    "    print(f\"모델 학습 완료!\")\n",
    "    print(f\"저장 위치: {save_dir}\")\n",
    "    print(f\"학습된 모델: {len(training_summary)}개\")\n",
    "    print(f\"학습 요약: {summary_path}\")\n",
    "    print(f\"={'='*60}\")\n",
    "    \n",
    "    # 전체 모델 결과 테이블 저장\n",
    "    if all_training_results:\n",
    "        print(f\"\\n전체 모델 결과 테이블 저장 중...\")\n",
    "        save_comprehensive_results_table(all_training_results, save_dir)\n",
    "        print(f\"전체 {len(all_training_results)}개 모델 결과 저장 완료\")\n",
    "\n",
    "    # 수정:\n",
    "    if create_plots and training_summary:\n",
    "        create_extended_performance_comparison_plots(training_summary, centers_data, results_dir)\n",
    "        \n",
    "        # Split method 비교를 위해 별도 호출\n",
    "        if all_training_results:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            plot_split_method_detailed_comparison_fixed(all_training_results, results_dir, timestamp)\n",
    "    \n",
    "    return saved_models, training_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34df7da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "1단계: 모델 학습 및 저장 (SHAP 분석 포함)\n",
      "================================================================================\n",
      "\n",
      "[NANJI 센터 모델 학습]\n",
      "\n",
      "=== make_features 디버깅 ===\n",
      "입력 데이터: 3062행\n",
      "날짜 범위: 2017-01-01 00:00:00 ~ 2025-05-20 00:00:00\n",
      "cutoff_date: 2025-05-20\n",
      "  cutoff 이전 데이터: 3062행\n",
      "  cutoff 이후 데이터: 0행\n",
      "등급 구분 기준: q15=514889, q70=597260, q90=778880\n",
      "타겟 변수 생성 (cutoff 적용)\n",
      "  1일후 타겟 생성: 3061개\n",
      "  2일후 타겟 생성: 3060개\n",
      "dropna 처리: 3062행 → 3058행 (4행 제거)\n",
      "날짜 필터링: 3058행 → 3058행 (0행 제거)\n",
      "최종 등급 분포: {0.0: 458, 1.0: 1682, 2.0: 611, 3.0: 307}\n",
      "최종 출력: 3058행, 44컬럼\n",
      "최종 날짜 범위: 2017-01-03 00:00:00 ~ 2025-05-18 00:00:00\n",
      "=== make_features 완료 ===\n",
      "\n",
      "  학습 데이터: 3058행\n",
      "\n",
      "======================================================================\n",
      "센터: nanji - Stratified vs 시계열 분할 비교\n",
      "======================================================================\n",
      "데이터 크기: 3058행, 44컬럼\n",
      "등급 분포: {0.0: 458, 1.0: 1682, 2.0: 611, 3.0: 307}\n",
      "클래스 불균형 비율: 5.5:1 (최대:1682, 최소:307)\n",
      "\n",
      "==================================================\n",
      "분할 방법: TEMPORAL\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (temporal) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  17%|█▋        | 1/6 [00:00<00:04,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.557, MAE=49542, MAPE=7.3%\n",
      "  LinearRegression  : R²=0.513, MAE=62439, MAPE=9.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  50%|█████     | 3/6 [00:02<00:02,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.395, MAE=58665, MAPE=9.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  67%|██████▋   | 4/6 [00:02<00:01,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.480, MAE=56356, MAPE=8.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  83%|████████▎ | 5/6 [00:04<00:01,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.492, MAE=54005, MAPE=8.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.540, MAE=48369, MAPE=7.1%\n",
      "\n",
      "--- 분류 모델 평가 (temporal) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 453, 1: 1309, 2: 435, 3: 249}\n",
      "테스트 세트 등급 분포: {0: 5, 1: 373, 2: 176, 3: 58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  17%|█▋        | 1/6 [00:00<00:01,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.748, F1=0.481, 극값F1=0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  33%|███▎      | 2/6 [00:06<00:15,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.665, F1=0.468, 극값F1=0.386\n",
      "  LogisticRegression_Clf: ACC=0.314, F1=0.308, 극값F1=0.199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  67%|██████▋   | 4/6 [00:08<00:03,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.722, F1=0.567, 극값F1=0.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  83%|████████▎ | 5/6 [00:15<00:03,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.694, F1=0.527, 극값F1=0.432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.672, F1=0.508, 극값F1=0.404\n",
      "\n",
      "==================================================\n",
      "분할 방법: STRATIFIED\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (random_shuffle) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  17%|█▋        | 1/6 [00:00<00:04,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.612, MAE=47300, MAPE=7.0%\n",
      "  LinearRegression  : R²=0.592, MAE=50530, MAPE=7.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  50%|█████     | 3/6 [00:02<00:02,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.620, MAE=46878, MAPE=6.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  67%|██████▋   | 4/6 [00:03<00:01,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.637, MAE=45859, MAPE=6.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  83%|████████▎ | 5/6 [00:04<00:01,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.605, MAE=48431, MAPE=7.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.648, MAE=44875, MAPE=6.6%\n",
      "\n",
      "--- 분류 모델 평가 (stratified) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 366, 1: 1345, 2: 489, 3: 246}\n",
      "테스트 세트 등급 분포: {0: 92, 1: 337, 2: 122, 3: 61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  17%|█▋        | 1/6 [00:00<00:01,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.725, F1=0.664, 극값F1=0.660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  33%|███▎      | 2/6 [00:06<00:16,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.725, F1=0.658, 극값F1=0.648\n",
      "  LogisticRegression_Clf: ACC=0.613, F1=0.578, 극값F1=0.597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  67%|██████▋   | 4/6 [00:08<00:04,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.745, F1=0.667, 극값F1=0.645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  83%|████████▎ | 5/6 [00:16<00:03,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.735, F1=0.665, 극값F1=0.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.675, F1=0.630, 극값F1=0.632\n",
      "  최고 회귀 모델: CatBoost_Reg (R²=0.648)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    저장됨: nanji_CatBoost_Reg_regression_20250828_171858.pkl\n",
      "    Top 5 피처: 강수량_7일_누적, 강수량_1일_누적, 강수량_5일_누적, 하천, 세탁업\n",
      "    Feature Importance 저장: feature_importance_nanji_CatBoost_Reg_regression_20250828_171858.png\n",
      "    SHAP 분석 시작...\n",
      "원본 SHAP 값 타입: <class 'numpy.ndarray'>\n",
      "배열 shape: (50, 33)\n",
      "2차원 배열: (50, 33)\n",
      "최종 사용할 SHAP 값 shape: (50, 33)\n",
      "X_processed shape: (50, 33)\n",
      "feature_names 길이: 33\n",
      "    SHAP bar plot 성공\n",
      "    SHAP beeswarm plot 성공\n",
      "    SHAP bar 저장: shap_bar_nanji_CatBoost_Reg_regression_20250828_171858.png\n",
      "    SHAP beeswarm 저장: shap_beeswarm_nanji_CatBoost_Reg_regression_20250828_171858.png\n",
      "    SHAP 요약 저장: shap_summary_nanji_CatBoost_Reg_regression_20250828_171858.csv\n",
      "    메모리 정리 완료\n",
      "  최고 분류 모델: XGBoost_Clf (F1=0.667)\n",
      "    저장됨: nanji_XGBoost_Clf_classification_20250828_171901.pkl\n",
      "    Top 5 피처: 체감온도(°C), 강수량_7일_누적, 강수량_1일_누적, 목욕장업, 수영장업\n",
      "    Feature Importance 저장: feature_importance_nanji_XGBoost_Clf_classification_20250828_171901.png\n",
      "    SHAP 분석 시작...\n",
      "원본 SHAP 값 타입: <class 'numpy.ndarray'>\n",
      "배열 shape: (50, 33, 4)\n",
      "3차원 배열: (50, 33, 4)\n",
      "첫 번째 클래스 추출 후: (50, 33)\n",
      "최종 사용할 SHAP 값 shape: (50, 33)\n",
      "X_processed shape: (50, 33)\n",
      "feature_names 길이: 33\n",
      "    SHAP bar plot 성공\n",
      "    SHAP beeswarm plot 성공\n",
      "    SHAP bar 저장: shap_bar_nanji_XGBoost_Clf_classification_20250828_171901.png\n",
      "    SHAP beeswarm 저장: shap_beeswarm_nanji_XGBoost_Clf_classification_20250828_171901.png\n",
      "    SHAP 분석 실패: Per-column arrays must each be 1-dimensional\n",
      "  nanji 센터 메모리 정리 완료\n",
      "\n",
      "[JUNGNANG 센터 모델 학습]\n",
      "\n",
      "=== make_features 디버깅 ===\n",
      "입력 데이터: 3062행\n",
      "날짜 범위: 2017-01-01 00:00:00 ~ 2025-05-20 00:00:00\n",
      "cutoff_date: 2025-05-20\n",
      "  cutoff 이전 데이터: 3062행\n",
      "  cutoff 이후 데이터: 0행\n",
      "등급 구분 기준: q15=1141318, q70=1286034, q90=1523142\n",
      "타겟 변수 생성 (cutoff 적용)\n",
      "  1일후 타겟 생성: 3061개\n",
      "  2일후 타겟 생성: 3060개\n",
      "dropna 처리: 3062행 → 3058행 (4행 제거)\n",
      "날짜 필터링: 3058행 → 3058행 (0행 제거)\n",
      "최종 등급 분포: {0.0: 460, 1.0: 1680, 2.0: 611, 3.0: 307}\n",
      "최종 출력: 3058행, 44컬럼\n",
      "최종 날짜 범위: 2017-01-03 00:00:00 ~ 2025-05-18 00:00:00\n",
      "=== make_features 완료 ===\n",
      "\n",
      "  학습 데이터: 3058행\n",
      "\n",
      "======================================================================\n",
      "센터: jungnang - Stratified vs 시계열 분할 비교\n",
      "======================================================================\n",
      "데이터 크기: 3058행, 44컬럼\n",
      "등급 분포: {0.0: 460, 1.0: 1680, 2.0: 611, 3.0: 307}\n",
      "클래스 불균형 비율: 5.5:1 (최대:1680, 최소:307)\n",
      "\n",
      "==================================================\n",
      "분할 방법: TEMPORAL\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (temporal) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  17%|█▋        | 1/6 [00:00<00:04,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.304, MAE=91088, MAPE=7.1%\n",
      "  LinearRegression  : R²=0.055, MAE=131262, MAPE=10.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  50%|█████     | 3/6 [00:02<00:02,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.137, MAE=105765, MAPE=8.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  67%|██████▋   | 4/6 [00:03<00:01,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.269, MAE=96690, MAPE=7.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  83%|████████▎ | 5/6 [00:04<00:01,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.254, MAE=96761, MAPE=7.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.259, MAE=99822, MAPE=7.9%\n",
      "\n",
      "--- 분류 모델 평가 (temporal) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 405, 1: 1321, 2: 458, 3: 262}\n",
      "테스트 세트 등급 분포: {0: 55, 1: 359, 2: 153, 3: 45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  17%|█▋        | 1/6 [00:00<00:01,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.627, F1=0.425, 극값F1=0.311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  33%|███▎      | 2/6 [00:07<00:17,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.547, F1=0.380, 극값F1=0.303\n",
      "  LogisticRegression_Clf: ACC=0.359, F1=0.296, 극값F1=0.312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  67%|██████▋   | 4/6 [00:09<00:04,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.541, F1=0.442, 극값F1=0.342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  83%|████████▎ | 5/6 [00:16<00:03,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.531, F1=0.468, 극값F1=0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.554, F1=0.411, 극값F1=0.325\n",
      "\n",
      "==================================================\n",
      "분할 방법: STRATIFIED\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (random_shuffle) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  17%|█▋        | 1/6 [00:00<00:03,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.576, MAE=76240, MAPE=5.5%\n",
      "  LinearRegression  : R²=0.522, MAE=89786, MAPE=6.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  50%|█████     | 3/6 [00:02<00:02,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.572, MAE=78344, MAPE=5.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  67%|██████▋   | 4/6 [00:02<00:01,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.575, MAE=76245, MAPE=5.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  83%|████████▎ | 5/6 [00:04<00:01,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.562, MAE=78893, MAPE=5.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.602, MAE=75195, MAPE=5.5%\n",
      "\n",
      "--- 분류 모델 평가 (stratified) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 368, 1: 1344, 2: 489, 3: 245}\n",
      "테스트 세트 등급 분포: {0: 92, 1: 336, 2: 122, 3: 62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  17%|█▋        | 1/6 [00:00<00:01,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.711, F1=0.629, 극값F1=0.606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  33%|███▎      | 2/6 [00:07<00:16,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.730, F1=0.640, 극값F1=0.603\n",
      "  LogisticRegression_Clf: ACC=0.560, F1=0.548, 극값F1=0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  67%|██████▋   | 4/6 [00:08<00:04,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.745, F1=0.661, 극값F1=0.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  83%|████████▎ | 5/6 [00:16<00:03,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.739, F1=0.647, 극값F1=0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.691, F1=0.653, 극값F1=0.654\n",
      "  최고 회귀 모델: CatBoost_Reg (R²=0.602)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    저장됨: jungnang_CatBoost_Reg_regression_20250828_171951.pkl\n",
      "    Top 5 피처: 체력단련장업, 강수량_7일_누적, 세탁업, 생활인구, 강수량_1일_누적\n",
      "    Feature Importance 저장: feature_importance_jungnang_CatBoost_Reg_regression_20250828_171951.png\n",
      "    SHAP 분석 시작...\n",
      "원본 SHAP 값 타입: <class 'numpy.ndarray'>\n",
      "배열 shape: (50, 33)\n",
      "2차원 배열: (50, 33)\n",
      "최종 사용할 SHAP 값 shape: (50, 33)\n",
      "X_processed shape: (50, 33)\n",
      "feature_names 길이: 33\n",
      "    SHAP bar plot 성공\n",
      "    SHAP beeswarm plot 성공\n",
      "    SHAP bar 저장: shap_bar_jungnang_CatBoost_Reg_regression_20250828_171951.png\n",
      "    SHAP beeswarm 저장: shap_beeswarm_jungnang_CatBoost_Reg_regression_20250828_171951.png\n",
      "    SHAP 요약 저장: shap_summary_jungnang_CatBoost_Reg_regression_20250828_171951.csv\n",
      "    메모리 정리 완료\n",
      "  최고 분류 모델: XGBoost_Clf (F1=0.661)\n",
      "    저장됨: jungnang_XGBoost_Clf_classification_20250828_171954.pkl\n",
      "    Top 5 피처: 강수량_7일_누적, 체력단련장업, 세탁업, 일_일강수량(mm), 월\n",
      "    Feature Importance 저장: feature_importance_jungnang_XGBoost_Clf_classification_20250828_171954.png\n",
      "    SHAP 분석 시작...\n",
      "원본 SHAP 값 타입: <class 'numpy.ndarray'>\n",
      "배열 shape: (50, 33, 4)\n",
      "3차원 배열: (50, 33, 4)\n",
      "첫 번째 클래스 추출 후: (50, 33)\n",
      "최종 사용할 SHAP 값 shape: (50, 33)\n",
      "X_processed shape: (50, 33)\n",
      "feature_names 길이: 33\n",
      "    SHAP bar plot 성공\n",
      "    SHAP beeswarm plot 성공\n",
      "    SHAP bar 저장: shap_bar_jungnang_XGBoost_Clf_classification_20250828_171954.png\n",
      "    SHAP beeswarm 저장: shap_beeswarm_jungnang_XGBoost_Clf_classification_20250828_171954.png\n",
      "    SHAP 분석 실패: Per-column arrays must each be 1-dimensional\n",
      "  jungnang 센터 메모리 정리 완료\n",
      "\n",
      "[SEONAM 센터 모델 학습]\n",
      "\n",
      "=== make_features 디버깅 ===\n",
      "입력 데이터: 3062행\n",
      "날짜 범위: 2017-01-01 00:00:00 ~ 2025-05-20 00:00:00\n",
      "cutoff_date: 2025-05-20\n",
      "  cutoff 이전 데이터: 3062행\n",
      "  cutoff 이후 데이터: 0행\n",
      "등급 구분 기준: q15=1415538, q70=1572874, q90=1917382\n",
      "타겟 변수 생성 (cutoff 적용)\n",
      "  1일후 타겟 생성: 3061개\n",
      "  2일후 타겟 생성: 3060개\n",
      "dropna 처리: 3062행 → 3058행 (4행 제거)\n",
      "날짜 필터링: 3058행 → 3058행 (0행 제거)\n",
      "최종 등급 분포: {0.0: 459, 1.0: 1680, 2.0: 612, 3.0: 307}\n",
      "최종 출력: 3058행, 43컬럼\n",
      "최종 날짜 범위: 2017-01-03 00:00:00 ~ 2025-05-18 00:00:00\n",
      "=== make_features 완료 ===\n",
      "\n",
      "  학습 데이터: 3058행\n",
      "\n",
      "======================================================================\n",
      "센터: seonam - Stratified vs 시계열 분할 비교\n",
      "======================================================================\n",
      "데이터 크기: 3058행, 43컬럼\n",
      "등급 분포: {0.0: 459, 1.0: 1680, 2.0: 612, 3.0: 307}\n",
      "클래스 불균형 비율: 5.5:1 (최대:1680, 최소:307)\n",
      "\n",
      "==================================================\n",
      "분할 방법: TEMPORAL\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (temporal) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  17%|█▋        | 1/6 [00:00<00:04,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.037, MAE=157633, MAPE=10.9%\n",
      "  LinearRegression  : R²=0.058, MAE=156666, MAPE=10.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  50%|█████     | 3/6 [00:02<00:02,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.270, MAE=124493, MAPE=8.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  67%|██████▋   | 4/6 [00:03<00:01,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.244, MAE=127234, MAPE=8.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  83%|████████▎ | 5/6 [00:05<00:01,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.280, MAE=120014, MAPE=8.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.327, MAE=124794, MAPE=8.5%\n",
      "\n",
      "--- 분류 모델 평가 (temporal) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 136, 1: 1493, 2: 544, 3: 273}\n",
      "테스트 세트 등급 분포: {0: 323, 1: 187, 2: 68, 3: 34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  17%|█▋        | 1/6 [00:00<00:01,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.304, F1=0.323, 극값F1=0.124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  33%|███▎      | 2/6 [00:07<00:17,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.477, F1=0.432, 극값F1=0.593\n",
      "  LogisticRegression_Clf: ACC=0.271, F1=0.306, 극값F1=0.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  67%|██████▋   | 4/6 [00:09<00:04,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.317, F1=0.307, 극값F1=0.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  83%|████████▎ | 5/6 [00:17<00:04,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.301, F1=0.265, 극값F1=0.057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.252, F1=0.265, 극값F1=0.149\n",
      "\n",
      "==================================================\n",
      "분할 방법: STRATIFIED\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (random_shuffle) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  17%|█▋        | 1/6 [00:00<00:04,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.562, MAE=91557, MAPE=5.4%\n",
      "  LinearRegression  : R²=0.528, MAE=100152, MAPE=6.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  50%|█████     | 3/6 [00:02<00:02,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.604, MAE=86879, MAPE=5.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  67%|██████▋   | 4/6 [00:03<00:01,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.590, MAE=87201, MAPE=5.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  83%|████████▎ | 5/6 [00:04<00:01,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.578, MAE=89065, MAPE=5.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.594, MAE=86488, MAPE=5.1%\n",
      "\n",
      "--- 분류 모델 평가 (stratified) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 367, 1: 1344, 2: 489, 3: 246}\n",
      "테스트 세트 등급 분포: {0: 92, 1: 336, 2: 123, 3: 61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  17%|█▋        | 1/6 [00:00<00:01,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.727, F1=0.652, 극값F1=0.669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  33%|███▎      | 2/6 [00:07<00:17,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.735, F1=0.655, 극값F1=0.662\n",
      "  LogisticRegression_Clf: ACC=0.595, F1=0.562, 극값F1=0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  67%|██████▋   | 4/6 [00:09<00:04,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.745, F1=0.658, 극값F1=0.659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  83%|████████▎ | 5/6 [00:17<00:04,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.745, F1=0.652, 극값F1=0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.699, F1=0.644, 극값F1=0.673\n",
      "  최고 회귀 모델: GradientBoosting_Reg (R²=0.604)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    저장됨: seonam_GradientBoosting_Reg_regression_20250828_172049.pkl\n",
      "    Top 5 피처: 일_일강수량(mm), 강수량_1일_누적, 강수량_7일_누적, 일_최저기온(°C), 세탁업\n",
      "    Feature Importance 저장: feature_importance_seonam_GradientBoosting_Reg_regression_20250828_172049.png\n",
      "    SHAP 분석 시작...\n",
      "원본 SHAP 값 타입: <class 'numpy.ndarray'>\n",
      "배열 shape: (50, 33)\n",
      "2차원 배열: (50, 33)\n",
      "최종 사용할 SHAP 값 shape: (50, 33)\n",
      "X_processed shape: (50, 33)\n",
      "feature_names 길이: 33\n",
      "    SHAP bar plot 성공\n",
      "    SHAP beeswarm plot 성공\n",
      "    SHAP bar 저장: shap_bar_seonam_GradientBoosting_Reg_regression_20250828_172049.png\n",
      "    SHAP beeswarm 저장: shap_beeswarm_seonam_GradientBoosting_Reg_regression_20250828_172049.png\n",
      "    SHAP 요약 저장: shap_summary_seonam_GradientBoosting_Reg_regression_20250828_172049.csv\n",
      "    메모리 정리 완료\n",
      "  최고 분류 모델: XGBoost_Clf (F1=0.658)\n",
      "    저장됨: seonam_XGBoost_Clf_classification_20250828_172054.pkl\n",
      "    Top 5 피처: 일_일강수량(mm), 체력단련장업, 폭우_여부, 강수량_1일_누적, 세탁업\n",
      "    Feature Importance 저장: feature_importance_seonam_XGBoost_Clf_classification_20250828_172054.png\n",
      "    SHAP 분석 시작...\n",
      "원본 SHAP 값 타입: <class 'numpy.ndarray'>\n",
      "배열 shape: (50, 33, 4)\n",
      "3차원 배열: (50, 33, 4)\n",
      "첫 번째 클래스 추출 후: (50, 33)\n",
      "최종 사용할 SHAP 값 shape: (50, 33)\n",
      "X_processed shape: (50, 33)\n",
      "feature_names 길이: 33\n",
      "    SHAP bar plot 성공\n",
      "    SHAP beeswarm plot 성공\n",
      "    SHAP bar 저장: shap_bar_seonam_XGBoost_Clf_classification_20250828_172054.png\n",
      "    SHAP beeswarm 저장: shap_beeswarm_seonam_XGBoost_Clf_classification_20250828_172054.png\n",
      "    SHAP 분석 실패: Per-column arrays must each be 1-dimensional\n",
      "  seonam 센터 메모리 정리 완료\n",
      "\n",
      "[TANCHEON 센터 모델 학습]\n",
      "\n",
      "=== make_features 디버깅 ===\n",
      "입력 데이터: 3062행\n",
      "날짜 범위: 2017-01-01 00:00:00 ~ 2025-05-20 00:00:00\n",
      "cutoff_date: 2025-05-20\n",
      "  cutoff 이전 데이터: 3062행\n",
      "  cutoff 이후 데이터: 0행\n",
      "등급 구분 기준: q15=661218, q70=759972, q90=874444\n",
      "타겟 변수 생성 (cutoff 적용)\n",
      "  1일후 타겟 생성: 3061개\n",
      "  2일후 타겟 생성: 3060개\n",
      "dropna 처리: 3062행 → 3058행 (4행 제거)\n",
      "날짜 필터링: 3058행 → 3058행 (0행 제거)\n",
      "최종 등급 분포: {0.0: 459, 1.0: 1680, 2.0: 612, 3.0: 307}\n",
      "최종 출력: 3058행, 42컬럼\n",
      "최종 날짜 범위: 2017-01-03 00:00:00 ~ 2025-05-18 00:00:00\n",
      "=== make_features 완료 ===\n",
      "\n",
      "  학습 데이터: 3058행\n",
      "\n",
      "======================================================================\n",
      "센터: tancheon - Stratified vs 시계열 분할 비교\n",
      "======================================================================\n",
      "데이터 크기: 3058행, 42컬럼\n",
      "등급 분포: {0.0: 459, 1.0: 1680, 2.0: 612, 3.0: 307}\n",
      "클래스 불균형 비율: 5.5:1 (최대:1680, 최소:307)\n",
      "\n",
      "==================================================\n",
      "분할 방법: TEMPORAL\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (temporal) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  17%|█▋        | 1/6 [00:00<00:03,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.275, MAE=62889, MAPE=8.3%\n",
      "  LinearRegression  : R²=0.287, MAE=66726, MAPE=8.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  50%|█████     | 3/6 [00:02<00:02,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.027, MAE=75273, MAPE=10.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  67%|██████▋   | 4/6 [00:02<00:01,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.109, MAE=72585, MAPE=9.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(temporal):  83%|████████▎ | 5/6 [00:05<00:01,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.202, MAE=66029, MAPE=8.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.197, MAE=68829, MAPE=9.2%\n",
      "\n",
      "--- 분류 모델 평가 (temporal) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 345, 1: 1348, 2: 496, 3: 257}\n",
      "테스트 세트 등급 분포: {0: 114, 1: 332, 2: 116, 3: 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  17%|█▋        | 1/6 [00:00<00:01,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.557, F1=0.410, 극값F1=0.246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  33%|███▎      | 2/6 [00:06<00:15,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.364, F1=0.361, 극값F1=0.371\n",
      "  LogisticRegression_Clf: ACC=0.268, F1=0.259, 극값F1=0.444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  67%|██████▋   | 4/6 [00:08<00:03,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.477, F1=0.405, 극값F1=0.322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(temporal):  83%|████████▎ | 5/6 [00:16<00:03,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.454, F1=0.382, 극값F1=0.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.358, F1=0.354, 극값F1=0.369\n",
      "\n",
      "==================================================\n",
      "분할 방법: STRATIFIED\n",
      "==================================================\n",
      "\n",
      "--- 회귀 모델 평가 (random_shuffle) ---\n",
      "회귀용 데이터: 학습 2446행, 테스트 612행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  17%|█▋        | 1/6 [00:00<00:03,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Reg  : R²=0.500, MAE=48022, MAPE=6.0%\n",
      "  LinearRegression  : R²=0.377, MAE=57635, MAPE=7.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  50%|█████     | 3/6 [00:02<00:02,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Reg: R²=0.510, MAE=47588, MAPE=6.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  67%|██████▋   | 4/6 [00:02<00:01,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Reg       : R²=0.514, MAE=46528, MAPE=5.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "회귀(random_shuffle):  83%|████████▎ | 5/6 [00:04<00:01,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Reg      : R²=0.482, MAE=48315, MAPE=6.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Reg      : R²=0.515, MAE=47014, MAPE=5.9%\n",
      "\n",
      "--- 분류 모델 평가 (stratified) ---\n",
      "분류용 데이터: 학습 2446행, 테스트 612행\n",
      "학습 세트 등급 분포: {0: 367, 1: 1344, 2: 489, 3: 246}\n",
      "테스트 세트 등급 분포: {0: 92, 1: 336, 2: 123, 3: 61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  17%|█▋        | 1/6 [00:00<00:01,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RandomForest_Clf  : ACC=0.641, F1=0.538, 극값F1=0.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  33%|███▎      | 2/6 [00:06<00:16,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GradientBoosting_Clf: ACC=0.636, F1=0.524, 극값F1=0.485\n",
      "  LogisticRegression_Clf: ACC=0.449, F1=0.432, 극값F1=0.459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  67%|██████▋   | 4/6 [00:08<00:04,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost_Clf       : ACC=0.658, F1=0.547, 극값F1=0.488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "분류(stratified):  83%|████████▎ | 5/6 [00:16<00:03,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM_Clf      : ACC=0.647, F1=0.545, 극값F1=0.515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CatBoost_Clf      : ACC=0.614, F1=0.559, 극값F1=0.577\n",
      "  최고 회귀 모델: CatBoost_Reg (R²=0.515)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    저장됨: tancheon_CatBoost_Reg_regression_20250828_172144.pkl\n",
      "    Top 5 피처: 세탁업, 하천, 일_일강수량(mm), 체력단련장업, 강수량_7일_누적\n",
      "    Feature Importance 저장: feature_importance_tancheon_CatBoost_Reg_regression_20250828_172144.png\n",
      "    SHAP 분석 시작...\n",
      "원본 SHAP 값 타입: <class 'numpy.ndarray'>\n",
      "배열 shape: (50, 33)\n",
      "2차원 배열: (50, 33)\n",
      "최종 사용할 SHAP 값 shape: (50, 33)\n",
      "X_processed shape: (50, 33)\n",
      "feature_names 길이: 33\n",
      "    SHAP bar plot 성공\n",
      "    SHAP beeswarm plot 성공\n",
      "    SHAP bar 저장: shap_bar_tancheon_CatBoost_Reg_regression_20250828_172144.png\n",
      "    SHAP beeswarm 저장: shap_beeswarm_tancheon_CatBoost_Reg_regression_20250828_172144.png\n",
      "    SHAP 요약 저장: shap_summary_tancheon_CatBoost_Reg_regression_20250828_172144.csv\n",
      "    메모리 정리 완료\n",
      "  최고 분류 모델: CatBoost_Clf (F1=0.559)\n",
      "    저장됨: tancheon_CatBoost_Clf_classification_20250828_172148.pkl\n",
      "    Top 5 피처: 강수량_7일_누적, 세탁업, 월, 체력단련장업, 생활인구\n",
      "    Feature Importance 저장: feature_importance_tancheon_CatBoost_Clf_classification_20250828_172148.png\n",
      "    SHAP 분석 시작...\n",
      "원본 SHAP 값 타입: <class 'numpy.ndarray'>\n",
      "배열 shape: (50, 33, 4)\n",
      "3차원 배열: (50, 33, 4)\n",
      "첫 번째 클래스 추출 후: (50, 33)\n",
      "최종 사용할 SHAP 값 shape: (50, 33)\n",
      "X_processed shape: (50, 33)\n",
      "feature_names 길이: 33\n",
      "    SHAP bar plot 성공\n",
      "    SHAP beeswarm plot 성공\n",
      "    SHAP bar 저장: shap_bar_tancheon_CatBoost_Clf_classification_20250828_172148.png\n",
      "    SHAP beeswarm 저장: shap_beeswarm_tancheon_CatBoost_Clf_classification_20250828_172148.png\n",
      "    SHAP 분석 실패: Per-column arrays must each be 1-dimensional\n",
      "  tancheon 센터 메모리 정리 완료\n",
      "\n",
      "=============================================================\n",
      "모델 학습 완료!\n",
      "저장 위치: ./trained_model_v5\n",
      "학습된 모델: 8개\n",
      "학습 요약: ./trained_model_v5/training_summary.csv\n",
      "=============================================================\n",
      "\n",
      "전체 모델 결과 테이블 저장 중...\n",
      "  회귀 모델 결과 저장: regression_results_detailed_20250828_172150.csv (48개)\n",
      "  분류 모델 결과 저장: classification_results_detailed_20250828_172150.csv (48개)\n",
      "  통합 결과 저장: all_models_results_detailed_20250828_172150.csv (96개)\n",
      "  요약 통계 저장: results_summary_stats_20250828_172150.csv\n",
      "전체 96개 모델 결과 저장 완료\n",
      "\n",
      "확장된 성능 비교 그래프 생성 중...\n",
      "  회귀 성능 비교 저장: regression_performance_comparison_20250828_172150.png\n",
      "  분류 성능 비교 저장: classification_performance_comparison_20250828_172150.png\n",
      "  모델 종합 비교 저장: model_performance_comparison_20250828_172150.png\n",
      "  최고 성능 요약 저장: best_performance_summary_20250828_172150.png\n",
      "  기존 4개 그래프 생성 완료\n",
      "  데이터 특성 비교 저장: data_characteristics_comparison_20250828_172150.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  복잡도 vs 성능 저장: model_complexity_vs_performance_20250828_172150.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  예측 난이도 분석 저장: prediction_difficulty_analysis_20250828_172150.png\n",
      "  추가 그래프 생성 중 오류: name 'all_training_results' is not defined\n",
      "\n",
      "총 7개 그래프가 생성됨: ./results_v5\n",
      "확장 그래프 생성 메모리 정리 완료\n",
      "  Split Method 상세 비교 저장: split_method_detailed_comparison_20250828_172155.png\n"
     ]
    }
   ],
   "source": [
    "saved_models, summary = train_and_save_best_models_with_plots_and_shap()\n",
    "# results = predict_with_saved_models('새_데이터.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f464b9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 폰트: ['AppleGothic']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"현재 폰트:\", plt.rcParams[\"font.family\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19a4fab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 폰트: ['AppleGothic']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(\"현재 폰트:\", plt.rcParams.get(\"font.family\"))\n",
    "\n",
    "# 한글이 실제로 그려지는지 테스트\n",
    "plt.figure()\n",
    "plt.title(\"한글 제목: 서울 하수처리량 추이\")\n",
    "plt.plot([0,1,2],[1,4,9])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6231e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youngwon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
