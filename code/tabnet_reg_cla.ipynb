{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c7601dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] mps\n",
      "\n",
      "======================================================================\n",
      "[NANJI] 파이프라인 시작\n",
      "======================================================================\n",
      "[REG] trial 1/20 params={'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 80 with best_epoch = 50 and best_val_0_rmse = 0.88896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> new best on val R²: 0.5082\n",
      "[REG] trial 2/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_steps': 4, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 182 with best_epoch = 152 and best_val_0_rmse = 0.83399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> new best on val R²: 0.5672\n",
      "[REG] trial 3/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 96 with best_epoch = 66 and best_val_0_rmse = 0.94417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 4/20 params={'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_steps': 4, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 92 and best_val_0_rmse = 0.8387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 5/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 80 with best_epoch = 50 and best_val_0_rmse = 0.90163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 6/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 119 with best_epoch = 89 and best_val_0_rmse = 0.86531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 7/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_steps': 4, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 74 with best_epoch = 44 and best_val_0_rmse = 1.06916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 8/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 57 with best_epoch = 27 and best_val_0_rmse = 0.80316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> new best on val R²: 0.5986\n",
      "[REG] trial 9/20 params={'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_steps': 4, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 92 and best_val_0_rmse = 0.83298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 10/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 108 with best_epoch = 78 and best_val_0_rmse = 0.9212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 11/20 params={'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_steps': 4, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 80 with best_epoch = 50 and best_val_0_rmse = 0.87815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 12/20 params={'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 67 with best_epoch = 37 and best_val_0_rmse = 0.88915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 13/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 80 with best_epoch = 50 and best_val_0_rmse = 0.87488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 14/20 params={'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 118 with best_epoch = 88 and best_val_0_rmse = 0.91912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 15/20 params={'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_steps': 4, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 91 with best_epoch = 61 and best_val_0_rmse = 0.9487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 16/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_steps': 4, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 119 with best_epoch = 89 and best_val_0_rmse = 0.84415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 17/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 149 with best_epoch = 119 and best_val_0_rmse = 0.84882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 18/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 121 with best_epoch = 91 and best_val_0_rmse = 0.88399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 19/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 158 with best_epoch = 128 and best_val_0_rmse = 0.8826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 20/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 194 with best_epoch = 164 and best_val_0_rmse = 0.89412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nanji] REG done: R2=0.3707, RMSE=74913.024\n",
      "[CLS] trial 1/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.003, 'n_a': 24, 'n_d': 32, 'n_steps': 3, 'weight_decay': 1e-05}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'class_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 324\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# ===== 분류: 등급_1일후 (0~3 다중분류) =====\u001b[39;00m\n\u001b[1;32m    323\u001b[0m pack_cls \u001b[38;5;241m=\u001b[39m load_and_split(center, target_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m등급_1일후\u001b[39m\u001b[38;5;124m'\u001b[39m, for_task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 324\u001b[0m cls_model, cls_params, cls_val_best, cls_time, cls_pred, cls_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtune_tabnet_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpack_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# 결과/모델 저장\u001b[39;00m\n\u001b[1;32m    327\u001b[0m cls_pickle \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: cls_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m: cls_pred\n\u001b[1;32m    336\u001b[0m }\n",
      "Cell \u001b[0;32mIn[4], line 244\u001b[0m, in \u001b[0;36mtune_tabnet_classification\u001b[0;34m(pack, n_classes, max_epochs, batch, vbatch, patience)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trials, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CLS] trial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(trials)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m params=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 244\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mTabNetClassifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_d\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_a\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_steps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgamma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlambda_sparse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# 불균형 클래스 가중치\u001b[39;49;00m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_list\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    254\u001b[0m         X_train\u001b[38;5;241m=\u001b[39mXtr, y_train\u001b[38;5;241m=\u001b[39my_train\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m    255\u001b[0m         eval_set\u001b[38;5;241m=\u001b[39m[(Xva, y_val\u001b[38;5;241m.\u001b[39mvalues)],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    260\u001b[0m         pin_memory\u001b[38;5;241m=\u001b[39mPIN_MEM,  \u001b[38;5;66;03m# ✅ 추가\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     )\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# 검증 성능(F1_weighted)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'class_weights'"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 멀티센터 + 회귀/분류 동시 파이프라인\n",
    "# ================================================================\n",
    "import os, time, math, pickle, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    r2_score, mean_squared_error, mean_absolute_error,\n",
    "    f1_score, accuracy_score, confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor, TabNetClassifier\n",
    "\n",
    "# 맨 위쪽에 추가\n",
    "import random\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 0) 환경/출력 디렉토리 설정\n",
    "# ----------------------------\n",
    "OUT_DIR = \"../results_tabnet_corrected_v2\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 기존 코드에 device 변수가 없다면 안전하게 감지\n",
    "if \"device\" not in globals():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"[Device] {device}\")\n",
    "\n",
    "# MPS면 pin_memory=False, 그 외엔 True\n",
    "PIN_MEM = False if device == 'mps' else True\n",
    "\n",
    "\n",
    "# GPU 세팅에 따른 배치/에폭 기본값\n",
    "if device == 'mps':\n",
    "    BATCH_REG = 256; VBS_REG = 64; EPOCH_REG = 400\n",
    "    BATCH_CLS = 256; VBS_CLS = 64; EPOCH_CLS = 300\n",
    "elif device == 'cuda':\n",
    "    BATCH_REG = 512; VBS_REG = 128; EPOCH_REG = 600\n",
    "    BATCH_CLS = 512; VBS_CLS = 128; EPOCH_CLS = 400\n",
    "else:\n",
    "    BATCH_REG = 256; VBS_REG = 64; EPOCH_REG = 250\n",
    "    BATCH_CLS = 256; VBS_CLS = 64; EPOCH_CLS = 200\n",
    "\n",
    "# ----------------------------\n",
    "# 1) 공통 유틸\n",
    "# ----------------------------\n",
    "DROP_COLS_BASE = ['날짜','요일','1처리장','2처리장','정화조','중계펌프장','시설현대화','3처리장','4처리장']\n",
    "EXCLUDE_REG = ['합계_1일후','합계_2일후','등급','등급_1일후','등급_2일후','합계']\n",
    "EXCLUDE_CLS = ['합계','합계_2일후','등급','등급_2일후']  # 분류에선 '등급_1일후'가 타깃\n",
    "\n",
    "def load_and_split(center_name, target_col, for_task=\"regression\"):\n",
    "    \"\"\"센터 데이터를 로드하고 시계열 분할 및 스케일링을 수행.\n",
    "    - for_task: 'regression' or 'classification'\n",
    "    반환: dict(X_train_s, X_val_s, X_test_s, y_train, y_val, y_test, scalers, feature_names)\n",
    "    \"\"\"\n",
    "    # 1) 로드\n",
    "    path = f\"../data/add_feature/{center_name}_add_feature.csv\"\n",
    "    df = pd.read_csv(path, encoding='utf-8-sig')\n",
    "    # 2) 불필요 컬럼 제거\n",
    "    df = df.drop([c for c in DROP_COLS_BASE if c in df.columns], axis=1)\n",
    "    # 3) 결측 제거\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    # 4) 피처/타깃 분리\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"[{center_name}] Target '{target_col}' not found. available={list(df.columns)[:10]}...\")\n",
    "\n",
    "    if for_task == \"regression\":\n",
    "        exclude = [c for c in EXCLUDE_REG if c in df.columns]\n",
    "    else:\n",
    "        exclude = [c for c in EXCLUDE_CLS if c in df.columns]\n",
    "\n",
    "    X = df.drop(exclude + [target_col], axis=1, errors='ignore').copy()\n",
    "    y = df[target_col].copy()\n",
    "\n",
    "    # 분류 타깃을 int로 정리(0~3 가정)\n",
    "    if for_task == \"classification\":\n",
    "        y = y.astype(int)\n",
    "\n",
    "    # 5) 시계열 70/20/10\n",
    "    n = len(X)\n",
    "    tr_end = int(n*0.7); va_end = int(n*0.9)\n",
    "    X_train, X_val, X_test = X.iloc[:tr_end], X.iloc[tr_end:va_end], X.iloc[va_end:]\n",
    "    y_train, y_val, y_test = y.iloc[:tr_end], y.iloc[tr_end:va_end], y.iloc[va_end:]\n",
    "\n",
    "    # 6) 스케일링(누수방지): 회귀는 X, y 둘 다 / 분류는 X만\n",
    "    x_scaler = StandardScaler()\n",
    "    X_train_s = x_scaler.fit_transform(X_train).astype(np.float32)\n",
    "    X_val_s   = x_scaler.transform(X_val).astype(np.float32)\n",
    "    X_test_s  = x_scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "    if for_task == \"regression\":\n",
    "        y_scaler = StandardScaler()\n",
    "        y_train_s = y_scaler.fit_transform(y_train.values.reshape(-1,1)).astype(np.float32)\n",
    "        y_val_s   = y_scaler.transform(y_val.values.reshape(-1,1)).astype(np.float32)\n",
    "        y_test_s  = y_scaler.transform(y_test.values.reshape(-1,1)).astype(np.float32)\n",
    "        scalers = dict(x=x_scaler, y=y_scaler)\n",
    "        y_pack  = (y_train, y_val, y_test, y_train_s, y_val_s, y_test_s)\n",
    "    else:\n",
    "        scalers = dict(x=x_scaler, y=None)\n",
    "        y_pack  = (y_train, y_val, y_test, None, None, None)\n",
    "\n",
    "    return dict(\n",
    "        X_train_s=X_train_s, X_val_s=X_val_s, X_test_s=X_test_s,\n",
    "        y_pack=y_pack, feature_names=X.columns.tolist(), n_samples=n, splits=(len(X_train), len(X_val), len(X_test)),\n",
    "        scalers=scalers\n",
    "    )\n",
    "\n",
    "def regression_metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return dict(\n",
    "        R2=r2_score(y_true, y_pred),\n",
    "        RMSE=rmse,\n",
    "        MAE=mean_absolute_error(y_true, y_pred),\n",
    "        MAPE=(np.mean(np.abs((y_true - y_pred)/(y_true + 1e-8)))*100),\n",
    "        SMAPE=(np.mean(2*np.abs(y_true - y_pred)/(np.abs(y_true)+np.abs(y_pred)+1e-8))*100),\n",
    "    )\n",
    "\n",
    "def classification_metrics(y_true, y_pred):\n",
    "    return dict(\n",
    "        Accuracy=accuracy_score(y_true, y_pred),\n",
    "        F1_weighted=f1_score(y_true, y_pred, average='weighted'),\n",
    "        F1_macro=f1_score(y_true, y_pred, average='macro'),\n",
    "        CM=confusion_matrix(y_true, y_pred)\n",
    "    )\n",
    "\n",
    "def compute_class_weight(y_train):\n",
    "    \"\"\"클래스 불균형 자동 가중치 계산(단순 역빈도).\"\"\"\n",
    "    vals, cnts = np.unique(y_train, return_counts=True)\n",
    "    freq = cnts / cnts.sum()\n",
    "    weight = {int(k): float(1.0/(f+1e-8)) for k, f in zip(vals, freq)}\n",
    "    # 평균이 1 되도록 정규화\n",
    "    m = np.mean(list(weight.values()))\n",
    "    weight = {k: v/m for k,v in weight.items()}\n",
    "    return weight\n",
    "\n",
    "# ----------------------------\n",
    "# 2) 하이퍼파라미터 그리드\n",
    "# ----------------------------\n",
    "grid_reg = {\n",
    "    'n_d': [32, 64],\n",
    "    'n_a': [32, 64],\n",
    "    'n_steps': [4, 5],\n",
    "    'gamma': [1.0, 1.2],\n",
    "    'lambda_sparse': [1e-4, 1e-5],\n",
    "    'learning_rate': [0.001, 0.002, 0.005],\n",
    "    'weight_decay': [1e-5, 1e-6],\n",
    "}\n",
    "grid_cls = {\n",
    "    'n_d': [24, 32, 48],\n",
    "    'n_a': [24, 32, 48],\n",
    "    'n_steps': [3, 4, 5],\n",
    "    'gamma': [1.0, 1.3],\n",
    "    'lambda_sparse': [1e-4, 1e-5],\n",
    "    'learning_rate': [0.001, 0.002, 0.003],\n",
    "    'weight_decay': [1e-5, 1e-6],\n",
    "}\n",
    "\n",
    "# 샘플링(시간 제한)\n",
    "def sample_grid(grid, max_trials=20, seed=42):\n",
    "    combos = list(ParameterGrid(grid))\n",
    "    random.seed(seed)\n",
    "    if len(combos) > max_trials:\n",
    "        combos = random.sample(combos, max_trials)\n",
    "    return combos\n",
    "\n",
    "# ----------------------------\n",
    "# 3) 트레이닝 루틴\n",
    "# ----------------------------\n",
    "def tune_tabnet_regression(pack, max_epochs=EPOCH_REG, batch=BATCH_REG, vbatch=VBS_REG, patience=30):\n",
    "    Xtr, Xva, Xte = pack['X_train_s'], pack['X_val_s'], pack['X_test_s']\n",
    "    # y_*_s 가 (n, 1) 2D인 상태로 들어오게 됨\n",
    "    y_train, y_val, y_test, ytr_s, yva_s, yte_s = pack['y_pack']\n",
    "    y_scaler = pack['scalers']['y']\n",
    "\n",
    "    trials = sample_grid(grid_reg, max_trials=20)\n",
    "    best_score = -np.inf; best_params=None; best_model=None\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i, p in enumerate(trials, 1):\n",
    "        print(f\"[REG] trial {i}/{len(trials)} params={p}\")\n",
    "        model = TabNetRegressor(\n",
    "            n_d=p['n_d'], n_a=p['n_a'], n_steps=p['n_steps'],\n",
    "            gamma=p['gamma'], lambda_sparse=p['lambda_sparse'],\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params=dict(lr=p['learning_rate'], weight_decay=p['weight_decay']),\n",
    "            device_name=device, seed=42, verbose=0\n",
    "        )\n",
    "        # ✅ ytr_s, yva_s 모두 (n,1) 2D\n",
    "        model.fit(\n",
    "            X_train=Xtr, y_train=ytr_s,\n",
    "            eval_set=[(Xva, yva_s)],\n",
    "            eval_metric=['rmse'],\n",
    "            max_epochs=max_epochs, patience=patience,\n",
    "            batch_size=batch, virtual_batch_size=vbatch,\n",
    "            num_workers=0, drop_last=False,\n",
    "            pin_memory=PIN_MEM,  # ✅ 추가\n",
    "        )\n",
    "\n",
    "        # 예측도 (n,1) → inverse_transform 후 1D로 평탄화\n",
    "        yva_pred_s = model.predict(Xva)                        # (n,1)\n",
    "        yva_pred   = y_scaler.inverse_transform(yva_pred_s)    # (n,1)\n",
    "        yva_pred   = yva_pred.ravel()                          # (n,)\n",
    "\n",
    "        val_r2 = r2_score(y_val.values, yva_pred)\n",
    "        if val_r2 > best_score:\n",
    "            best_score, best_params, best_model = val_r2, p, model\n",
    "            print(\"  -> new best on val R²:\", round(val_r2, 4))\n",
    "\n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    # 테스트 성능\n",
    "    yte_pred_s = best_model.predict(Xte)                       # (n,1)\n",
    "    yte_pred   = y_scaler.inverse_transform(yte_pred_s).ravel()\n",
    "    reg_metrics = regression_metrics(y_test.values, yte_pred)\n",
    "\n",
    "    return best_model, best_params, best_score, train_time, dict(y_true=y_test.values, y_pred=yte_pred), reg_metrics\n",
    "\n",
    "\n",
    "def tune_tabnet_classification(pack, n_classes=4, max_epochs=EPOCH_CLS, batch=BATCH_CLS, vbatch=VBS_CLS, patience=30):\n",
    "    Xtr, Xva, Xte = pack['X_train_s'], pack['X_val_s'], pack['X_test_s']\n",
    "    y_train, y_val, y_test, *_ = pack['y_pack']\n",
    "\n",
    "    trials = sample_grid(grid_cls, max_trials=20)\n",
    "    class_weight = compute_class_weight(y_train.values)\n",
    "    # TabNetClassifier는 class_weights 리스트/np.array 형식 기대\n",
    "    weights_list = np.array([class_weight.get(i,1.0) for i in range(n_classes)], dtype=np.float32)\n",
    "\n",
    "    best_score = -np.inf; best_params=None; best_model=None\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i, p in enumerate(trials, 1):\n",
    "        print(f\"[CLS] trial {i}/{len(trials)} params={p}\")\n",
    "        model = TabNetClassifier(\n",
    "            n_d=p['n_d'], n_a=p['n_a'], n_steps=p['n_steps'],\n",
    "            gamma=p['gamma'], lambda_sparse=p['lambda_sparse'],\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params=dict(lr=p['learning_rate'], weight_decay=p['weight_decay']),\n",
    "            device_name=device, seed=42, verbose=0,\n",
    "            # 불균형 클래스 가중치\n",
    "            class_weights=weights_list\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train=Xtr, y_train=y_train.values,\n",
    "            eval_set=[(Xva, y_val.values)],\n",
    "            eval_metric=['accuracy'],\n",
    "            max_epochs=max_epochs, patience=patience,\n",
    "            batch_size=batch, virtual_batch_size=vbatch,\n",
    "            num_workers=0, drop_last=False,\n",
    "            pin_memory=PIN_MEM,  # ✅ 추가\n",
    "        )\n",
    "        # 검증 성능(F1_weighted)\n",
    "        yva_pred = model.predict(Xva).ravel()\n",
    "        f1w = f1_score(y_val.values, yva_pred, average='weighted')\n",
    "        if f1w > best_score:\n",
    "            best_score = f1w; best_params = p; best_model = model\n",
    "            print(\"  -> new best on val F1w:\", round(f1w,4))\n",
    "\n",
    "    train_time = time.time()-t0\n",
    "    # 테스트 성능\n",
    "    yte_pred = best_model.predict(Xte).ravel()\n",
    "    cls_metrics = classification_metrics(y_test.values, yte_pred)\n",
    "\n",
    "    return best_model, best_params, best_score, train_time, dict(y_true=y_test.values, y_pred=yte_pred), cls_metrics\n",
    "\n",
    "# ----------------------------\n",
    "# 4) 실행 오케스트레이션\n",
    "# ----------------------------\n",
    "centers = ['nanji','jungnang','seonam','tancheon']  # 필요 시 추가\n",
    "summary_rows = []  # 센터/과제별 요약 집계\n",
    "\n",
    "for center in centers:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"[{center.upper()}] 파이프라인 시작\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # ===== 회귀: 합계_1일후 =====\n",
    "    pack_reg = load_and_split(center, target_col='합계_1일후', for_task='regression')\n",
    "    reg_model, reg_params, reg_val_best, reg_time, reg_pred, reg_metrics = tune_tabnet_regression(pack_reg)\n",
    "\n",
    "    # 결과/모델 저장\n",
    "    center_dir = os.path.join(OUT_DIR, center)\n",
    "    os.makedirs(center_dir, exist_ok=True)\n",
    "\n",
    "    reg_pickle = {\n",
    "        'task': 'regression',\n",
    "        'model': reg_model,\n",
    "        'params': reg_params,\n",
    "        'metrics': reg_metrics,\n",
    "        'feature_names': pack_reg['feature_names'],\n",
    "        'scalers': pack_reg['scalers'],\n",
    "        'predictions': reg_pred\n",
    "    }\n",
    "    with open(os.path.join(center_dir, f\"{center}_tabnet_reg.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(reg_pickle, f)\n",
    "\n",
    "    pd.DataFrame([{\n",
    "        'center': center, 'task': 'regression', 'val_best_R2': reg_val_best,\n",
    "        'R2': reg_metrics['R2'], 'RMSE': reg_metrics['RMSE'], 'MAE': reg_metrics['MAE'],\n",
    "        'MAPE': reg_metrics['MAPE'], 'SMAPE': reg_metrics['SMAPE'],\n",
    "        'training_time_sec': reg_time, 'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }]).to_csv(os.path.join(center_dir, f\"{center}_tabnet_reg_results.csv\"),\n",
    "               index=False, encoding='utf-8-sig')\n",
    "\n",
    "    summary_rows.append({\n",
    "        'center': center, 'task': 'regression',\n",
    "        **reg_metrics, 'training_time_sec': reg_time\n",
    "    })\n",
    "\n",
    "    print(f\"[{center}] REG done: R2={reg_metrics['R2']:.4f}, RMSE={reg_metrics['RMSE']:.3f}\")\n",
    "\n",
    "    # ===== 분류: 등급_1일후 (0~3 다중분류) =====\n",
    "    pack_cls = load_and_split(center, target_col='등급_1일후', for_task='classification')\n",
    "    cls_model, cls_params, cls_val_best, cls_time, cls_pred, cls_metrics = tune_tabnet_classification(pack_cls, n_classes=4)\n",
    "\n",
    "    # 결과/모델 저장\n",
    "    cls_pickle = {\n",
    "        'task': 'classification',\n",
    "        'model': cls_model,\n",
    "        'params': cls_params,\n",
    "        'metrics': {k:v for k,v in cls_metrics.items() if k!='CM'},\n",
    "        'confusion_matrix': cls_metrics['CM'],\n",
    "        'feature_names': pack_cls['feature_names'],\n",
    "        'scalers': pack_cls['scalers'],\n",
    "        'predictions': cls_pred\n",
    "    }\n",
    "    with open(os.path.join(center_dir, f\"{center}_tabnet_cls.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(cls_pickle, f)\n",
    "\n",
    "    # 혼동행렬은 따로 CSV로 저장\n",
    "    pd.DataFrame(cls_metrics['CM']).to_csv(os.path.join(center_dir, f\"{center}_tabnet_cls_confusion_matrix.csv\"),\n",
    "                                           index=False, encoding='utf-8-sig')\n",
    "\n",
    "    pd.DataFrame([{\n",
    "        'center': center, 'task': 'classification', 'val_best_F1w': cls_val_best,\n",
    "        'Accuracy': cls_metrics['Accuracy'], 'F1_weighted': cls_metrics['F1_weighted'],\n",
    "        'F1_macro': cls_metrics['F1_macro'],\n",
    "        'training_time_sec': cls_time, 'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }]).to_csv(os.path.join(center_dir, f\"{center}_tabnet_cls_results.csv\"),\n",
    "               index=False, encoding='utf-8-sig')\n",
    "\n",
    "    summary_rows.append({\n",
    "        'center': center, 'task': 'classification',\n",
    "        'Accuracy': cls_metrics['Accuracy'],\n",
    "        'F1_weighted': cls_metrics['F1_weighted'],\n",
    "        'F1_macro': cls_metrics['F1_macro'],\n",
    "        'training_time_sec': cls_time\n",
    "    })\n",
    "\n",
    "    print(f\"[{center}] CLS done: Acc={cls_metrics['Accuracy']:.3f}, F1w={cls_metrics['F1_weighted']:.3f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) 전체 요약 저장\n",
    "# ----------------------------\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_path = os.path.join(OUT_DIR, \"multi_center_summary.csv\")\n",
    "summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')\n",
    "print(\"\\n✅ 전체 요약 저장:\", summary_path)\n",
    "\n",
    "# 사람이 보기 좋게 출력\n",
    "print(\"\\n[요약] 회귀 성능(R²) by center\")\n",
    "print(summary_df[summary_df['task']=='regression'][['center','R2','RMSE','MAE','MAPE']])\n",
    "\n",
    "print(\"\\n[요약] 분류 성능(F1_weighted) by center\")\n",
    "print(summary_df[summary_df['task']=='classification'][['center','Accuracy','F1_weighted','F1_macro']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4da75afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] mps, pin_memory=False\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: 환경/라이브러리 & 시드/디바이스\n",
    "import os, time, math, pickle, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    r2_score, mean_squared_error, mean_absolute_error,\n",
    "    f1_score, accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor, TabNetClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 재현성\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# 디바이스\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "PIN_MEM = False if device == 'mps' else True\n",
    "print(f\"[Device] {device}, pin_memory={PIN_MEM}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    try: torch.set_float32_matmul_precision('high')\n",
    "    except: pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16292a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: 경로/하이퍼파라미터/글로벌 설정\n",
    "OUT_DIR = \"../results_tabnet_corrected_v2\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DROP_COLS_BASE = ['날짜','요일','1처리장','2처리장','정화조','중계펌프장','시설현대화','3처리장','4처리장']\n",
    "EXCLUDE_REG = ['합계_1일후','합계_2일후','등급','등급_1일후','등급_2일후','합계']\n",
    "EXCLUDE_CLS = ['합계','합계_2일후','등급','등급_2일후']\n",
    "\n",
    "# 디바이스별 배치/에폭\n",
    "if device == 'mps':\n",
    "    BATCH_REG = 256; VBS_REG = 64; EPOCH_REG = 400\n",
    "    BATCH_CLS = 256; VBS_CLS = 64; EPOCH_CLS = 300\n",
    "elif device == 'cuda':\n",
    "    BATCH_REG = 512; VBS_REG = 128; EPOCH_REG = 600\n",
    "    BATCH_CLS = 512; VBS_CLS = 128; EPOCH_CLS = 400\n",
    "else:\n",
    "    BATCH_REG = 256; VBS_REG = 64; EPOCH_REG = 250\n",
    "    BATCH_CLS = 256; VBS_CLS = 64; EPOCH_CLS = 200\n",
    "\n",
    "# 하이퍼파라미터 그리드(샘플링 예정)\n",
    "grid_reg = {\n",
    "    'n_d': [32, 64],\n",
    "    'n_a': [32, 64],\n",
    "    'n_steps': [4, 5],\n",
    "    'gamma': [1.0, 1.2],\n",
    "    'lambda_sparse': [1e-4, 1e-5],\n",
    "    'learning_rate': [0.001, 0.002, 0.005],\n",
    "    'weight_decay': [1e-5, 1e-6],\n",
    "}\n",
    "grid_cls = {\n",
    "    'n_d': [24, 32, 48],\n",
    "    'n_a': [24, 32, 48],\n",
    "    'n_steps': [3, 4, 5],\n",
    "    'gamma': [1.0, 1.3],\n",
    "    'lambda_sparse': [1e-4, 1e-5],\n",
    "    'learning_rate': [0.001, 0.002, 0.003],\n",
    "    'weight_decay': [1e-5, 1e-6],\n",
    "}\n",
    "\n",
    "def sample_grid(grid, max_trials=20, seed=SEED):\n",
    "    combos = list(ParameterGrid(grid))\n",
    "    random.seed(seed)\n",
    "    if len(combos) > max_trials:\n",
    "        combos = random.sample(combos, max_trials)\n",
    "    return combos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc438a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 유틸 함수\n",
    "def load_and_split(center_name, target_col, for_task=\"regression\"):\n",
    "    path = f\"../data/add_feature/{center_name}_add_feature.csv\"\n",
    "    df = pd.read_csv(path, encoding='utf-8-sig')\n",
    "    df = df.drop([c for c in DROP_COLS_BASE if c in df.columns], axis=1)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"[{center_name}] Target '{target_col}' not found. available={list(df.columns)[:12]}...\")\n",
    "\n",
    "    exclude = EXCLUDE_REG if for_task == \"regression\" else EXCLUDE_CLS\n",
    "    exclude = [c for c in exclude if c in df.columns]\n",
    "\n",
    "    X = df.drop(exclude + [target_col], axis=1, errors='ignore').copy()\n",
    "    y = df[target_col].copy()\n",
    "    if for_task == \"classification\":\n",
    "        y = y.astype(int)\n",
    "\n",
    "    n = len(X); tr_end = int(n*0.7); va_end = int(n*0.9)\n",
    "    X_train, X_val, X_test = X.iloc[:tr_end], X.iloc[tr_end:va_end], X.iloc[va_end:]\n",
    "    y_train, y_val, y_test = y.iloc[:tr_end], y.iloc[tr_end:va_end], y.iloc[va_end:]\n",
    "\n",
    "    x_scaler = StandardScaler()\n",
    "    X_train_s = x_scaler.fit_transform(X_train).astype(np.float32)\n",
    "    X_val_s   = x_scaler.transform(X_val).astype(np.float32)\n",
    "    X_test_s  = x_scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "    if for_task == \"regression\":\n",
    "        y_scaler = StandardScaler()\n",
    "        y_train_s = y_scaler.fit_transform(y_train.values.reshape(-1,1)).astype(np.float32)\n",
    "        y_val_s   = y_scaler.transform(y_val.values.reshape(-1,1)).astype(np.float32)\n",
    "        y_test_s  = y_scaler.transform(y_test.values.reshape(-1,1)).astype(np.float32)\n",
    "        scalers = dict(x=x_scaler, y=y_scaler)\n",
    "        y_pack  = (y_train, y_val, y_test, y_train_s, y_val_s, y_test_s)\n",
    "    else:\n",
    "        scalers = dict(x=x_scaler, y=None)\n",
    "        y_pack  = (y_train, y_val, y_test, None, None, None)\n",
    "\n",
    "    return dict(\n",
    "        X_train_s=X_train_s, X_val_s=X_val_s, X_test_s=X_test_s,\n",
    "        y_pack=y_pack,\n",
    "        feature_names=X.columns.tolist(),\n",
    "        n_samples=n, splits=(len(X_train), len(X_val), len(X_test)),\n",
    "        scalers=scalers\n",
    "    )\n",
    "\n",
    "def regression_metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return dict(\n",
    "        R2=r2_score(y_true, y_pred),\n",
    "        RMSE=rmse,\n",
    "        MAE=mean_absolute_error(y_true, y_pred),\n",
    "        MAPE=np.mean(np.abs((y_true - y_pred)/(y_true + 1e-8)))*100,\n",
    "        SMAPE=np.mean(2*np.abs(y_true - y_pred)/(np.abs(y_true)+np.abs(y_pred)+1e-8))*100,\n",
    "    )\n",
    "\n",
    "def classification_metrics(y_true, y_pred):\n",
    "    return dict(\n",
    "        Accuracy=accuracy_score(y_true, y_pred),\n",
    "        F1_weighted=f1_score(y_true, y_pred, average='weighted'),\n",
    "        F1_macro=f1_score(y_true, y_pred, average='macro'),\n",
    "        CM=confusion_matrix(y_true, y_pred)\n",
    "    )\n",
    "\n",
    "def compute_class_weight(y_train):\n",
    "    vals, cnts = np.unique(y_train, return_counts=True)\n",
    "    freq = cnts / cnts.sum()\n",
    "    weight = {int(k): float(1.0/(f+1e-8)) for k, f in zip(vals, freq)}\n",
    "    m = np.mean(list(weight.values()))\n",
    "    weight = {k: v/m for k,v in weight.items()}\n",
    "    return weight\n",
    "\n",
    "def make_weighted_ce(weight_arr):\n",
    "    w_cpu = torch.tensor(weight_arr, dtype=torch.float32)\n",
    "    def _loss_fn(inputs, targets):\n",
    "        return F.cross_entropy(inputs, targets, weight=w_cpu.to(inputs.device))\n",
    "    return _loss_fn\n",
    "\n",
    "# (선택) 학습 히스토리 파서: dict/History 객체 모두 대응\n",
    "def to_history_dict(h):\n",
    "    if isinstance(h, dict): return h\n",
    "    if hasattr(h, \"history\") and isinstance(h.history, dict): return h.history\n",
    "    try: return dict(h)\n",
    "    except: pass\n",
    "    try:\n",
    "        v = vars(h)\n",
    "        if isinstance(v, dict):\n",
    "            if \"history\" in v and isinstance(v[\"history\"], dict):\n",
    "                return v[\"history\"]\n",
    "            return v\n",
    "    except: pass\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d6fae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: 트레이닝 루틴\n",
    "def tune_tabnet_regression(pack, max_epochs=EPOCH_REG, batch=BATCH_REG, vbatch=VBS_REG, patience=30):\n",
    "    Xtr, Xva, Xte = pack['X_train_s'], pack['X_val_s'], pack['X_test_s']\n",
    "    y_train, y_val, y_test, ytr_s, yva_s, yte_s = pack['y_pack']\n",
    "    y_scaler = pack['scalers']['y']\n",
    "\n",
    "    trials = sample_grid(grid_reg, max_trials=20)\n",
    "    best_score = -np.inf; best_params=None; best_model=None\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i, p in enumerate(trials, 1):\n",
    "        print(f\"[REG] trial {i}/{len(trials)} params={p}\")\n",
    "        model = TabNetRegressor(\n",
    "            n_d=p['n_d'], n_a=p['n_a'], n_steps=p['n_steps'],\n",
    "            gamma=p['gamma'], lambda_sparse=p['lambda_sparse'],\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params=dict(lr=p['learning_rate'], weight_decay=p['weight_decay']),\n",
    "            device_name=device, seed=SEED, verbose=0,\n",
    "            mask_type='entmax'\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train=Xtr, y_train=ytr_s,\n",
    "            eval_set=[(Xva, yva_s)],\n",
    "            eval_metric=['rmse'],\n",
    "            max_epochs=max_epochs, patience=patience,\n",
    "            batch_size=batch, virtual_batch_size=vbatch,\n",
    "            num_workers=0, drop_last=False,\n",
    "            pin_memory=PIN_MEM,\n",
    "        )\n",
    "        yva_pred_s = model.predict(Xva)\n",
    "        yva_pred   = y_scaler.inverse_transform(yva_pred_s).ravel()\n",
    "        val_r2 = r2_score(y_val.values, yva_pred)\n",
    "        if val_r2 > best_score:\n",
    "            best_score = val_r2; best_params = p; best_model = model\n",
    "            print(\"  -> new best on val R²:\", round(val_r2, 4))\n",
    "\n",
    "    train_time = time.time() - t0\n",
    "    yte_pred_s = best_model.predict(Xte)\n",
    "    yte_pred   = y_scaler.inverse_transform(yte_pred_s).ravel()\n",
    "    reg_metrics = regression_metrics(y_test.values, yte_pred)\n",
    "\n",
    "    return best_model, best_params, best_score, train_time, dict(y_true=y_test.values, y_pred=yte_pred), reg_metrics\n",
    "\n",
    "\n",
    "def tune_tabnet_classification(pack, max_epochs=EPOCH_CLS, batch=BATCH_CLS, vbatch=VBS_CLS, patience=30, base_n_classes=4):\n",
    "    Xtr, Xva, Xte = pack['X_train_s'], pack['X_val_s'], pack['X_test_s']\n",
    "    y_train, y_val, y_test, *_ = pack['y_pack']\n",
    "\n",
    "    classes = np.unique(y_train.values.astype(int))\n",
    "    n_classes = max(int(classes.max()) + 1, base_n_classes)\n",
    "\n",
    "    class_weight = compute_class_weight(y_train.values)\n",
    "    weights_list = np.array([class_weight.get(i, 1.0) for i in range(n_classes)], dtype=np.float32)\n",
    "    loss_fn = make_weighted_ce(weights_list)\n",
    "\n",
    "    trials = sample_grid(grid_cls, max_trials=20)\n",
    "    best_score = -np.inf; best_params=None; best_model=None\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i, p in enumerate(trials, 1):\n",
    "        print(f\"[CLS] trial {i}/{len(trials)} params={p}\")\n",
    "        model = TabNetClassifier(\n",
    "            n_d=p['n_d'], n_a=p['n_a'], n_steps=p['n_steps'],\n",
    "            gamma=p['gamma'], lambda_sparse=p['lambda_sparse'],\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params=dict(lr=p['learning_rate'], weight_decay=p['weight_decay']),\n",
    "            device_name=device, seed=SEED, verbose=0,\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train=Xtr, y_train=y_train.values.astype(int),\n",
    "            eval_set=[(Xva, y_val.values.astype(int))],\n",
    "            eval_metric=['accuracy'],\n",
    "            loss_fn=loss_fn,\n",
    "            max_epochs=max_epochs, patience=patience,\n",
    "            batch_size=batch, virtual_batch_size=vbatch,\n",
    "            num_workers=0, drop_last=False,\n",
    "            pin_memory=PIN_MEM,\n",
    "        )\n",
    "        yva_pred = model.predict(Xva).ravel().astype(int)\n",
    "        f1w = f1_score(y_val.values.astype(int), yva_pred, average='weighted')\n",
    "        if f1w > best_score:\n",
    "            best_score = f1w; best_params = p; best_model = model\n",
    "            print(\"  -> new best on val F1w:\", round(f1w, 4))\n",
    "\n",
    "    train_time = time.time()-t0\n",
    "    yte_pred = best_model.predict(Xte).ravel().astype(int)\n",
    "    cls_metrics = classification_metrics(y_test.values.astype(int), yte_pred)\n",
    "\n",
    "    return best_model, best_params, best_score, train_time, dict(y_true=y_test.values, y_pred=yte_pred), cls_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3af1230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      "[NANJI] 파이프라인 시작\n",
      "===========================================================================\n",
      "[REG] trial 1/20 params={'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 143 with best_epoch = 113 and best_val_0_rmse = 0.8691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> new best on val R²: 0.53\n",
      "[REG] trial 2/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_steps': 4, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 91 with best_epoch = 61 and best_val_0_rmse = 0.91679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 3/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 166 with best_epoch = 136 and best_val_0_rmse = 0.87802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 4/20 params={'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_steps': 4, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 68 with best_epoch = 38 and best_val_0_rmse = 0.88103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 5/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 83 with best_epoch = 53 and best_val_0_rmse = 0.93002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 6/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 84 with best_epoch = 54 and best_val_0_rmse = 1.06853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 7/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_steps': 4, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 117 with best_epoch = 87 and best_val_0_rmse = 0.87629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 8/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 119 with best_epoch = 89 and best_val_0_rmse = 0.80241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> new best on val R²: 0.5993\n",
      "[REG] trial 9/20 params={'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_steps': 4, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 80 with best_epoch = 50 and best_val_0_rmse = 0.93543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 10/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 84 with best_epoch = 54 and best_val_0_rmse = 0.89043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 11/20 params={'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_steps': 4, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 75 with best_epoch = 45 and best_val_0_rmse = 0.85593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 12/20 params={'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 106 with best_epoch = 76 and best_val_0_rmse = 0.94747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 13/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 106 with best_epoch = 76 and best_val_0_rmse = 0.89172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 14/20 params={'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 169 with best_epoch = 139 and best_val_0_rmse = 0.8731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 15/20 params={'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_steps': 4, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 93 with best_epoch = 63 and best_val_0_rmse = 0.9327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 16/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_steps': 4, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 119 with best_epoch = 89 and best_val_0_rmse = 0.899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 17/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 96 with best_epoch = 66 and best_val_0_rmse = 1.08533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 18/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 94 with best_epoch = 64 and best_val_0_rmse = 0.86444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 19/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 108 with best_epoch = 78 and best_val_0_rmse = 0.98238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 20/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 183 with best_epoch = 153 and best_val_0_rmse = 0.88477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nanji] REG done: R2=0.4363, RMSE=70899.696\n",
      "[CLS] trial 1/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.003, 'n_a': 24, 'n_d': 32, 'n_steps': 3, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 138 with best_epoch = 108 and best_val_0_accuracy = 0.97231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> new best on val F1w: 0.9723\n",
      "[CLS] trial 2/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 32, 'n_steps': 3, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 158 with best_epoch = 128 and best_val_0_accuracy = 0.88599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 3/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.003, 'n_a': 24, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 94 with best_epoch = 64 and best_val_0_accuracy = 0.9658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 4/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 48, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 112 with best_epoch = 82 and best_val_0_accuracy = 0.93485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 5/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 24, 'n_d': 48, 'n_steps': 3, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 108 with best_epoch = 78 and best_val_0_accuracy = 0.92997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 6/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.003, 'n_a': 32, 'n_d': 48, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 96 with best_epoch = 66 and best_val_0_accuracy = 0.87459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 7/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 48, 'n_d': 48, 'n_steps': 4, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 74 with best_epoch = 44 and best_val_0_accuracy = 0.83225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 8/20 params={'gamma': 1.3, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 24, 'n_steps': 3, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 74 with best_epoch = 44 and best_val_0_accuracy = 0.92997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 9/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 48, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 83 with best_epoch = 53 and best_val_0_accuracy = 0.76221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 10/20 params={'gamma': 1.3, 'lambda_sparse': 1e-05, 'learning_rate': 0.003, 'n_a': 24, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 127 with best_epoch = 97 and best_val_0_accuracy = 0.96417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 11/20 params={'gamma': 1.3, 'lambda_sparse': 0.0001, 'learning_rate': 0.003, 'n_a': 24, 'n_d': 24, 'n_steps': 3, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 94 with best_epoch = 64 and best_val_0_accuracy = 0.89414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 12/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 48, 'n_steps': 4, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 90 with best_epoch = 60 and best_val_0_accuracy = 0.66938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 13/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 48, 'n_steps': 3, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 76 with best_epoch = 46 and best_val_0_accuracy = 0.6987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 14/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 48, 'n_d': 24, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 232 with best_epoch = 202 and best_val_0_accuracy = 0.94625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 15/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 24, 'n_d': 32, 'n_steps': 3, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 125 with best_epoch = 95 and best_val_0_accuracy = 0.91857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 16/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 24, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 157 with best_epoch = 127 and best_val_0_accuracy = 0.96254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 17/20 params={'gamma': 1.3, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 48, 'n_steps': 3, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 184 with best_epoch = 154 and best_val_0_accuracy = 0.81922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 18/20 params={'gamma': 1.3, 'lambda_sparse': 1e-05, 'learning_rate': 0.003, 'n_a': 32, 'n_d': 24, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 143 with best_epoch = 113 and best_val_0_accuracy = 0.96743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 19/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 32, 'n_steps': 4, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 147 with best_epoch = 117 and best_val_0_accuracy = 0.84365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 20/20 params={'gamma': 1.3, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 48, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 137 with best_epoch = 107 and best_val_0_accuracy = 0.91857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nanji] ❌ 실패: Can't pickle local object 'make_weighted_ce.<locals>._loss_fn'\n",
      "\n",
      "===========================================================================\n",
      "[JUNGNANG] 파이프라인 시작\n",
      "===========================================================================\n",
      "[REG] trial 1/20 params={'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 101 with best_epoch = 71 and best_val_0_rmse = 0.68488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> new best on val R²: 0.3768\n",
      "[REG] trial 2/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_steps': 4, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 119 with best_epoch = 89 and best_val_0_rmse = 0.67852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> new best on val R²: 0.3883\n",
      "[REG] trial 3/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 116 with best_epoch = 86 and best_val_0_rmse = 0.65924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> new best on val R²: 0.4226\n",
      "[REG] trial 4/20 params={'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_steps': 4, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 64 with best_epoch = 34 and best_val_0_rmse = 0.67783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 5/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 120 with best_epoch = 90 and best_val_0_rmse = 0.6229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> new best on val R²: 0.4845\n",
      "[REG] trial 6/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 114 with best_epoch = 84 and best_val_0_rmse = 0.69966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 7/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_steps': 4, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 92 with best_epoch = 62 and best_val_0_rmse = 0.71737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 8/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 32, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 104 with best_epoch = 74 and best_val_0_rmse = 0.63508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 9/20 params={'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 64, 'n_steps': 4, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 77 with best_epoch = 47 and best_val_0_rmse = 0.6494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 10/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 112 with best_epoch = 82 and best_val_0_rmse = 0.66927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 11/20 params={'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 64, 'n_d': 64, 'n_steps': 4, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 83 with best_epoch = 53 and best_val_0_rmse = 0.68356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 12/20 params={'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.005, 'n_a': 64, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 78 with best_epoch = 48 and best_val_0_rmse = 0.64656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 13/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 91 with best_epoch = 61 and best_val_0_rmse = 0.64706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 14/20 params={'gamma': 1.2, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 121 with best_epoch = 91 and best_val_0_rmse = 0.66652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 15/20 params={'gamma': 1.2, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 64, 'n_steps': 4, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 163 with best_epoch = 133 and best_val_0_rmse = 0.64356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 16/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_steps': 4, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 150 with best_epoch = 120 and best_val_0_rmse = 0.67932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 17/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 199 with best_epoch = 169 and best_val_0_rmse = 0.64259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 18/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 114 with best_epoch = 84 and best_val_0_rmse = 0.67244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 19/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 64, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 109 with best_epoch = 79 and best_val_0_rmse = 0.69664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] trial 20/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.001, 'n_a': 64, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 96 with best_epoch = 66 and best_val_0_rmse = 0.72167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jungnang] REG done: R2=-0.0676, RMSE=152933.831\n",
      "[CLS] trial 1/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.003, 'n_a': 24, 'n_d': 32, 'n_steps': 3, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 176 with best_epoch = 146 and best_val_0_accuracy = 0.96906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> new best on val F1w: 0.9692\n",
      "[CLS] trial 2/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 32, 'n_steps': 3, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 199 with best_epoch = 169 and best_val_0_accuracy = 0.95114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 3/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.003, 'n_a': 24, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 102 with best_epoch = 72 and best_val_0_accuracy = 0.94463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 4/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 48, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 119 with best_epoch = 89 and best_val_0_accuracy = 0.90879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 5/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 24, 'n_d': 48, 'n_steps': 3, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 142 with best_epoch = 112 and best_val_0_accuracy = 0.94463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 6/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.003, 'n_a': 32, 'n_d': 48, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 81 with best_epoch = 51 and best_val_0_accuracy = 0.93485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 7/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 48, 'n_d': 48, 'n_steps': 4, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 59 with best_epoch = 29 and best_val_0_accuracy = 0.80293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 8/20 params={'gamma': 1.3, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 24, 'n_steps': 3, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 79 with best_epoch = 49 and best_val_0_accuracy = 0.92671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 9/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 32, 'n_d': 48, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 137 with best_epoch = 107 and best_val_0_accuracy = 0.94951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 10/20 params={'gamma': 1.3, 'lambda_sparse': 1e-05, 'learning_rate': 0.003, 'n_a': 24, 'n_d': 32, 'n_steps': 5, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 62 with best_epoch = 32 and best_val_0_accuracy = 0.8355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 11/20 params={'gamma': 1.3, 'lambda_sparse': 0.0001, 'learning_rate': 0.003, 'n_a': 24, 'n_d': 24, 'n_steps': 3, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 103 with best_epoch = 73 and best_val_0_accuracy = 0.93485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 12/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 48, 'n_steps': 4, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 94 with best_epoch = 64 and best_val_0_accuracy = 0.64984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 13/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.001, 'n_a': 32, 'n_d': 48, 'n_steps': 3, 'weight_decay': 1e-05}\n",
      "\n",
      "Early stopping occurred at epoch 117 with best_epoch = 87 and best_val_0_accuracy = 0.85831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 14/20 params={'gamma': 1.0, 'lambda_sparse': 0.0001, 'learning_rate': 0.002, 'n_a': 48, 'n_d': 24, 'n_steps': 5, 'weight_decay': 1e-06}\n",
      "\n",
      "Early stopping occurred at epoch 137 with best_epoch = 107 and best_val_0_accuracy = 0.94788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] trial 15/20 params={'gamma': 1.0, 'lambda_sparse': 1e-05, 'learning_rate': 0.002, 'n_a': 24, 'n_d': 32, 'n_steps': 3, 'weight_decay': 1e-06}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# 분류\u001b[39;00m\n\u001b[1;32m     33\u001b[0m pack_cls \u001b[38;5;241m=\u001b[39m load_and_split(center, target_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m등급_1일후\u001b[39m\u001b[38;5;124m'\u001b[39m, for_task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m cls_model, cls_params, cls_val_best, cls_time, cls_pred, cls_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtune_tabnet_classification\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpack_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCH_CLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_CLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVBS_CLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_n_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\n\u001b[1;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(center_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcenter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_tabnet_cls.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     38\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump({\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: cls_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: cls_params,\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m: {k:v \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m cls_metrics\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCM\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscalers\u001b[39m\u001b[38;5;124m'\u001b[39m: pack_cls[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscalers\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m: cls_pred\n\u001b[1;32m     44\u001b[0m     }, f)\n",
      "Cell \u001b[0;32mIn[9], line 69\u001b[0m, in \u001b[0;36mtune_tabnet_classification\u001b[0;34m(pack, max_epochs, batch, vbatch, patience, base_n_classes)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CLS] trial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(trials)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m params=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m model \u001b[38;5;241m=\u001b[39m TabNetClassifier(\n\u001b[1;32m     63\u001b[0m     n_d\u001b[38;5;241m=\u001b[39mp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_d\u001b[39m\u001b[38;5;124m'\u001b[39m], n_a\u001b[38;5;241m=\u001b[39mp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_a\u001b[39m\u001b[38;5;124m'\u001b[39m], n_steps\u001b[38;5;241m=\u001b[39mp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_steps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     64\u001b[0m     gamma\u001b[38;5;241m=\u001b[39mp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m], lambda_sparse\u001b[38;5;241m=\u001b[39mp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda_sparse\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m     device_name\u001b[38;5;241m=\u001b[39mdevice, seed\u001b[38;5;241m=\u001b[39mSEED, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     68\u001b[0m )\n\u001b[0;32m---> 69\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXva\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIN_MEM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m yva_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(Xva)\u001b[38;5;241m.\u001b[39mravel()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     80\u001b[0m f1w \u001b[38;5;241m=\u001b[39m f1_score(y_val\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m), yva_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:258\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs):\n\u001b[1;32m    254\u001b[0m \n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch_idx)\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m eval_name, valid_dataloader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(eval_names, valid_dataloaders):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:486\u001b[0m, in \u001b[0;36mTabModel._train_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;124;03mTrains one epoch of the network in self.network\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m    DataLoader with train set\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 486\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_batch_begin(batch_idx)\n\u001b[1;32m    489\u001b[0m     batch_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch(X, y)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/torch/utils/data/dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    740\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/torch/utils/data/dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:285\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/youngwon/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 5: 오케스트레이션 실행\n",
    "centers = ['nanji','jungnang','seonam','tancheon']\n",
    "summary_rows = []\n",
    "\n",
    "for center in centers:\n",
    "    print(\"\\n\" + \"=\"*75)\n",
    "    print(f\"[{center.upper()}] 파이프라인 시작\")\n",
    "    print(\"=\"*75)\n",
    "\n",
    "    try:\n",
    "        # 회귀\n",
    "        pack_reg = load_and_split(center, target_col='합계_1일후', for_task='regression')\n",
    "        reg_model, reg_params, reg_val_best, reg_time, reg_pred, reg_metrics = tune_tabnet_regression(\n",
    "            pack_reg, max_epochs=EPOCH_REG, batch=BATCH_REG, vbatch=VBS_REG, patience=30\n",
    "        )\n",
    "        center_dir = os.path.join(OUT_DIR, center); os.makedirs(center_dir, exist_ok=True)\n",
    "        with open(os.path.join(center_dir, f\"{center}_tabnet_reg.pkl\"), \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                'task': 'regression', 'model': reg_model, 'params': reg_params,\n",
    "                'metrics': reg_metrics, 'feature_names': pack_reg['feature_names'],\n",
    "                'scalers': pack_reg['scalers'], 'predictions': reg_pred\n",
    "            }, f)\n",
    "        pd.DataFrame([{\n",
    "            'center': center, 'task': 'regression', 'val_best_R2': reg_val_best,\n",
    "            'R2': reg_metrics['R2'], 'RMSE': reg_metrics['RMSE'], 'MAE': reg_metrics['MAE'],\n",
    "            'MAPE': reg_metrics['MAPE'], 'SMAPE': reg_metrics['SMAPE'],\n",
    "            'training_time_sec': reg_time, 'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }]).to_csv(os.path.join(center_dir, f\"{center}_tabnet_reg_results.csv\"), index=False, encoding='utf-8-sig')\n",
    "        summary_rows.append({'center': center, 'task':'regression', **reg_metrics, 'training_time_sec': reg_time})\n",
    "        print(f\"[{center}] REG done: R2={reg_metrics['R2']:.4f}, RMSE={reg_metrics['RMSE']:.3f}\")\n",
    "\n",
    "        # 분류\n",
    "        pack_cls = load_and_split(center, target_col='등급_1일후', for_task='classification')\n",
    "        cls_model, cls_params, cls_val_best, cls_time, cls_pred, cls_metrics = tune_tabnet_classification(\n",
    "            pack_cls, max_epochs=EPOCH_CLS, batch=BATCH_CLS, vbatch=VBS_CLS, patience=30, base_n_classes=4\n",
    "        )\n",
    "        with open(os.path.join(center_dir, f\"{center}_tabnet_cls.pkl\"), \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                'task': 'classification', 'model': cls_model, 'params': cls_params,\n",
    "                'metrics': {k:v for k,v in cls_metrics.items() if k!='CM'},\n",
    "                'confusion_matrix': cls_metrics['CM'],\n",
    "                'feature_names': pack_cls['feature_names'],\n",
    "                'scalers': pack_cls['scalers'], 'predictions': cls_pred\n",
    "            }, f)\n",
    "        cm_df = pd.DataFrame(cls_metrics['CM'],\n",
    "                             index=[f\"true_{i}\" for i in range(cls_metrics['CM'].shape[0])],\n",
    "                             columns=[f\"pred_{i}\" for i in range(cls_metrics['CM'].shape[1])])\n",
    "        cm_df.to_csv(os.path.join(center_dir, f\"{center}_tabnet_cls_confusion_matrix.csv\"), index=True, encoding='utf-8-sig')\n",
    "        pd.DataFrame([{\n",
    "            'center': center, 'task': 'classification', 'val_best_F1w': cls_val_best,\n",
    "            'Accuracy': cls_metrics['Accuracy'], 'F1_weighted': cls_metrics['F1_weighted'],\n",
    "            'F1_macro': cls_metrics['F1_macro'],\n",
    "            'training_time_sec': cls_time, 'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }]).to_csv(os.path.join(center_dir, f\"{center}_tabnet_cls_results.csv\"), index=False, encoding='utf-8-sig')\n",
    "        summary_rows.append({'center': center, 'task':'classification',\n",
    "                             'Accuracy': cls_metrics['Accuracy'],\n",
    "                             'F1_weighted': cls_metrics['F1_weighted'],\n",
    "                             'F1_macro': cls_metrics['F1_macro'],\n",
    "                             'training_time_sec': cls_time})\n",
    "        print(f\"[{center}] CLS done: Acc={cls_metrics['Accuracy']:.3f}, F1w={cls_metrics['F1_weighted']:.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{center}] ❌ 실패: {e}\")\n",
    "        continue\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_path = os.path.join(OUT_DIR, \"multi_center_summary.csv\")\n",
    "summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')\n",
    "print(\"\\n✅ 전체 요약 저장:\", summary_path)\n",
    "\n",
    "# 확인 출력\n",
    "print(\"\\n[요약] 회귀 성능(R²) by center\")\n",
    "if (summary_df['task']=='regression').any():\n",
    "    display(summary_df[summary_df['task']=='regression'][['center','R2','RMSE','MAE','MAPE']])\n",
    "\n",
    "print(\"\\n[요약] 분류 성능(F1_weighted) by center\")\n",
    "if (summary_df['task']=='classification').any():\n",
    "    display(summary_df[summary_df['task']=='classification'][['center','Accuracy','F1_weighted','F1_macro']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32026804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: 회귀 결과 시각화 (센터 1개 선택)\n",
    "center_to_plot = 'nanji'  # 변경 가능\n",
    "center_dir = os.path.join(OUT_DIR, center_to_plot)\n",
    "\n",
    "with open(os.path.join(center_dir, f\"{center_to_plot}_tabnet_reg.pkl\"), \"rb\") as f:\n",
    "    reg_obj = pickle.load(f)\n",
    "\n",
    "y_true = reg_obj['predictions']['y_true']\n",
    "y_pred = reg_obj['predictions']['y_pred']\n",
    "metrics = reg_obj['metrics']\n",
    "feature_names = reg_obj['feature_names']\n",
    "tabnet_reg = reg_obj['model']\n",
    "\n",
    "print(f\"[{center_to_plot.upper()}][REG] R2={metrics['R2']:.4f}, RMSE={metrics['RMSE']:.1f}, MAE={metrics['MAE']:.1f}\")\n",
    "\n",
    "# 1) 예측 vs 실제\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_true, y_pred, alpha=0.6)\n",
    "mn, mx = min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())\n",
    "plt.plot([mn,mx],[mn,mx],'--')\n",
    "plt.title(f\"{center_to_plot.upper()} 예측 vs 실제 (R²={metrics['R2']:.3f})\")\n",
    "plt.xlabel(\"실제값\"); plt.ylabel(\"예측값\"); plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 2) 잔차\n",
    "resid = y_true - y_pred\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(y_pred, resid, alpha=0.6)\n",
    "plt.axhline(0, ls='--', c='r')\n",
    "plt.title(\"잔차 플롯\"); plt.xlabel(\"예측값\"); plt.ylabel(\"잔차\"); plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 3) 최근 구간 시계열(뒤 50개)\n",
    "n_show = min(50, len(y_true))\n",
    "idx = np.arange(len(y_true))[-n_show:]\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(idx, y_true[-n_show:], label='실제')\n",
    "plt.plot(idx, y_pred[-n_show:], ls='--', label='예측')\n",
    "plt.fill_between(idx, y_true[-n_show:], y_pred[-n_show:], alpha=0.2)\n",
    "plt.title(f\"시계열 결과 (최근 {n_show}개)\")\n",
    "plt.xlabel(\"시점\"); plt.ylabel(\"값\"); plt.legend(); plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 4) Feature Importance (Top 15)\n",
    "if hasattr(tabnet_reg, 'feature_importances_'):\n",
    "    fi = tabnet_reg.feature_importances_\n",
    "    fi_df = pd.DataFrame({'Feature': feature_names, 'Importance': fi}).sort_values('Importance', ascending=False).head(15)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.barh(range(len(fi_df)), fi_df['Importance'])\n",
    "    plt.yticks(range(len(fi_df)), fi_df['Feature'])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(\"Feature Importance (Top 15)\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1af454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: 학습 곡선 (회귀 모델 히스토리)\n",
    "hist = getattr(tabnet_reg, 'history', None)\n",
    "if hist is None:\n",
    "    print(\"history 없음\")\n",
    "else:\n",
    "    H = to_history_dict(hist)\n",
    "    print(\"history keys:\", list(H.keys()))\n",
    "\n",
    "    # 키 후보\n",
    "    loss_key_candidates = [\"loss\",\"train_loss\",\"training_loss\",\"loss_epoch\",\"train_loss_epoch\"]\n",
    "    val_key_candidates  = [\"val_loss\",\"val_0_rmse\",\"valid_0_rmse\",\"validation_0_rmse\",\"valid_rmse\",\"valid_loss\"]\n",
    "\n",
    "    def pick_first(d, cands):\n",
    "        for k in cands:\n",
    "            if k in d: return k\n",
    "        return None\n",
    "\n",
    "    lk, vk = pick_first(H, loss_key_candidates), pick_first(H, val_key_candidates)\n",
    "    if lk is None and vk is None:\n",
    "        print(\"usable loss/val keys not found\")\n",
    "    else:\n",
    "        plt.figure(figsize=(7,4))\n",
    "        if lk is not None: plt.plot(H[lk], label='train')\n",
    "        if vk is not None: plt.plot(H[vk], label='valid')\n",
    "        plt.title(\"학습 곡선\"); plt.xlabel(\"epoch\"); plt.ylabel(\"loss/metric\"); plt.legend(); plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(7,4))\n",
    "        if lk is not None: plt.semilogy(H[lk], label='train')\n",
    "        if vk is not None: plt.semilogy(H[vk], label='valid')\n",
    "        plt.title(\"학습 곡선 (log)\"); plt.xlabel(\"epoch\"); plt.ylabel(\"loss/metric (log)\"); plt.legend(); plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "        if hasattr(tabnet_reg, 'best_epoch'):\n",
    "            print(\"best_epoch:\", tabnet_reg.best_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61943d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: 분류 결과 시각화 (센터 1개 선택)\n",
    "with open(os.path.join(center_dir, f\"{center_to_plot}_tabnet_cls.pkl\"), \"rb\") as f:\n",
    "    cls_obj = pickle.load(f)\n",
    "\n",
    "y_true_cls = cls_obj['predictions']['y_true'].astype(int)\n",
    "y_pred_cls = cls_obj['predictions']['y_pred'].astype(int)\n",
    "m_cls = cls_obj['metrics']\n",
    "cm = cls_obj['confusion_matrix']\n",
    "\n",
    "print(f\"[{center_to_plot.upper()}][CLS] Acc={m_cls['Accuracy']:.3f}, F1w={m_cls['F1_weighted']:.3f}, F1m={m_cls['F1_macro']:.3f}\")\n",
    "\n",
    "# 혼동행렬\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title(\"Confusion Matrix\"); plt.xlabel(\"Pred\"); plt.ylabel(\"True\")\n",
    "plt.colorbar(); plt.xticks(range(cm.shape[1])); plt.yticks(range(cm.shape[0]))\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 클래스별 PRF(옵션)\n",
    "prec, rec, f1, support = precision_recall_fscore_support(y_true_cls, y_pred_cls, labels=np.unique(y_true_cls), zero_division=0)\n",
    "cls_table = pd.DataFrame({\n",
    "    'class': np.unique(y_true_cls),\n",
    "    'precision': prec, 'recall': rec, 'f1': f1, 'support': support\n",
    "})\n",
    "display(cls_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f067930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: 전체 요약 시각화\n",
    "summary_df = pd.read_csv(os.path.join(OUT_DIR, \"multi_center_summary.csv\"), encoding='utf-8-sig')\n",
    "\n",
    "# 회귀 R²\n",
    "reg_df = summary_df[summary_df['task']=='regression'].copy()\n",
    "if not reg_df.empty:\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.bar(reg_df['center'], reg_df['R2'])\n",
    "    plt.title(\"센터별 회귀 R²\"); plt.xlabel(\"center\"); plt.ylabel(\"R²\"); plt.ylim(0, 1)\n",
    "    for x,y in zip(reg_df['center'], reg_df['R2']):\n",
    "        plt.text(x, y+0.01, f\"{y:.2f}\", ha='center')\n",
    "    plt.show()\n",
    "\n",
    "# 분류 F1-weighted\n",
    "cls_df = summary_df[summary_df['task']=='classification'].copy()\n",
    "if not cls_df.empty:\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.bar(cls_df['center'], cls_df['F1_weighted'])\n",
    "    plt.title(\"센터별 분류 F1-weighted\"); plt.xlabel(\"center\"); plt.ylabel(\"F1-weighted\"); plt.ylim(0, 1)\n",
    "    for x,y in zip(cls_df['center'], cls_df['F1_weighted']):\n",
    "        plt.text(x, y+0.01, f\"{y:.2f}\", ha='center')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbe02ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youngwon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
