{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ebb1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필수 라이브러리 import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import time, os, json, warnings\n",
    "import joblib\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import gc\n",
    "\n",
    "# 선택적 라이브러리 import\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGB = True\n",
    "except ImportError:\n",
    "    HAS_LGB = False\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    HAS_CATBOOST = True\n",
    "except ImportError:\n",
    "    HAS_CATBOOST = False\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    HAS_SHAP = True\n",
    "except ImportError:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "# 한글 폰트 설정\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "try:\n",
    "    plt.rcParams['font.family'] = 'AppleGothic' # 맥\n",
    "except Exception:\n",
    "    plt.rcParams['font.family'] ='Malgun Gothic' # 윈도우\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"하수처리장 예측 파이프라인 초기화 완료\")\n",
    "print(f\"XGBoost: {'✓' if HAS_XGB else '✗'}\")\n",
    "print(f\"LightGBM: {'✓' if HAS_LGB else '✗'}\")\n",
    "print(f\"CatBoost: {'✓' if HAS_CATBOOST else '✗'}\")\n",
    "print(f\"SHAP: {'✓' if HAS_SHAP else '✗'}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 각 SHAP 분석 후 메모리 정리\n",
    "import gc\n",
    "plt.close('all')  # 모든 matplotlib 그래프 닫기\n",
    "gc.collect()      # 가비지 컬렉션 강제 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8cd064",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ================================================================================================\n",
    "# 완전한 하수처리장 예측 파이프라인 (학습-저장-예측)\n",
    "# ================================================================================================\n",
    "\n",
    "# ================================================================================================\n",
    "# 1. 모델 정의 함수들\n",
    "# ================================================================================================\n",
    "def build_regression_models():\n",
    "    \"\"\"회귀 모델들\"\"\"\n",
    "    models = {}\n",
    "    models[\"RandomForest_Reg\"] = RandomForestRegressor(\n",
    "        n_estimators=300, min_samples_leaf=2, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    models[\"LinearRegression\"] = LinearRegression()\n",
    "    models[\"GradientBoosting_Reg\"] = GradientBoostingRegressor(\n",
    "        n_estimators=200, learning_rate=0.1, random_state=42\n",
    "    )\n",
    "    if HAS_XGB:\n",
    "        models[\"XGBoost_Reg\"] = xgb.XGBRegressor(\n",
    "            n_estimators=400, max_depth=5, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            random_state=42, n_jobs=-1, verbosity=0\n",
    "        )\n",
    "    if HAS_LGB:\n",
    "        models[\"LightGBM_Reg\"] = lgb.LGBMRegressor(\n",
    "            n_estimators=500, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            random_state=42, n_jobs=-1, verbosity=-1\n",
    "        )\n",
    "    if HAS_CATBOOST:\n",
    "        models[\"CatBoost_Reg\"] = cb.CatBoostRegressor(\n",
    "            iterations=500, learning_rate=0.05, depth=6,\n",
    "            random_state=42, verbose=False\n",
    "        )\n",
    "    return models\n",
    "\n",
    "def build_classification_models():\n",
    "    \"\"\"분류 모델들 (4등급)\"\"\"\n",
    "    models = {}\n",
    "    models[\"RandomForest_Clf\"] = RandomForestClassifier(\n",
    "        n_estimators=300, min_samples_leaf=2, random_state=42, \n",
    "        n_jobs=-1, class_weight='balanced'\n",
    "    ) \n",
    "    models[\"GradientBoosting_Clf\"] = GradientBoostingClassifier(\n",
    "        n_estimators=200, learning_rate=0.1, random_state=42\n",
    "    )\n",
    "    models[\"LogisticRegression_Clf\"] = LogisticRegression(\n",
    "        multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=1000,\n",
    "        random_state=42, class_weight='balanced'\n",
    "    )\n",
    "    if HAS_XGB:\n",
    "        models[\"XGBoost_Clf\"] = xgb.XGBClassifier(\n",
    "            n_estimators=400, max_depth=5, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            objective=\"multi:softprob\", num_class=4,\n",
    "            tree_method=\"hist\", random_state=42, n_jobs=-1, verbosity=0\n",
    "        )\n",
    "    if HAS_LGB:\n",
    "        models[\"LightGBM_Clf\"] = lgb.LGBMClassifier(\n",
    "            n_estimators=500, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            objective=\"multiclass\", num_class=4,\n",
    "            random_state=42, n_jobs=-1, verbosity=-1, is_unbalance=True\n",
    "        )\n",
    "    if HAS_CATBOOST:\n",
    "        models[\"CatBoost_Clf\"] = cb.CatBoostClassifier(\n",
    "            iterations=500, learning_rate=0.05, depth=6,\n",
    "            random_state=42, verbose=False, auto_class_weights='Balanced'\n",
    "        )\n",
    "    return models\n",
    "\n",
    "# ================================================================================================\n",
    "# 2. 데이터 처리 함수들\n",
    "# ================================================================================================\n",
    "def make_pipeline_unified(model, model_name, model_type):\n",
    "    \"\"\"통합 전처리 파이프라인\"\"\"\n",
    "    if model_name in [\"LinearRegression\", \"LogisticRegression_Clf\"]:\n",
    "        pre = Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ])\n",
    "    else:\n",
    "        pre = Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        ])\n",
    "    return Pipeline(steps=[(\"pre\", pre), (\"model\", model)])\n",
    "\n",
    "def prepare_data_stratified(df, target_col, model_type, test_size=0.2, split_method='stratified'):\n",
    "    \"\"\"데이터 준비 - Stratified vs 시계열 분할\"\"\"\n",
    "    work = df.sort_values('날짜').reset_index(drop=True).copy()\n",
    "    dates = pd.to_datetime(work['날짜'])\n",
    "\n",
    "    not_use_col = [\n",
    "        '날짜',\n",
    "        '1처리장','2처리장','정화조','중계펌프장','합계','시설현대화',\n",
    "        '3처리장','4처리장','합계', '합계_1일후','합계_2일후',\n",
    "        '등급','등급_1일후','등급_2일후'\n",
    "    ]\n",
    "    \n",
    "    drop_cols = [c for c in (set(not_use_col) | {target_col}) if c in work.columns]\n",
    "    X_raw = work.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    \n",
    "    for c in X_raw.columns:\n",
    "        X_raw[c] = pd.to_numeric(X_raw[c], errors=\"coerce\")\n",
    "\n",
    "    if model_type == \"regression\":\n",
    "        y = pd.to_numeric(work[target_col], errors=\"coerce\")\n",
    "    else:\n",
    "        y = work[target_col].astype(\"int64\")\n",
    "\n",
    "    valid_idx = (~X_raw.isnull().all(axis=1)) & (~pd.isnull(y))\n",
    "    X_raw = X_raw[valid_idx].reset_index(drop=True)\n",
    "    y = y[valid_idx].reset_index(drop=True)\n",
    "    dates = dates[valid_idx].reset_index(drop=True)\n",
    "    \n",
    "    if split_method == 'stratified':\n",
    "        if model_type == \"classification\":\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "            train_idx, test_idx = next(sss.split(X_raw, y))\n",
    "        else:\n",
    "            train_idx, test_idx = train_test_split(\n",
    "                range(len(X_raw)), test_size=test_size, random_state=42\n",
    "            )\n",
    "            \n",
    "        X_train, X_test = X_raw.iloc[train_idx].copy(), X_raw.iloc[test_idx].copy()\n",
    "        y_train, y_test = y.iloc[train_idx].copy(), y.iloc[test_idx].copy()\n",
    "        dates_train, dates_test = dates.iloc[train_idx].copy(), dates.iloc[test_idx].copy()\n",
    "        \n",
    "    else:  # temporal split\n",
    "        n = len(X_raw)\n",
    "        split = int(n * (1 - test_size))\n",
    "        X_train, X_test = X_raw.iloc[:split].copy(), X_raw.iloc[split:].copy()\n",
    "        y_train, y_test = y.iloc[:split].copy(), y.iloc[split:].copy()\n",
    "        dates_train, dates_test = dates.iloc[:split].copy(), dates.iloc[split:].copy()\n",
    "\n",
    "    feature_names = list(X_raw.columns)\n",
    "    return X_train, X_test, y_train, y_test, feature_names, dates_train, dates_test\n",
    "\n",
    "def make_features(df, cutoff_date=None):\n",
    "    \"\"\"파생변수 생성 함수 - Data Leakage 방지 버전\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['날짜'] = pd.to_datetime(df['날짜'])\n",
    "    df = df.sort_values('날짜').reset_index(drop=True)\n",
    "    \n",
    "    # 디버깅 정보 - 시작 시점\n",
    "    print(f\"\\n=== make_features 디버깅 ===\")\n",
    "    print(f\"입력 데이터: {len(df)}행\")\n",
    "    print(f\"날짜 범위: {df['날짜'].min()} ~ {df['날짜'].max()}\")\n",
    "    if cutoff_date is not None:\n",
    "        cutoff = pd.to_datetime(cutoff_date)\n",
    "        print(f\"cutoff_date: {cutoff_date}\")\n",
    "        print(f\"  cutoff 이전 데이터: {len(df[df['날짜'] <= cutoff])}행\")\n",
    "        print(f\"  cutoff 이후 데이터: {len(df[df['날짜'] > cutoff])}행\")\n",
    "\n",
    "    df['월'] = df['날짜'].dt.month\n",
    "    df['요일'] = df['날짜'].dt.weekday\n",
    "\n",
    "    season_map = {'봄': 0, '여름': 1, '가을': 2, '겨울': 3}\n",
    "    discomfort_map = {'쾌적': 0, '약간 불쾌': 1, '불쾌': 2, '매우 불쾌': 3, '극심한 불쾌': 4}\n",
    "    df['계절'] = df['계절'].map(season_map).astype('Int64')\n",
    "    df['불쾌지수등급'] = df['불쾌지수등급'].map(discomfort_map).astype('Int64')\n",
    "\n",
    "    # 강수량 시차 피처\n",
    "    df['강수량_1일전'] = df['일_일강수량(mm)'].shift(1)\n",
    "    df['강수량_2일전'] = df['일_일강수량(mm)'].shift(2)\n",
    "    df['강수량_1일_누적'] = df['일_일강수량(mm)'].rolling(1, min_periods=1).sum()\n",
    "    df['강수량_2일_누적'] = df['일_일강수량(mm)'].rolling(2, min_periods=1).sum()\n",
    "    df['강수량_3일_누적'] = df['일_일강수량(mm)'].rolling(3, min_periods=1).sum()\n",
    "    df['강수량_5일_누적'] = df['일_일강수량(mm)'].rolling(5, min_periods=1).sum()\n",
    "    df['강수량_7일_누적'] = df['일_일강수량(mm)'].rolling(7, min_periods=1).sum()\n",
    "\n",
    "    df['일교차'] = df['일_최고기온(°C)'] - df['일_최저기온(°C)']\n",
    "    df['폭우_여부'] = (df['일_일강수량(mm)'] >= 80).astype(int)\n",
    "    \n",
    "    # 체감온도 계산 (간단 버전)\n",
    "    T = pd.to_numeric(df.get('일_평균기온(°C)', np.nan), errors='coerce')\n",
    "    V_ms = pd.to_numeric(df.get('일_평균풍속(m/s)', np.nan), errors='coerce')\n",
    "    RH = pd.to_numeric(df.get('평균습도(%)', np.nan), errors='coerce')\n",
    "    \n",
    "    e = (RH/100.0) * 6.105 * np.exp(17.27*T/(237.7 + T))\n",
    "    df['체감온도(°C)'] = T + 0.33*e - 0.70*V_ms - 4.00\n",
    "    \n",
    "    # 분류용 등급 계산\n",
    "    q = df['합계'].dropna().quantile([0.15, 0.70, 0.90])\n",
    "    q15, q70, q90 = float(q.loc[0.15]), float(q.loc[0.70]), float(q.loc[0.90])\n",
    "    print(f\"등급 구분 기준: q15={q15:.0f}, q70={q70:.0f}, q90={q90:.0f}\")\n",
    "\n",
    "    def categorize(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        if x < q15:\n",
    "            return 0\n",
    "        elif x < q70:\n",
    "            return 1\n",
    "        elif x < q90:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    df['등급'] = df['합계'].apply(categorize)\n",
    "    \n",
    "    # 타겟 변수 생성 (Data Leakage 방지)\n",
    "    if cutoff_date is not None:\n",
    "        cutoff = pd.to_datetime(cutoff_date)\n",
    "        print(f\"타겟 변수 생성 (cutoff 적용)\")\n",
    "        \n",
    "        df['합계_1일후'] = np.nan\n",
    "        df['합계_2일후'] = np.nan\n",
    "        df['등급_1일후'] = np.nan\n",
    "        df['등급_2일후'] = np.nan\n",
    "        \n",
    "        # 생성된 타겟 개수 추적\n",
    "        target_1day_count = 0\n",
    "        target_2day_count = 0\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            current_date = df.loc[i, '날짜']\n",
    "            \n",
    "            if i + 1 < len(df) and current_date <= cutoff:\n",
    "                next_date = df.loc[i+1, '날짜']\n",
    "                if next_date <= cutoff:\n",
    "                    df.loc[i, '합계_1일후'] = df.loc[i+1, '합계']\n",
    "                    df.loc[i, '등급_1일후'] = df.loc[i+1, '등급']\n",
    "                    target_1day_count += 1\n",
    "            \n",
    "            if i + 2 < len(df) and current_date <= cutoff:\n",
    "                next2_date = df.loc[i+2, '날짜']\n",
    "                if next2_date <= cutoff:\n",
    "                    df.loc[i, '합계_2일후'] = df.loc[i+2, '합계']\n",
    "                    df.loc[i, '등급_2일후'] = df.loc[i+2, '등급']\n",
    "                    target_2day_count += 1\n",
    "                    \n",
    "        print(f\"  1일후 타겟 생성: {target_1day_count}개\")\n",
    "        print(f\"  2일후 타겟 생성: {target_2day_count}개\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"타겟 변수 생성 (shift 사용)\")\n",
    "        df['합계_1일후'] = df['합계'].shift(-1)\n",
    "        df['합계_2일후'] = df['합계'].shift(-2)\n",
    "        df['등급_1일후'] = df['등급'].shift(-1).astype('Int64')\n",
    "        df['등급_2일후'] = df['등급'].shift(-2).astype('Int64')\n",
    "        print(f\"  1일후 타겟 유효값: {df['합계_1일후'].notna().sum()}개\")\n",
    "        print(f\"  2일후 타겟 유효값: {df['합계_2일후'].notna().sum()}개\")\n",
    "\n",
    "    df.attrs['cutoffs'] = {\"q15\": q15, \"q70\": q70, \"q90\": q90}\n",
    "    \n",
    "    # dropna 전후 비교\n",
    "    before_dropna = len(df)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    after_dropna = len(df)\n",
    "    print(f\"dropna 처리: {before_dropna}행 → {after_dropna}행 ({before_dropna - after_dropna}행 제거)\")\n",
    "    \n",
    "    # 2025-06-01 필터링\n",
    "    before_filter = len(df)\n",
    "    df = df[df[\"날짜\"] < \"2025-06-01\"]\n",
    "    after_filter = len(df)\n",
    "    print(f\"날짜 필터링: {before_filter}행 → {after_filter}행 ({before_filter - after_filter}행 제거)\")\n",
    "    \n",
    "    # 최종 결과\n",
    "    if '등급_1일후' in df.columns:\n",
    "        final_grade_dist = df['등급_1일후'].value_counts().sort_index()\n",
    "        print(f\"최종 등급 분포: {dict(final_grade_dist)}\")\n",
    "    \n",
    "    print(f\"최종 출력: {len(df)}행, {len(df.columns)}컬럼\")\n",
    "    print(f\"최종 날짜 범위: {df['날짜'].min()} ~ {df['날짜'].max()}\")\n",
    "    print(f\"=== make_features 완료 ===\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def make_features_for_prediction(historical_df, future_df):\n",
    "    \"\"\"새로운 데이터에 대한 파생변수 생성 (과거 데이터 활용)\"\"\"\n",
    "    combined_df = pd.concat([historical_df, future_df], ignore_index=True)\n",
    "    combined_df['날짜'] = pd.to_datetime(combined_df['날짜'])\n",
    "    combined_df = combined_df.sort_values('날짜').reset_index(drop=True)\n",
    "    \n",
    "    combined_df['월'] = combined_df['날짜'].dt.month\n",
    "    combined_df['요일'] = combined_df['날짜'].dt.weekday\n",
    "    \n",
    "    season_map = {'봄': 0, '여름': 1, '가을': 2, '겨울': 3}\n",
    "    discomfort_map = {'쾌적': 0, '약간 불쾌': 1, '불쾌': 2, '매우 불쾌': 3, '극심한 불쾌': 4}\n",
    "    combined_df['계절'] = combined_df['계절'].map(season_map).astype('Int64')\n",
    "    combined_df['불쾌지수등급'] = combined_df['불쾌지수등급'].map(discomfort_map).astype('Int64')\n",
    "    \n",
    "    # 시차 변수들\n",
    "    combined_df['강수량_1일전'] = combined_df['일_일강수량(mm)'].shift(1)\n",
    "    combined_df['강수량_2일전'] = combined_df['일_일강수량(mm)'].shift(2)\n",
    "    combined_df['강수량_1일_누적'] = combined_df['일_일강수량(mm)'].rolling(1, min_periods=1).sum()\n",
    "    combined_df['강수량_2일_누적'] = combined_df['일_일강수량(mm)'].rolling(2, min_periods=1).sum()\n",
    "    combined_df['강수량_3일_누적'] = combined_df['일_일강수량(mm)'].rolling(3, min_periods=1).sum()\n",
    "    combined_df['강수량_5일_누적'] = combined_df['일_일강수량(mm)'].rolling(5, min_periods=1).sum()\n",
    "    combined_df['강수량_7일_누적'] = combined_df['일_일강수량(mm)'].rolling(7, min_periods=1).sum()\n",
    "    \n",
    "    combined_df['일교차'] = combined_df['일_최고기온(°C)'] - combined_df['일_최저기온(°C)']\n",
    "    combined_df['폭우_여부'] = (combined_df['일_일강수량(mm)'] >= 80).astype(int)\n",
    "    \n",
    "    # 체감온도 계산\n",
    "    T = pd.to_numeric(combined_df.get('일_평균기온(°C)', np.nan), errors='coerce')\n",
    "    V_ms = pd.to_numeric(combined_df.get('일_평균풍속(m/s)', np.nan), errors='coerce')\n",
    "    RH = pd.to_numeric(combined_df.get('평균습도(%)', np.nan), errors='coerce')\n",
    "    \n",
    "    e = (RH/100.0) * 6.105 * np.exp(17.27*T/(237.7 + T))\n",
    "    combined_df['체감온도(°C)'] = T + 0.33*e - 0.70*V_ms - 4.00\n",
    "    \n",
    "    # 새 데이터 부분만 반환\n",
    "    historical_len = len(historical_df)\n",
    "    return combined_df.iloc[historical_len:].reset_index(drop=True)\n",
    "\n",
    "def make_simple_features(data):\n",
    "    \"\"\"간단한 피처 생성 (시차 변수 제외)\"\"\"\n",
    "    df = data.copy()\n",
    "    df['날짜'] = pd.to_datetime(df['날짜'])\n",
    "    df = df.sort_values('날짜').reset_index(drop=True)\n",
    "    \n",
    "    # 기본 피처들\n",
    "    df['월'] = df['날짜'].dt.month\n",
    "    df['요일'] = df['날짜'].dt.weekday\n",
    "    \n",
    "    # 계절/불쾌지수 매핑\n",
    "    season_map = {'봄': 0, '여름': 1, '가을': 2, '겨울': 3}\n",
    "    discomfort_map = {'쾌적': 0, '약간 불쾌': 1, '불쾌': 2, '매우 불쾌': 3, '극심한 불쾌': 4}\n",
    "    \n",
    "    if '계절' in df.columns:\n",
    "        df['계절'] = df['계절'].map(season_map).astype('Int64')\n",
    "    if '불쾌지수등급' in df.columns:\n",
    "        df['불쾌지수등급'] = df['불쾌지수등급'].map(discomfort_map).astype('Int64')\n",
    "    \n",
    "    # 기본 계산 피처들\n",
    "    if '일_최고기온(°C)' in df.columns and '일_최저기온(°C)' in df.columns:\n",
    "        df['일교차'] = df['일_최고기온(°C)'] - df['일_최저기온(°C)']\n",
    "    \n",
    "    if '일_일강수량(mm)' in df.columns:\n",
    "        df['폭우_여부'] = (df['일_일강수량(mm)'] >= 80).astype(int)\n",
    "    \n",
    "    # 체감온도 계산\n",
    "    T = pd.to_numeric(df.get('일_평균기온(°C)', np.nan), errors='coerce')\n",
    "    V_ms = pd.to_numeric(df.get('일_평균풍속(m/s)', np.nan), errors='coerce')\n",
    "    RH = pd.to_numeric(df.get('평균습도(%)', np.nan), errors='coerce')\n",
    "    \n",
    "    e = (RH/100.0) * 6.105 * np.exp(17.27*T/(237.7 + T))\n",
    "    df['체감온도(°C)'] = T + 0.33*e - 0.70*V_ms - 4.00\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ================================================================================================\n",
    "# 3. 평가 함수들\n",
    "# ================================================================================================\n",
    "def evaluate_regression_model(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"회귀 모델 평가\"\"\"\n",
    "    try:\n",
    "        pipe = make_pipeline_unified(model, model_name, \"regression\")\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = pipe.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-8))) * 100\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'regression',\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'mape': mape,\n",
    "            'success': True\n",
    "        }, pipe, y_pred\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'regression',\n",
    "            'mae': np.nan,\n",
    "            'rmse': np.nan,\n",
    "            'r2': np.nan,\n",
    "            'mape': np.nan,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }, None, None\n",
    "\n",
    "def evaluate_classification_model(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"분류 모델 평가\"\"\"\n",
    "    try:\n",
    "        pipe = make_pipeline_unified(model, model_name, \"classification\")\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = pipe.predict(X_test)\n",
    "        \n",
    "        if isinstance(y_pred, np.ndarray) and y_pred.ndim > 1:\n",
    "            y_pred = y_pred.ravel()\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1_macro = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "        f1_weighted = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "        \n",
    "        extreme_classes = [0, 3]\n",
    "        y_true_extreme = pd.Series(y_test).isin(extreme_classes).astype(int)\n",
    "        y_pred_extreme = pd.Series(y_pred).isin(extreme_classes).astype(int)\n",
    "        extreme_f1 = f1_score(y_true_extreme, y_pred_extreme, zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'classification',\n",
    "            'accuracy': acc,\n",
    "            'macro_f1': f1_macro,\n",
    "            'weighted_f1': f1_weighted,\n",
    "            'extreme_f1': extreme_f1,\n",
    "            'success': True\n",
    "        }, pipe, y_pred\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'classification',\n",
    "            'accuracy': np.nan,\n",
    "            'macro_f1': np.nan,\n",
    "            'weighted_f1': np.nan,\n",
    "            'extreme_f1': np.nan,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }, None, None\n",
    "\n",
    "def comprehensive_evaluation_comparison(center_name, df):\n",
    "    \"\"\"Stratified vs 시계열 분할 비교 평가\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"센터: {center_name} - Stratified vs 시계열 분할 비교\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"데이터 크기: {len(df)}행, {len(df.columns)}컬럼\")\n",
    "    \n",
    "    if '등급_1일후' in df.columns:\n",
    "        grade_dist = df['등급_1일후'].value_counts().sort_index()\n",
    "        print(f\"등급 분포: {dict(grade_dist)}\")\n",
    "        \n",
    "        min_class = grade_dist.min()\n",
    "        max_class = grade_dist.max()\n",
    "        imbalance_ratio = max_class / min_class\n",
    "        print(f\"클래스 불균형 비율: {imbalance_ratio:.1f}:1 (최대:{max_class}, 최소:{min_class})\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for split_method in ['temporal', 'stratified']:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"분할 방법: {split_method.upper()}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # 회귀 모델 평가\n",
    "        reg_method_name = \"random_shuffle\" if split_method == \"stratified\" else split_method\n",
    "        print(f\"\\n--- 회귀 모델 평가 ({reg_method_name}) ---\")\n",
    "        \n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test, feature_names, dates_train, dates_test = prepare_data_stratified(\n",
    "                df, target_col=\"합계_1일후\", model_type=\"regression\", test_size=0.2, split_method=split_method\n",
    "            )\n",
    "            \n",
    "            print(f\"회귀용 데이터: 학습 {len(X_train)}행, 테스트 {len(X_test)}행\")\n",
    "            \n",
    "            regression_models = build_regression_models()\n",
    "            \n",
    "            for model_name, model in tqdm(regression_models.items(), desc=f\"회귀({reg_method_name})\", leave=False):\n",
    "                result, pipe, y_pred = evaluate_regression_model(model, model_name, X_train, X_test, y_train, y_test)\n",
    "                result['center'] = center_name\n",
    "                result['split_method'] = split_method\n",
    "                results.append(result)\n",
    "                \n",
    "                if result['success']:\n",
    "                    print(f\"  {model_name:18s}: R²={result['r2']:.3f}, MAE={result['mae']:.0f}, MAPE={result['mape']:.1f}%\")\n",
    "                else:\n",
    "                    print(f\"  {model_name:18s}: 실패 - {result.get('error', '')[:50]}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"회귀 모델 평가 실패 ({reg_method_name}): {e}\")\n",
    "        \n",
    "        # 분류 모델 평가\n",
    "        print(f\"\\n--- 분류 모델 평가 ({split_method}) ---\")\n",
    "        \n",
    "        try:\n",
    "            X_train_clf, X_test_clf, y_train_clf, y_test_clf, feature_names_clf, _, _ = prepare_data_stratified(\n",
    "                df, target_col=\"등급_1일후\", model_type=\"classification\", test_size=0.2, split_method=split_method\n",
    "            )\n",
    "            \n",
    "            print(f\"분류용 데이터: 학습 {len(X_train_clf)}행, 테스트 {len(X_test_clf)}행\")\n",
    "            \n",
    "            test_dist = pd.Series(y_test_clf).value_counts().sort_index()\n",
    "            train_dist = pd.Series(y_train_clf).value_counts().sort_index()\n",
    "            print(f\"학습 세트 등급 분포: {dict(train_dist)}\")\n",
    "            print(f\"테스트 세트 등급 분포: {dict(test_dist)}\")\n",
    "            \n",
    "            classification_models = build_classification_models()\n",
    "            \n",
    "            for model_name, model in tqdm(classification_models.items(), desc=f\"분류({split_method})\", leave=False):\n",
    "                result, pipe, y_pred = evaluate_classification_model(model, model_name, X_train_clf, X_test_clf, y_train_clf, y_test_clf)\n",
    "                result['center'] = center_name\n",
    "                result['split_method'] = split_method\n",
    "                results.append(result)\n",
    "                \n",
    "                if result['success']:\n",
    "                    print(f\"  {model_name:18s}: ACC={result['accuracy']:.3f}, F1={result['macro_f1']:.3f}, 극값F1={result['extreme_f1']:.3f}\")\n",
    "                else:\n",
    "                    print(f\"  {model_name:18s}: 실패 - {result.get('error', '')[:50]}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"분류 모델 평가 실패 ({split_method}): {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ================================================================================================\n",
    "# 4. Feature Importance & SHAP 분석 함수들 (선택사항)\n",
    "# ================================================================================================\n",
    "def extract_feature_importance(model, model_name, feature_names):\n",
    "    \"\"\"모델별 Feature Importance 추출\"\"\"\n",
    "    try:\n",
    "        mdl = model.named_steps['model']\n",
    "        if hasattr(mdl, 'feature_importances_'):\n",
    "            importance = mdl.feature_importances_\n",
    "        elif hasattr(mdl, 'coef_'):\n",
    "            coef = mdl.coef_\n",
    "            if isinstance(coef, np.ndarray) and coef.ndim == 2:\n",
    "                importance = np.mean(np.abs(coef), axis=0)\n",
    "            else:\n",
    "                importance = np.abs(coef)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        if len(importance) != len(feature_names):\n",
    "            print(f\"[경고] importance 길이({len(importance)}) != feature_names({len(feature_names)})\")\n",
    "            m = min(len(importance), len(feature_names))\n",
    "            importance = np.asarray(importance)[:m]\n",
    "            feature_names = list(feature_names)[:m]\n",
    "\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        return importance_df\n",
    "    except Exception as e:\n",
    "        print(f\"Feature importance 추출 실패 ({model_name}): {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_feature_importance(importance_df, model_name, top_n=15):\n",
    "    \"\"\"Feature Importance 시각화\"\"\"\n",
    "    if importance_df is None or len(importance_df) == 0:\n",
    "        return None\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    top_features = importance_df.head(top_n)\n",
    "    \n",
    "    ax.barh(range(len(top_features)), top_features['importance'], color='skyblue')\n",
    "    ax.set_yticks(range(len(top_features)))\n",
    "    ax.set_yticklabels(top_features['feature'])\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title(f'{model_name} - Top {top_n} Feature Importance')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    for i, v in enumerate(top_features['importance']):\n",
    "        ax.text(v + 0.001, i, f'{v:.3f}', va='center')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analyze_model_with_shap(model, X_test, feature_names, model_name, max_samples=100):\n",
    "    \"\"\"SHAP 분석 - 다중분류 대응 강화\"\"\"\n",
    "    if not HAS_SHAP:\n",
    "        print(\"SHAP 라이브러리가 설치되지 않았습니다.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if len(X_test) > max_samples:\n",
    "            sample_idx = np.random.choice(len(X_test), max_samples, replace=False)\n",
    "            X_sample = X_test.iloc[sample_idx]\n",
    "        else:\n",
    "            X_sample = X_test\n",
    "        \n",
    "        X_processed = model.named_steps['pre'].transform(X_sample)\n",
    "        \n",
    "        if 'RandomForest' in model_name or 'GradientBoosting' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        elif 'XGBoost' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        elif 'LightGBM' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        elif 'CatBoost' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        else:\n",
    "            explainer = shap.LinearExplainer(model.named_steps['model'], X_processed)\n",
    "        \n",
    "        shap_values = explainer.shap_values(X_processed)\n",
    "        \n",
    "        # 원본 SHAP 값 디버깅\n",
    "        print(f\"원본 SHAP 값 타입: {type(shap_values)}\")\n",
    "        if isinstance(shap_values, list):\n",
    "            print(f\"리스트 길이: {len(shap_values)}\")\n",
    "            if len(shap_values) > 0:\n",
    "                print(f\"첫 번째 요소 shape: {shap_values[0].shape if hasattr(shap_values[0], 'shape') else 'no shape'}\")\n",
    "        elif hasattr(shap_values, 'shape'):\n",
    "            print(f\"배열 shape: {shap_values.shape}\")\n",
    "        \n",
    "        return shap_values, X_processed, explainer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"SHAP 분석 실패 ({model_name}): {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_shap_summary(shap_values, X_processed, feature_names, model_name):\n",
    "    \"\"\"SHAP Summary Plot - 완전 수정 버전\"\"\"\n",
    "    if shap_values is None or not HAS_SHAP:\n",
    "        return []\n",
    "\n",
    "    figs = []\n",
    "    try:\n",
    "        # 다중분류 SHAP 값 처리 - 더 세밀한 처리\n",
    "        shap_values_use = None\n",
    "        \n",
    "        if isinstance(shap_values, list):\n",
    "            print(f\"SHAP 값이 리스트, 길이: {len(shap_values)}\")\n",
    "            if len(shap_values) > 0:\n",
    "                # 다중분류의 경우 첫 번째 클래스만 사용\n",
    "                shap_values_use = shap_values[0]\n",
    "                print(f\"첫 번째 클래스 shape: {shap_values_use.shape}\")\n",
    "        elif isinstance(shap_values, np.ndarray):\n",
    "            if shap_values.ndim == 3:\n",
    "                print(f\"3차원 배열: {shap_values.shape}\")\n",
    "                # (samples, features, classes) -> (samples, features)\n",
    "                shap_values_use = shap_values[:, :, 0]\n",
    "                print(f\"첫 번째 클래스 추출 후: {shap_values_use.shape}\")\n",
    "            elif shap_values.ndim == 2:\n",
    "                print(f\"2차원 배열: {shap_values.shape}\")\n",
    "                shap_values_use = shap_values\n",
    "            else:\n",
    "                print(f\"예상치 못한 차원: {shap_values.ndim}\")\n",
    "                shap_values_use = shap_values\n",
    "        else:\n",
    "            print(f\"알 수 없는 타입: {type(shap_values)}\")\n",
    "            shap_values_use = shap_values\n",
    "        \n",
    "        # 최종 확인\n",
    "        if shap_values_use is None:\n",
    "            print(\"SHAP 값 처리 실패\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"최종 사용할 SHAP 값 shape: {shap_values_use.shape}\")\n",
    "        print(f\"X_processed shape: {X_processed.shape}\")\n",
    "        print(f\"feature_names 길이: {len(feature_names)}\")\n",
    "        \n",
    "        # 차원 일치성 확인\n",
    "        if hasattr(shap_values_use, 'shape') and len(shap_values_use.shape) >= 2:\n",
    "            if shap_values_use.shape[1] != len(feature_names):\n",
    "                print(f\"경고: SHAP 피처 수({shap_values_use.shape[1]}) != feature_names 수({len(feature_names)})\")\n",
    "                # 최소 길이로 맞춤\n",
    "                min_features = min(shap_values_use.shape[1], len(feature_names))\n",
    "                shap_values_use = shap_values_use[:, :min_features]\n",
    "                feature_names = feature_names[:min_features]\n",
    "                print(f\"조정 후 - SHAP: {shap_values_use.shape}, features: {len(feature_names)}\")\n",
    "        \n",
    "        # Bar plot 시도\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            shap.summary_plot(shap_values_use, X_processed[:len(shap_values_use)],\n",
    "                              feature_names=feature_names,\n",
    "                              plot_type=\"bar\", show=False)\n",
    "            plt.title(f'{model_name} - SHAP Feature Importance')\n",
    "            figs.append(plt.gcf())\n",
    "            print(\"    SHAP bar plot 성공\")\n",
    "        except Exception as e:\n",
    "            print(f\"    SHAP bar plot 실패: {e}\")\n",
    "        \n",
    "        # Beeswarm plot 시도 (더 까다로움)\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            shap.summary_plot(shap_values_use, X_processed[:len(shap_values_use)],\n",
    "                              feature_names=feature_names,\n",
    "                              show=False)\n",
    "            plt.title(f'{model_name} - SHAP Summary Plot')\n",
    "            figs.append(plt.gcf())\n",
    "            print(\"    SHAP beeswarm plot 성공\")\n",
    "        except Exception as e:\n",
    "            print(f\"    SHAP beeswarm plot 실패: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"SHAP 시각화 전체 실패 ({model_name}): {e}\")\n",
    "\n",
    "    plt.close('all')\n",
    "    return figs\n",
    "\n",
    "# ================================================================================================\n",
    "# 5. 유틸리티 함수들\n",
    "# ================================================================================================\n",
    "def load_original_data():\n",
    "    \"\"\"원본 데이터 로드\"\"\"\n",
    "    nanji_raw = pd.read_csv('../data/processed/center_season/nanji/난지_merged.csv', encoding='utf-8-sig')\n",
    "    jungnang_raw = pd.read_csv('../data/processed/center_season/jungnang/중랑_merged.csv', encoding='utf-8-sig')\n",
    "    seonam_raw = pd.read_csv('../data/processed/center_season/seonam/서남_merged.csv', encoding='utf-8-sig')\n",
    "    tancheon_raw = pd.read_csv('../data/processed/center_season/tancheon/탄천_merged.csv', encoding='utf-8-sig')\n",
    "    \n",
    "    return {\n",
    "        \"nanji\": nanji_raw,\n",
    "        \"jungnang\": jungnang_raw,\n",
    "        \"seonam\": seonam_raw,\n",
    "        \"tancheon\": tancheon_raw\n",
    "    }\n",
    "\n",
    "def prepare_prediction_features(future_data, expected_features):\n",
    "    \"\"\"예측용 피처 준비\"\"\"\n",
    "    not_use_col = [\n",
    "        '날짜', '1처리장','2처리장','정화조','중계펌프장','합계','시설현대화',\n",
    "        '3처리장','4처리장','합계', '합계_1일후','합계_2일후',\n",
    "        '등급','등급_1일후','등급_2일후'\n",
    "    ]\n",
    "    \n",
    "    available_cols = [col for col in future_data.columns if col not in not_use_col]\n",
    "    X_future = future_data[available_cols].copy()\n",
    "    \n",
    "    for c in X_future.columns:\n",
    "        X_future[c] = pd.to_numeric(X_future[c], errors=\"coerce\")\n",
    "    \n",
    "    missing_features = set(expected_features) - set(X_future.columns)\n",
    "    if missing_features:\n",
    "        for feature in missing_features:\n",
    "            X_future[feature] = 0\n",
    "    \n",
    "    X_future = X_future[expected_features].copy()\n",
    "    return X_future\n",
    "\n",
    "# ================================================================================================\n",
    "# 6. 모델 성능 비교 시각화 함수들\n",
    "# ================================================================================================\n",
    "def plot_data_characteristics_comparison(centers_data, results_dir, timestamp):\n",
    "    \"\"\"센터별 데이터 특성 비교\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('센터별 데이터 특성 비교', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    center_stats = []\n",
    "    \n",
    "    for center_name, df in centers_data.items():\n",
    "        df['날짜'] = pd.to_datetime(df['날짜'])\n",
    "        df = df.sort_values('날짜').reset_index(drop=True)\n",
    "        \n",
    "        stats = {\n",
    "            'center': center_name,\n",
    "            'total_records': len(df),\n",
    "            'date_range': (df['날짜'].max() - df['날짜'].min()).days,\n",
    "            'avg_flow': df['합계'].mean() if '합계' in df.columns else 0,\n",
    "            'std_flow': df['합계'].std() if '합계' in df.columns else 0,\n",
    "            'missing_ratio': df.isnull().sum().sum() / (len(df) * len(df.columns))\n",
    "        }\n",
    "        center_stats.append(stats)\n",
    "    \n",
    "    center_stats_df = pd.DataFrame(center_stats)\n",
    "    \n",
    "    # 1. 데이터 양 비교\n",
    "    axes[0,0].bar(center_stats_df['center'], center_stats_df['total_records'], color='skyblue')\n",
    "    axes[0,0].set_title('센터별 데이터 양')\n",
    "    axes[0,0].set_ylabel('레코드 수')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    for i, v in enumerate(center_stats_df['total_records']):\n",
    "        axes[0,0].text(i, v + 10, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    # 2. 평균 유량 비교\n",
    "    axes[0,1].bar(center_stats_df['center'], center_stats_df['avg_flow'], color='lightgreen')\n",
    "    axes[0,1].set_title('센터별 평균 유량')\n",
    "    axes[0,1].set_ylabel('평균 유량')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    for i, v in enumerate(center_stats_df['avg_flow']):\n",
    "        axes[0,1].text(i, v + max(center_stats_df['avg_flow'])*0.01, f'{v:.0f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. 유량 변동성 비교\n",
    "    axes[1,0].bar(center_stats_df['center'], center_stats_df['std_flow'], color='coral')\n",
    "    axes[1,0].set_title('센터별 유량 변동성 (표준편차)')\n",
    "    axes[1,0].set_ylabel('표준편차')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    for i, v in enumerate(center_stats_df['std_flow']):\n",
    "        axes[1,0].text(i, v + max(center_stats_df['std_flow'])*0.01, f'{v:.0f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. 데이터 완성도\n",
    "    missing_pct = center_stats_df['missing_ratio'] * 100\n",
    "    axes[1,1].bar(center_stats_df['center'], 100 - missing_pct, color='gold')\n",
    "    axes[1,1].set_title('센터별 데이터 완성도')\n",
    "    axes[1,1].set_ylabel('완성도 (%)')\n",
    "    axes[1,1].set_ylim(0, 100)\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    for i, v in enumerate(100 - missing_pct):\n",
    "        axes[1,1].text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"data_characteristics_comparison_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  데이터 특성 비교 저장: {os.path.basename(save_path)}\")\n",
    "\n",
    "def plot_model_complexity_vs_performance(training_summary, results_dir, timestamp):\n",
    "    \"\"\"모델 복잡도 vs 성능 관계\"\"\"\n",
    "    complexity_map = {\n",
    "        'LinearRegression': 1, 'LogisticRegression_Clf': 1,\n",
    "        'RandomForest_Reg': 3, 'RandomForest_Clf': 3,\n",
    "        'GradientBoosting_Reg': 4, 'GradientBoosting_Clf': 4,\n",
    "        'XGBoost_Reg': 4, 'XGBoost_Clf': 4,\n",
    "        'LightGBM_Reg': 4, 'LightGBM_Clf': 4,\n",
    "        'CatBoost_Reg': 5, 'CatBoost_Clf': 5\n",
    "    }\n",
    "    \n",
    "    if isinstance(training_summary, list):\n",
    "        summary_df = pd.DataFrame(training_summary)\n",
    "    else:\n",
    "        summary_df = training_summary.copy()\n",
    "    \n",
    "    summary_df['complexity'] = summary_df['model_name'].map(complexity_map)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle('모델 복잡도 vs 성능 관계', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 회귀 모델\n",
    "    reg_data = summary_df[summary_df['task_type'] == 'regression'].copy()\n",
    "    if len(reg_data) > 0:\n",
    "        reg_data['r2'] = reg_data['performance'].apply(lambda x: x.get('r2', 0))\n",
    "        \n",
    "        scatter = axes[0].scatter(reg_data['complexity'], reg_data['r2'], \n",
    "                                 c=reg_data.index, cmap='viridis', s=100, alpha=0.7)\n",
    "        \n",
    "        for i, row in reg_data.iterrows():\n",
    "            axes[0].annotate(f\"{row['center'][:3]}\", \n",
    "                           (row['complexity'], row['r2']),\n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        axes[0].set_xlabel('모델 복잡도 (1=단순, 5=복잡)')\n",
    "        axes[0].set_ylabel('R² Score')\n",
    "        axes[0].set_title('회귀: 복잡도 vs 성능')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].set_xticks(range(1, 6))\n",
    "    \n",
    "    # 분류 모델\n",
    "    clf_data = summary_df[summary_df['task_type'] == 'classification'].copy()\n",
    "    if len(clf_data) > 0:\n",
    "        clf_data['macro_f1'] = clf_data['performance'].apply(lambda x: x.get('macro_f1', 0))\n",
    "        \n",
    "        scatter = axes[1].scatter(clf_data['complexity'], clf_data['macro_f1'], \n",
    "                                 c=clf_data.index, cmap='plasma', s=100, alpha=0.7)\n",
    "        \n",
    "        for i, row in clf_data.iterrows():\n",
    "            axes[1].annotate(f\"{row['center'][:3]}\", \n",
    "                           (row['complexity'], row['macro_f1']),\n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        axes[1].set_xlabel('모델 복잡도 (1=단순, 5=복잡)')\n",
    "        axes[1].set_ylabel('Macro F1 Score')\n",
    "        axes[1].set_title('분류: 복잡도 vs 성능')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].set_xticks(range(1, 6))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"model_complexity_vs_performance_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  복잡도 vs 성능 저장: {os.path.basename(save_path)}\")\n",
    "\n",
    "def plot_prediction_difficulty_analysis(training_summary, results_dir, timestamp):\n",
    "    \"\"\"센터별 예측 난이도 분석\"\"\"\n",
    "    if isinstance(training_summary, list):\n",
    "        summary_df = pd.DataFrame(training_summary)\n",
    "    else:\n",
    "        summary_df = training_summary.copy()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('센터별 예측 난이도 분석', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    centers = summary_df['center'].unique()\n",
    "    \n",
    "    # 1. 센터별 최고 R² 성능\n",
    "    reg_best_scores = []\n",
    "    for center in centers:\n",
    "        center_reg = summary_df[(summary_df['center'] == center) & \n",
    "                               (summary_df['task_type'] == 'regression')]\n",
    "        if len(center_reg) > 0:\n",
    "            center_reg = center_reg.copy()\n",
    "            center_reg['r2'] = center_reg['performance'].apply(lambda x: x.get('r2', 0))\n",
    "            reg_best_scores.append(center_reg['r2'].max())\n",
    "        else:\n",
    "            reg_best_scores.append(0)\n",
    "    \n",
    "    bars1 = axes[0,0].bar(centers, reg_best_scores, color='skyblue')\n",
    "    axes[0,0].set_title('센터별 최고 R² 성능 (높을수록 예측 용이)')\n",
    "    axes[0,0].set_ylabel('최고 R² Score')\n",
    "    axes[0,0].set_ylim(0, 1)\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, score in zip(bars1, reg_best_scores):\n",
    "        axes[0,0].text(bar.get_x() + bar.get_width()/2, score + 0.01, \n",
    "                      f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. 센터별 성능 분산 (일관성)\n",
    "    reg_score_vars = []\n",
    "    for center in centers:\n",
    "        center_reg = summary_df[(summary_df['center'] == center) & \n",
    "                               (summary_df['task_type'] == 'regression')]\n",
    "        if len(center_reg) > 0:\n",
    "            center_reg = center_reg.copy()\n",
    "            center_reg['r2'] = center_reg['performance'].apply(lambda x: x.get('r2', 0))\n",
    "            reg_score_vars.append(center_reg['r2'].std())\n",
    "        else:\n",
    "            reg_score_vars.append(0)\n",
    "    \n",
    "    bars2 = axes[0,1].bar(centers, reg_score_vars, color='lightcoral')\n",
    "    axes[0,1].set_title('센터별 성능 분산 (낮을수록 일관성 높음)')\n",
    "    axes[0,1].set_ylabel('R² Score 표준편차')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, var in zip(bars2, reg_score_vars):\n",
    "        axes[0,1].text(bar.get_x() + bar.get_width()/2, var + max(reg_score_vars)*0.01, \n",
    "                      f'{var:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. 센터별 최고 F1 성능\n",
    "    clf_best_scores = []\n",
    "    for center in centers:\n",
    "        center_clf = summary_df[(summary_df['center'] == center) & \n",
    "                               (summary_df['task_type'] == 'classification')]\n",
    "        if len(center_clf) > 0:\n",
    "            center_clf = center_clf.copy()\n",
    "            center_clf['macro_f1'] = center_clf['performance'].apply(lambda x: x.get('macro_f1', 0))\n",
    "            clf_best_scores.append(center_clf['macro_f1'].max())\n",
    "        else:\n",
    "            clf_best_scores.append(0)\n",
    "    \n",
    "    bars3 = axes[1,0].bar(centers, clf_best_scores, color='lightgreen')\n",
    "    axes[1,0].set_title('센터별 최고 F1 성능 (높을수록 분류 용이)')\n",
    "    axes[1,0].set_ylabel('최고 Macro F1 Score')\n",
    "    axes[1,0].set_ylim(0, 1)\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, score in zip(bars3, clf_best_scores):\n",
    "        axes[1,0].text(bar.get_x() + bar.get_width()/2, score + 0.01, \n",
    "                      f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. 종합 예측 난이도 점수\n",
    "    difficulty_scores = [(r2 + f1) / 2 for r2, f1 in zip(reg_best_scores, clf_best_scores)]\n",
    "    colors = plt.cm.RdYlGn([score for score in difficulty_scores])\n",
    "    \n",
    "    bars4 = axes[1,1].bar(centers, difficulty_scores, color=colors)\n",
    "    axes[1,1].set_title('종합 예측 용이도 점수')\n",
    "    axes[1,1].set_ylabel('종합 점수 (R² + F1) / 2')\n",
    "    axes[1,1].set_ylim(0, 1)\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, score in zip(bars4, difficulty_scores):\n",
    "        axes[1,1].text(bar.get_x() + bar.get_width()/2, score + 0.01, \n",
    "                      f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"prediction_difficulty_analysis_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  예측 난이도 분석 저장: {os.path.basename(save_path)}\")\n",
    "\n",
    "def plot_split_method_detailed_comparison_fixed(all_training_results, results_dir, timestamp):\n",
    "    \"\"\"Split Method 상세 비교 분석 - 수정된 버전\"\"\"\n",
    "    if isinstance(all_training_results, list):\n",
    "        summary_df = pd.DataFrame(all_training_results)\n",
    "    else:\n",
    "        summary_df = all_training_results.copy()\n",
    "    \n",
    "    successful_results = summary_df[summary_df['success'] == True].copy()\n",
    "    \n",
    "    if len(successful_results) == 0 or 'split_method' not in successful_results.columns:\n",
    "        print(\"Split method 정보가 없거나 성공한 결과가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('Stratified vs Temporal Split 상세 비교', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 회귀 데이터 처리\n",
    "    reg_data = successful_results[successful_results['type'] == 'regression'].copy()\n",
    "    if len(reg_data) > 0:\n",
    "        reg_pivot = reg_data.pivot_table(index='center', columns='split_method', values='r2')\n",
    "        reg_pivot.plot(kind='bar', ax=axes[0,0])\n",
    "        axes[0,0].set_title('센터별 R² 성능: Split Method 비교')\n",
    "        axes[0,0].set_ylabel('R² Score')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        axes[0,0].legend(title='Split Method')\n",
    "        \n",
    "        split_methods = reg_data['split_method'].unique()\n",
    "        r2_by_split = [reg_data[reg_data['split_method'] == method]['r2'].values \n",
    "                       for method in split_methods]\n",
    "        \n",
    "        axes[0,1].boxplot(r2_by_split, labels=split_methods)\n",
    "        axes[0,1].set_title('Split Method별 R² 분포')\n",
    "        axes[0,1].set_ylabel('R² Score')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        if len(split_methods) == 2:\n",
    "            method1, method2 = split_methods\n",
    "            perf_diff = []\n",
    "            centers = reg_data['center'].unique()\n",
    "            \n",
    "            for center in centers:\n",
    "                center_data = reg_data[reg_data['center'] == center]\n",
    "                perf1 = center_data[center_data['split_method'] == method1]['r2']\n",
    "                perf2 = center_data[center_data['split\n",
    "                                                \n",
    "    return created_plots\n",
    "\n",
    "# ================================================================================================\n",
    "# 7. SHAP 분석 및 모델 저장 함수들\n",
    "# ================================================================================================\n",
    "# \n",
    "# \n",
    "def train_and_save_single_model_with_shap(center_name, train_data, best_result, task_type, save_dir, results_dir):\n",
    "    \"\"\"개별 모델 학습 및 저장 + SHAP 분석 포함\"\"\"\n",
    "    try:\n",
    "        model_name = best_result['model']\n",
    "        split_method = best_result['split_method']\n",
    "        target_col = \"합계_1일후\" if task_type == \"regression\" else \"등급_1일후\"\n",
    "        \n",
    "        X_train, X_test, y_train, y_test, feature_names, _, _ = prepare_data_stratified(\n",
    "            train_data, target_col=target_col, model_type=task_type, \n",
    "            test_size=0.2, split_method=split_method\n",
    "        )\n",
    "        \n",
    "        X_all = pd.concat([X_train, X_test], ignore_index=True)\n",
    "        y_all = pd.concat([y_train, y_test], ignore_index=True)\n",
    "        \n",
    "        if task_type == \"regression\":\n",
    "            models = build_regression_models()\n",
    "        else:\n",
    "            models = build_classification_models()\n",
    "        \n",
    "        model = models[model_name]\n",
    "        pipeline = make_pipeline_unified(model, model_name, task_type)\n",
    "        pipeline.fit(X_all, y_all)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{center_name}_{model_name}_{task_type}_{timestamp}.pkl\"\n",
    "        model_path = os.path.join(save_dir, filename)\n",
    "        \n",
    "        model_data = {\n",
    "            'pipeline': pipeline,\n",
    "            'feature_names': feature_names,\n",
    "            'model_name': model_name,\n",
    "            'center_name': center_name,\n",
    "            'task_type': task_type,\n",
    "            'performance': dict(best_result),\n",
    "            'split_method': split_method,\n",
    "            'training_date': datetime.now().isoformat(),\n",
    "            'target_column': target_col\n",
    "        }\n",
    "        \n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"    저장됨: {filename}\")\n",
    "        \n",
    "        analysis_pipeline = make_pipeline_unified(model, model_name, task_type)\n",
    "        analysis_pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Feature Importance 분석\n",
    "        try:\n",
    "            importance_df = extract_feature_importance(analysis_pipeline, model_name, feature_names)\n",
    "            if importance_df is not None:\n",
    "                print(f\"    Top 5 피처: {', '.join(importance_df.head(5)['feature'].tolist())}\")\n",
    "                \n",
    "                fig = plot_feature_importance(importance_df, f\"{center_name}_{model_name}\")\n",
    "                if fig:\n",
    "                    img_path = os.path.join(results_dir, f\"feature_importance_{center_name}_{model_name}_{task_type}_{timestamp}.png\")\n",
    "                    fig.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "                    plt.close(fig)\n",
    "                    print(f\"    Feature Importance 저장: {os.path.basename(img_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Feature Importance 분석 실패: {e}\")\n",
    "        \n",
    "        # SHAP 분석\n",
    "        if HAS_SHAP:\n",
    "            try:\n",
    "                print(f\"    SHAP 분석 시작...\")\n",
    "                shap_result = analyze_model_with_shap(analysis_pipeline, X_test, feature_names, model_name, max_samples=50)\n",
    "                \n",
    "                if shap_result:\n",
    "                    shap_values, X_processed, explainer = shap_result\n",
    "                    shap_figs = plot_shap_summary(shap_values, X_processed, feature_names, f\"{center_name}_{model_name}\")\n",
    "                    \n",
    "                    for i, fig in enumerate(shap_figs):\n",
    "                        suffix = \"bar\" if i == 0 else \"beeswarm\"\n",
    "                        shap_img_path = os.path.join(results_dir, f\"shap_{suffix}_{center_name}_{model_name}_{task_type}_{timestamp}.png\")\n",
    "                        fig.savefig(shap_img_path, dpi=300, bbox_inches='tight')\n",
    "                        plt.close(fig)\n",
    "                        print(f\"    SHAP {suffix} 저장: {os.path.basename(shap_img_path)}\")\n",
    "                    \n",
    "                    if isinstance(shap_values, np.ndarray):\n",
    "                        mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
    "                        shap_summary_df = pd.DataFrame({\n",
    "                            'feature': feature_names,\n",
    "                            'mean_abs_shap': mean_abs_shap\n",
    "                        }).sort_values('mean_abs_shap', ascending=False)\n",
    "                        \n",
    "                        shap_csv_path = os.path.join(results_dir, f\"shap_summary_{center_name}_{model_name}_{task_type}_{timestamp}.csv\")\n",
    "                        shap_summary_df.to_csv(shap_csv_path, index=False, encoding='utf-8-sig')\n",
    "                        print(f\"    SHAP 요약 저장: {os.path.basename(shap_csv_path)}\")\n",
    "                \n",
    "                plt.close('all')\n",
    "                del shap_values, X_processed\n",
    "                gc.collect()\n",
    "                print(f\"    메모리 정리 완료\")\n",
    "            except Exception as e:\n",
    "                print(f\"    SHAP 분석 실패: {e}\")\n",
    "        else:\n",
    "            print(f\"    SHAP 라이브러리 없음 - SHAP 분석 건너뜀\")\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'performance': dict(best_result),\n",
    "            'saved_path': model_path,\n",
    "            'feature_names': feature_names,\n",
    "            'target_column': target_col\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    모델 저장 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def select_and_save_best_models_with_shap(center_name, train_data, training_results, save_dir, results_dir):\n",
    "    \"\"\"센터별 최고 성능 모델 선택 및 저장 + SHAP 분석\"\"\"\n",
    "    results_df = pd.DataFrame(training_results)\n",
    "    \n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8597d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================================================\n",
    "# 완전한 하수처리장 예측 파이프라인 (학습-저장-예측)\n",
    "# ================================================================================================\n",
    "\n",
    "# ================================================================================================\n",
    "# 1. 모델 정의 함수들\n",
    "# ================================================================================================\n",
    "def build_regression_models():\n",
    "    \"\"\"회귀 모델들\"\"\"\n",
    "    models = {}\n",
    "    models[\"RandomForest_Reg\"] = RandomForestRegressor(\n",
    "        n_estimators=300, min_samples_leaf=2, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    models[\"LinearRegression\"] = LinearRegression()\n",
    "    models[\"GradientBoosting_Reg\"] = GradientBoostingRegressor(\n",
    "        n_estimators=200, learning_rate=0.1, random_state=42\n",
    "    )\n",
    "    if HAS_XGB:\n",
    "        models[\"XGBoost_Reg\"] = xgb.XGBRegressor(\n",
    "            n_estimators=400, max_depth=5, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            random_state=42, n_jobs=-1, verbosity=0\n",
    "        )\n",
    "    if HAS_LGB:\n",
    "        models[\"LightGBM_Reg\"] = lgb.LGBMRegressor(\n",
    "            n_estimators=500, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            random_state=42, n_jobs=-1, verbosity=-1\n",
    "        )\n",
    "    if HAS_CATBOOST:\n",
    "        models[\"CatBoost_Reg\"] = cb.CatBoostRegressor(\n",
    "            iterations=500, learning_rate=0.05, depth=6,\n",
    "            random_state=42, verbose=False\n",
    "        )\n",
    "    return models\n",
    "\n",
    "def build_classification_models():\n",
    "    \"\"\"분류 모델들 (4등급)\"\"\"\n",
    "    models = {}\n",
    "    models[\"RandomForest_Clf\"] = RandomForestClassifier(\n",
    "        n_estimators=300, min_samples_leaf=2, random_state=42, \n",
    "        n_jobs=-1, class_weight='balanced'\n",
    "    ) \n",
    "    models[\"GradientBoosting_Clf\"] = GradientBoostingClassifier(\n",
    "        n_estimators=200, learning_rate=0.1, random_state=42\n",
    "    )\n",
    "    models[\"LogisticRegression_Clf\"] = LogisticRegression(\n",
    "        multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=1000,\n",
    "        random_state=42, class_weight='balanced'\n",
    "    )\n",
    "    if HAS_XGB:\n",
    "        models[\"XGBoost_Clf\"] = xgb.XGBClassifier(\n",
    "            n_estimators=400, max_depth=5, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            objective=\"multi:softprob\", num_class=4,\n",
    "            tree_method=\"hist\", random_state=42, n_jobs=-1, verbosity=0\n",
    "        )\n",
    "    if HAS_LGB:\n",
    "        models[\"LightGBM_Clf\"] = lgb.LGBMClassifier(\n",
    "            n_estimators=500, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            objective=\"multiclass\", num_class=4,\n",
    "            random_state=42, n_jobs=-1, verbosity=-1, is_unbalance=True\n",
    "        )\n",
    "    if HAS_CATBOOST:\n",
    "        models[\"CatBoost_Clf\"] = cb.CatBoostClassifier(\n",
    "            iterations=500, learning_rate=0.05, depth=6,\n",
    "            random_state=42, verbose=False, auto_class_weights='Balanced'\n",
    "        )\n",
    "    return models\n",
    "\n",
    "# ================================================================================================\n",
    "# 2. 데이터 처리 함수들\n",
    "# ================================================================================================\n",
    "def make_pipeline_unified(model, model_name, model_type):\n",
    "    \"\"\"통합 전처리 파이프라인\"\"\"\n",
    "    if model_name in [\"LinearRegression\", \"LogisticRegression_Clf\"]:\n",
    "        pre = Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ])\n",
    "    else:\n",
    "        pre = Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        ])\n",
    "    return Pipeline(steps=[(\"pre\", pre), (\"model\", model)])\n",
    "\n",
    "def prepare_data_stratified(df, target_col, model_type, test_size=0.2, split_method='stratified'):\n",
    "    \"\"\"데이터 준비 - Stratified vs 시계열 분할\"\"\"\n",
    "    work = df.sort_values('날짜').reset_index(drop=True).copy()\n",
    "    dates = pd.to_datetime(work['날짜'])\n",
    "\n",
    "    not_use_col = [\n",
    "        '날짜',\n",
    "        '1처리장','2처리장','정화조','중계펌프장','합계','시설현대화',\n",
    "        '3처리장','4처리장','합계', '합계_1일후','합계_2일후',\n",
    "        '등급','등급_1일후','등급_2일후'\n",
    "    ]\n",
    "    \n",
    "    drop_cols = [c for c in (set(not_use_col) | {target_col}) if c in work.columns]\n",
    "    X_raw = work.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    \n",
    "    for c in X_raw.columns:\n",
    "        X_raw[c] = pd.to_numeric(X_raw[c], errors=\"coerce\")\n",
    "\n",
    "    if model_type == \"regression\":\n",
    "        y = pd.to_numeric(work[target_col], errors=\"coerce\")\n",
    "    else:\n",
    "        y = work[target_col].astype(\"int64\")\n",
    "\n",
    "    valid_idx = (~X_raw.isnull().all(axis=1)) & (~pd.isnull(y))\n",
    "    X_raw = X_raw[valid_idx].reset_index(drop=True)\n",
    "    y = y[valid_idx].reset_index(drop=True)\n",
    "    dates = dates[valid_idx].reset_index(drop=True)\n",
    "    \n",
    "    if split_method == 'stratified':\n",
    "        if model_type == \"classification\":\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "            train_idx, test_idx = next(sss.split(X_raw, y))\n",
    "        else:\n",
    "            train_idx, test_idx = train_test_split(\n",
    "                range(len(X_raw)), test_size=test_size, random_state=42\n",
    "            )\n",
    "            \n",
    "        X_train, X_test = X_raw.iloc[train_idx].copy(), X_raw.iloc[test_idx].copy()\n",
    "        y_train, y_test = y.iloc[train_idx].copy(), y.iloc[test_idx].copy()\n",
    "        dates_train, dates_test = dates.iloc[train_idx].copy(), dates.iloc[test_idx].copy()\n",
    "        \n",
    "    else:  # temporal split\n",
    "        n = len(X_raw)\n",
    "        split = int(n * (1 - test_size))\n",
    "        X_train, X_test = X_raw.iloc[:split].copy(), X_raw.iloc[split:].copy()\n",
    "        y_train, y_test = y.iloc[:split].copy(), y.iloc[split:].copy()\n",
    "        dates_train, dates_test = dates.iloc[:split].copy(), dates.iloc[split:].copy()\n",
    "\n",
    "    feature_names = list(X_raw.columns)\n",
    "    return X_train, X_test, y_train, y_test, feature_names, dates_train, dates_test\n",
    "\n",
    "def make_features(df, cutoff_date=None):\n",
    "    \"\"\"파생변수 생성 함수 - Data Leakage 방지 버전\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['날짜'] = pd.to_datetime(df['날짜'])\n",
    "    df = df.sort_values('날짜').reset_index(drop=True)\n",
    "    \n",
    "    # 디버깅 정보 - 시작 시점\n",
    "    print(f\"\\n=== make_features 디버깅 ===\")\n",
    "    print(f\"입력 데이터: {len(df)}행\")\n",
    "    print(f\"날짜 범위: {df['날짜'].min()} ~ {df['날짜'].max()}\")\n",
    "    if cutoff_date is not None:\n",
    "        cutoff = pd.to_datetime(cutoff_date)\n",
    "        print(f\"cutoff_date: {cutoff_date}\")\n",
    "        print(f\"  cutoff 이전 데이터: {len(df[df['날짜'] <= cutoff])}행\")\n",
    "        print(f\"  cutoff 이후 데이터: {len(df[df['날짜'] > cutoff])}행\")\n",
    "\n",
    "    df['월'] = df['날짜'].dt.month\n",
    "    df['요일'] = df['날짜'].dt.weekday\n",
    "\n",
    "    season_map = {'봄': 0, '여름': 1, '가을': 2, '겨울': 3}\n",
    "    discomfort_map = {'쾌적': 0, '약간 불쾌': 1, '불쾌': 2, '매우 불쾌': 3, '극심한 불쾌': 4}\n",
    "    df['계절'] = df['계절'].map(season_map).astype('Int64')\n",
    "    df['불쾌지수등급'] = df['불쾌지수등급'].map(discomfort_map).astype('Int64')\n",
    "\n",
    "    # 강수량 시차 피처\n",
    "    df['강수량_1일전'] = df['일_일강수량(mm)'].shift(1)\n",
    "    df['강수량_2일전'] = df['일_일강수량(mm)'].shift(2)\n",
    "    df['강수량_1일_누적'] = df['일_일강수량(mm)'].rolling(1, min_periods=1).sum()\n",
    "    df['강수량_2일_누적'] = df['일_일강수량(mm)'].rolling(2, min_periods=1).sum()\n",
    "    df['강수량_3일_누적'] = df['일_일강수량(mm)'].rolling(3, min_periods=1).sum()\n",
    "    df['강수량_5일_누적'] = df['일_일강수량(mm)'].rolling(5, min_periods=1).sum()\n",
    "    df['강수량_7일_누적'] = df['일_일강수량(mm)'].rolling(7, min_periods=1).sum()\n",
    "\n",
    "    df['일교차'] = df['일_최고기온(°C)'] - df['일_최저기온(°C)']\n",
    "    df['폭우_여부'] = (df['일_일강수량(mm)'] >= 80).astype(int)\n",
    "    \n",
    "    # 체감온도 계산 (간단 버전)\n",
    "    T = pd.to_numeric(df.get('일_평균기온(°C)', np.nan), errors='coerce')\n",
    "    V_ms = pd.to_numeric(df.get('일_평균풍속(m/s)', np.nan), errors='coerce')\n",
    "    RH = pd.to_numeric(df.get('평균습도(%)', np.nan), errors='coerce')\n",
    "    \n",
    "    e = (RH/100.0) * 6.105 * np.exp(17.27*T/(237.7 + T))\n",
    "    df['체감온도(°C)'] = T + 0.33*e - 0.70*V_ms - 4.00\n",
    "    \n",
    "    # 분류용 등급 계산\n",
    "    q = df['합계'].dropna().quantile([0.15, 0.70, 0.90])\n",
    "    q15, q70, q90 = float(q.loc[0.15]), float(q.loc[0.70]), float(q.loc[0.90])\n",
    "    print(f\"등급 구분 기준: q15={q15:.0f}, q70={q70:.0f}, q90={q90:.0f}\")\n",
    "\n",
    "    def categorize(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        if x < q15:\n",
    "            return 0\n",
    "        elif x < q70:\n",
    "            return 1\n",
    "        elif x < q90:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    df['등급'] = df['합계'].apply(categorize)\n",
    "    \n",
    "    # 타겟 변수 생성 (Data Leakage 방지)\n",
    "    if cutoff_date is not None:\n",
    "        cutoff = pd.to_datetime(cutoff_date)\n",
    "        print(f\"타겟 변수 생성 (cutoff 적용)\")\n",
    "        \n",
    "        df['합계_1일후'] = np.nan\n",
    "        df['합계_2일후'] = np.nan\n",
    "        df['등급_1일후'] = np.nan\n",
    "        df['등급_2일후'] = np.nan\n",
    "        \n",
    "        # 생성된 타겟 개수 추적\n",
    "        target_1day_count = 0\n",
    "        target_2day_count = 0\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            current_date = df.loc[i, '날짜']\n",
    "            \n",
    "            if i + 1 < len(df) and current_date <= cutoff:\n",
    "                next_date = df.loc[i+1, '날짜']\n",
    "                if next_date <= cutoff:\n",
    "                    df.loc[i, '합계_1일후'] = df.loc[i+1, '합계']\n",
    "                    df.loc[i, '등급_1일후'] = df.loc[i+1, '등급']\n",
    "                    target_1day_count += 1\n",
    "            \n",
    "            if i + 2 < len(df) and current_date <= cutoff:\n",
    "                next2_date = df.loc[i+2, '날짜']\n",
    "                if next2_date <= cutoff:\n",
    "                    df.loc[i, '합계_2일후'] = df.loc[i+2, '합계']\n",
    "                    df.loc[i, '등급_2일후'] = df.loc[i+2, '등급']\n",
    "                    target_2day_count += 1\n",
    "                    \n",
    "        print(f\"  1일후 타겟 생성: {target_1day_count}개\")\n",
    "        print(f\"  2일후 타겟 생성: {target_2day_count}개\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"타겟 변수 생성 (shift 사용)\")\n",
    "        df['합계_1일후'] = df['합계'].shift(-1)\n",
    "        df['합계_2일후'] = df['합계'].shift(-2)\n",
    "        df['등급_1일후'] = df['등급'].shift(-1).astype('Int64')\n",
    "        df['등급_2일후'] = df['등급'].shift(-2).astype('Int64')\n",
    "        print(f\"  1일후 타겟 유효값: {df['합계_1일후'].notna().sum()}개\")\n",
    "        print(f\"  2일후 타겟 유효값: {df['합계_2일후'].notna().sum()}개\")\n",
    "\n",
    "    df.attrs['cutoffs'] = {\"q15\": q15, \"q70\": q70, \"q90\": q90}\n",
    "    \n",
    "    # dropna 전후 비교\n",
    "    before_dropna = len(df)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    after_dropna = len(df)\n",
    "    print(f\"dropna 처리: {before_dropna}행 → {after_dropna}행 ({before_dropna - after_dropna}행 제거)\")\n",
    "    \n",
    "    # 2025-06-01 필터링\n",
    "    before_filter = len(df)\n",
    "    df = df[df[\"날짜\"] < \"2025-06-01\"]\n",
    "    after_filter = len(df)\n",
    "    print(f\"날짜 필터링: {before_filter}행 → {after_filter}행 ({before_filter - after_filter}행 제거)\")\n",
    "    \n",
    "    # 최종 결과\n",
    "    if '등급_1일후' in df.columns:\n",
    "        final_grade_dist = df['등급_1일후'].value_counts().sort_index()\n",
    "        print(f\"최종 등급 분포: {dict(final_grade_dist)}\")\n",
    "    \n",
    "    print(f\"최종 출력: {len(df)}행, {len(df.columns)}컬럼\")\n",
    "    print(f\"최종 날짜 범위: {df['날짜'].min()} ~ {df['날짜'].max()}\")\n",
    "    print(f\"=== make_features 완료 ===\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def make_features_for_prediction(historical_df, future_df):\n",
    "    \"\"\"새로운 데이터에 대한 파생변수 생성 (과거 데이터 활용)\"\"\"\n",
    "    combined_df = pd.concat([historical_df, future_df], ignore_index=True)\n",
    "    combined_df['날짜'] = pd.to_datetime(combined_df['날짜'])\n",
    "    combined_df = combined_df.sort_values('날짜').reset_index(drop=True)\n",
    "    \n",
    "    combined_df['월'] = combined_df['날짜'].dt.month\n",
    "    combined_df['요일'] = combined_df['날짜'].dt.weekday\n",
    "    \n",
    "    season_map = {'봄': 0, '여름': 1, '가을': 2, '겨울': 3}\n",
    "    discomfort_map = {'쾌적': 0, '약간 불쾌': 1, '불쾌': 2, '매우 불쾌': 3, '극심한 불쾌': 4}\n",
    "    combined_df['계절'] = combined_df['계절'].map(season_map).astype('Int64')\n",
    "    combined_df['불쾌지수등급'] = combined_df['불쾌지수등급'].map(discomfort_map).astype('Int64')\n",
    "    \n",
    "    # 시차 변수들\n",
    "    combined_df['강수량_1일전'] = combined_df['일_일강수량(mm)'].shift(1)\n",
    "    combined_df['강수량_2일전'] = combined_df['일_일강수량(mm)'].shift(2)\n",
    "    combined_df['강수량_1일_누적'] = combined_df['일_일강수량(mm)'].rolling(1, min_periods=1).sum()\n",
    "    combined_df['강수량_2일_누적'] = combined_df['일_일강수량(mm)'].rolling(2, min_periods=1).sum()\n",
    "    combined_df['강수량_3일_누적'] = combined_df['일_일강수량(mm)'].rolling(3, min_periods=1).sum()\n",
    "    combined_df['강수량_5일_누적'] = combined_df['일_일강수량(mm)'].rolling(5, min_periods=1).sum()\n",
    "    combined_df['강수량_7일_누적'] = combined_df['일_일강수량(mm)'].rolling(7, min_periods=1).sum()\n",
    "    \n",
    "    combined_df['일교차'] = combined_df['일_최고기온(°C)'] - combined_df['일_최저기온(°C)']\n",
    "    combined_df['폭우_여부'] = (combined_df['일_일강수량(mm)'] >= 80).astype(int)\n",
    "    \n",
    "    # 체감온도 계산\n",
    "    T = pd.to_numeric(combined_df.get('일_평균기온(°C)', np.nan), errors='coerce')\n",
    "    V_ms = pd.to_numeric(combined_df.get('일_평균풍속(m/s)', np.nan), errors='coerce')\n",
    "    RH = pd.to_numeric(combined_df.get('평균습도(%)', np.nan), errors='coerce')\n",
    "    \n",
    "    e = (RH/100.0) * 6.105 * np.exp(17.27*T/(237.7 + T))\n",
    "    combined_df['체감온도(°C)'] = T + 0.33*e - 0.70*V_ms - 4.00\n",
    "    \n",
    "    # 새 데이터 부분만 반환\n",
    "    historical_len = len(historical_df)\n",
    "    return combined_df.iloc[historical_len:].reset_index(drop=True)\n",
    "\n",
    "def make_simple_features(data):\n",
    "    \"\"\"간단한 피처 생성 (시차 변수 제외)\"\"\"\n",
    "    df = data.copy()\n",
    "    df['날짜'] = pd.to_datetime(df['날짜'])\n",
    "    df = df.sort_values('날짜').reset_index(drop=True)\n",
    "    \n",
    "    # 기본 피처들\n",
    "    df['월'] = df['날짜'].dt.month\n",
    "    df['요일'] = df['날짜'].dt.weekday\n",
    "    \n",
    "    # 계절/불쾌지수 매핑\n",
    "    season_map = {'봄': 0, '여름': 1, '가을': 2, '겨울': 3}\n",
    "    discomfort_map = {'쾌적': 0, '약간 불쾌': 1, '불쾌': 2, '매우 불쾌': 3, '극심한 불쾌': 4}\n",
    "    \n",
    "    if '계절' in df.columns:\n",
    "        df['계절'] = df['계절'].map(season_map).astype('Int64')\n",
    "    if '불쾌지수등급' in df.columns:\n",
    "        df['불쾌지수등급'] = df['불쾌지수등급'].map(discomfort_map).astype('Int64')\n",
    "    \n",
    "    # 기본 계산 피처들\n",
    "    if '일_최고기온(°C)' in df.columns and '일_최저기온(°C)' in df.columns:\n",
    "        df['일교차'] = df['일_최고기온(°C)'] - df['일_최저기온(°C)']\n",
    "    \n",
    "    if '일_일강수량(mm)' in df.columns:\n",
    "        df['폭우_여부'] = (df['일_일강수량(mm)'] >= 80).astype(int)\n",
    "    \n",
    "    # 체감온도 계산\n",
    "    T = pd.to_numeric(df.get('일_평균기온(°C)', np.nan), errors='coerce')\n",
    "    V_ms = pd.to_numeric(df.get('일_평균풍속(m/s)', np.nan), errors='coerce')\n",
    "    RH = pd.to_numeric(df.get('평균습도(%)', np.nan), errors='coerce')\n",
    "    \n",
    "    e = (RH/100.0) * 6.105 * np.exp(17.27*T/(237.7 + T))\n",
    "    df['체감온도(°C)'] = T + 0.33*e - 0.70*V_ms - 4.00\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ================================================================================================\n",
    "# 3. 평가 함수들\n",
    "# ================================================================================================\n",
    "def evaluate_regression_model(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"회귀 모델 평가\"\"\"\n",
    "    try:\n",
    "        pipe = make_pipeline_unified(model, model_name, \"regression\")\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = pipe.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-8))) * 100\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'regression',\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'mape': mape,\n",
    "            'success': True\n",
    "        }, pipe, y_pred\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'regression',\n",
    "            'mae': np.nan,\n",
    "            'rmse': np.nan,\n",
    "            'r2': np.nan,\n",
    "            'mape': np.nan,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }, None, None\n",
    "\n",
    "def evaluate_classification_model(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"분류 모델 평가\"\"\"\n",
    "    try:\n",
    "        pipe = make_pipeline_unified(model, model_name, \"classification\")\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = pipe.predict(X_test)\n",
    "        \n",
    "        if isinstance(y_pred, np.ndarray) and y_pred.ndim > 1:\n",
    "            y_pred = y_pred.ravel()\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1_macro = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "        f1_weighted = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "        \n",
    "        extreme_classes = [0, 3]\n",
    "        y_true_extreme = pd.Series(y_test).isin(extreme_classes).astype(int)\n",
    "        y_pred_extreme = pd.Series(y_pred).isin(extreme_classes).astype(int)\n",
    "        extreme_f1 = f1_score(y_true_extreme, y_pred_extreme, zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'classification',\n",
    "            'accuracy': acc,\n",
    "            'macro_f1': f1_macro,\n",
    "            'weighted_f1': f1_weighted,\n",
    "            'extreme_f1': extreme_f1,\n",
    "            'success': True\n",
    "        }, pipe, y_pred\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'type': 'classification',\n",
    "            'accuracy': np.nan,\n",
    "            'macro_f1': np.nan,\n",
    "            'weighted_f1': np.nan,\n",
    "            'extreme_f1': np.nan,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }, None, None\n",
    "\n",
    "def comprehensive_evaluation_comparison(center_name, df):\n",
    "    \"\"\"Stratified vs 시계열 분할 비교 평가\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"센터: {center_name} - Stratified vs 시계열 분할 비교\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"데이터 크기: {len(df)}행, {len(df.columns)}컬럼\")\n",
    "    \n",
    "    if '등급_1일후' in df.columns:\n",
    "        grade_dist = df['등급_1일후'].value_counts().sort_index()\n",
    "        print(f\"등급 분포: {dict(grade_dist)}\")\n",
    "        \n",
    "        min_class = grade_dist.min()\n",
    "        max_class = grade_dist.max()\n",
    "        imbalance_ratio = max_class / min_class\n",
    "        print(f\"클래스 불균형 비율: {imbalance_ratio:.1f}:1 (최대:{max_class}, 최소:{min_class})\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for split_method in ['temporal', 'stratified']:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"분할 방법: {split_method.upper()}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # 회귀 모델 평가\n",
    "        reg_method_name = \"random_shuffle\" if split_method == \"stratified\" else split_method\n",
    "        print(f\"\\n--- 회귀 모델 평가 ({reg_method_name}) ---\")\n",
    "        \n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test, feature_names, dates_train, dates_test = prepare_data_stratified(\n",
    "                df, target_col=\"합계_1일후\", model_type=\"regression\", test_size=0.2, split_method=split_method\n",
    "            )\n",
    "            \n",
    "            print(f\"회귀용 데이터: 학습 {len(X_train)}행, 테스트 {len(X_test)}행\")\n",
    "            \n",
    "            regression_models = build_regression_models()\n",
    "            \n",
    "            for model_name, model in tqdm(regression_models.items(), desc=f\"회귀({reg_method_name})\", leave=False):\n",
    "                result, pipe, y_pred = evaluate_regression_model(model, model_name, X_train, X_test, y_train, y_test)\n",
    "                result['center'] = center_name\n",
    "                result['split_method'] = split_method\n",
    "                results.append(result)\n",
    "                \n",
    "                if result['success']:\n",
    "                    print(f\"  {model_name:18s}: R²={result['r2']:.3f}, MAE={result['mae']:.0f}, MAPE={result['mape']:.1f}%\")\n",
    "                else:\n",
    "                    print(f\"  {model_name:18s}: 실패 - {result.get('error', '')[:50]}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"회귀 모델 평가 실패 ({reg_method_name}): {e}\")\n",
    "        \n",
    "        # 분류 모델 평가\n",
    "        print(f\"\\n--- 분류 모델 평가 ({split_method}) ---\")\n",
    "        \n",
    "        try:\n",
    "            X_train_clf, X_test_clf, y_train_clf, y_test_clf, feature_names_clf, _, _ = prepare_data_stratified(\n",
    "                df, target_col=\"등급_1일후\", model_type=\"classification\", test_size=0.2, split_method=split_method\n",
    "            )\n",
    "            \n",
    "            print(f\"분류용 데이터: 학습 {len(X_train_clf)}행, 테스트 {len(X_test_clf)}행\")\n",
    "            \n",
    "            test_dist = pd.Series(y_test_clf).value_counts().sort_index()\n",
    "            train_dist = pd.Series(y_train_clf).value_counts().sort_index()\n",
    "            print(f\"학습 세트 등급 분포: {dict(train_dist)}\")\n",
    "            print(f\"테스트 세트 등급 분포: {dict(test_dist)}\")\n",
    "            \n",
    "            classification_models = build_classification_models()\n",
    "            \n",
    "            for model_name, model in tqdm(classification_models.items(), desc=f\"분류({split_method})\", leave=False):\n",
    "                result, pipe, y_pred = evaluate_classification_model(model, model_name, X_train_clf, X_test_clf, y_train_clf, y_test_clf)\n",
    "                result['center'] = center_name\n",
    "                result['split_method'] = split_method\n",
    "                results.append(result)\n",
    "                \n",
    "                if result['success']:\n",
    "                    print(f\"  {model_name:18s}: ACC={result['accuracy']:.3f}, F1={result['macro_f1']:.3f}, 극값F1={result['extreme_f1']:.3f}\")\n",
    "                else:\n",
    "                    print(f\"  {model_name:18s}: 실패 - {result.get('error', '')[:50]}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"분류 모델 평가 실패 ({split_method}): {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ================================================================================================\n",
    "# 4. Feature Importance & SHAP 분석 함수들 (선택사항)\n",
    "# ================================================================================================\n",
    "def extract_feature_importance(model, model_name, feature_names):\n",
    "    \"\"\"모델별 Feature Importance 추출\"\"\"\n",
    "    try:\n",
    "        mdl = model.named_steps['model']\n",
    "        if hasattr(mdl, 'feature_importances_'):\n",
    "            importance = mdl.feature_importances_\n",
    "        elif hasattr(mdl, 'coef_'):\n",
    "            coef = mdl.coef_\n",
    "            if isinstance(coef, np.ndarray) and coef.ndim == 2:\n",
    "                importance = np.mean(np.abs(coef), axis=0)\n",
    "            else:\n",
    "                importance = np.abs(coef)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        if len(importance) != len(feature_names):\n",
    "            print(f\"[경고] importance 길이({len(importance)}) != feature_names({len(feature_names)})\")\n",
    "            m = min(len(importance), len(feature_names))\n",
    "            importance = np.asarray(importance)[:m]\n",
    "            feature_names = list(feature_names)[:m]\n",
    "\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        return importance_df\n",
    "    except Exception as e:\n",
    "        print(f\"Feature importance 추출 실패 ({model_name}): {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_feature_importance(importance_df, model_name, top_n=15):\n",
    "    \"\"\"Feature Importance 시각화\"\"\"\n",
    "    if importance_df is None or len(importance_df) == 0:\n",
    "        return None\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    top_features = importance_df.head(top_n)\n",
    "    \n",
    "    ax.barh(range(len(top_features)), top_features['importance'], color='skyblue')\n",
    "    ax.set_yticks(range(len(top_features)))\n",
    "    ax.set_yticklabels(top_features['feature'])\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title(f'{model_name} - Top {top_n} Feature Importance')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    for i, v in enumerate(top_features['importance']):\n",
    "        ax.text(v + 0.001, i, f'{v:.3f}', va='center')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analyze_model_with_shap(model, X_test, feature_names, model_name, max_samples=100):\n",
    "    \"\"\"SHAP 분석 - 다중분류 대응 강화\"\"\"\n",
    "    if not HAS_SHAP:\n",
    "        print(\"SHAP 라이브러리가 설치되지 않았습니다.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if len(X_test) > max_samples:\n",
    "            sample_idx = np.random.choice(len(X_test), max_samples, replace=False)\n",
    "            X_sample = X_test.iloc[sample_idx]\n",
    "        else:\n",
    "            X_sample = X_test\n",
    "        \n",
    "        X_processed = model.named_steps['pre'].transform(X_sample)\n",
    "        \n",
    "        if 'RandomForest' in model_name or 'GradientBoosting' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        elif 'XGBoost' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        elif 'LightGBM' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        elif 'CatBoost' in model_name:\n",
    "            explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        else:\n",
    "            explainer = shap.LinearExplainer(model.named_steps['model'], X_processed)\n",
    "        \n",
    "        shap_values = explainer.shap_values(X_processed)\n",
    "        \n",
    "        # 원본 SHAP 값 디버깅\n",
    "        print(f\"원본 SHAP 값 타입: {type(shap_values)}\")\n",
    "        if isinstance(shap_values, list):\n",
    "            print(f\"리스트 길이: {len(shap_values)}\")\n",
    "            if len(shap_values) > 0:\n",
    "                print(f\"첫 번째 요소 shape: {shap_values[0].shape if hasattr(shap_values[0], 'shape') else 'no shape'}\")\n",
    "        elif hasattr(shap_values, 'shape'):\n",
    "            print(f\"배열 shape: {shap_values.shape}\")\n",
    "        \n",
    "        return shap_values, X_processed, explainer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"SHAP 분석 실패 ({model_name}): {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_shap_summary(shap_values, X_processed, feature_names, model_name):\n",
    "    \"\"\"SHAP Summary Plot - 완전 수정 버전\"\"\"\n",
    "    if shap_values is None or not HAS_SHAP:\n",
    "        return []\n",
    "\n",
    "    figs = []\n",
    "    try:\n",
    "        # 다중분류 SHAP 값 처리 - 더 세밀한 처리\n",
    "        shap_values_use = None\n",
    "        \n",
    "        if isinstance(shap_values, list):\n",
    "            print(f\"SHAP 값이 리스트, 길이: {len(shap_values)}\")\n",
    "            if len(shap_values) > 0:\n",
    "                # 다중분류의 경우 첫 번째 클래스만 사용\n",
    "                shap_values_use = shap_values[0]\n",
    "                print(f\"첫 번째 클래스 shape: {shap_values_use.shape}\")\n",
    "        elif isinstance(shap_values, np.ndarray):\n",
    "            if shap_values.ndim == 3:\n",
    "                print(f\"3차원 배열: {shap_values.shape}\")\n",
    "                # (samples, features, classes) -> (samples, features)\n",
    "                shap_values_use = shap_values[:, :, 0]\n",
    "                print(f\"첫 번째 클래스 추출 후: {shap_values_use.shape}\")\n",
    "            elif shap_values.ndim == 2:\n",
    "                print(f\"2차원 배열: {shap_values.shape}\")\n",
    "                shap_values_use = shap_values\n",
    "            else:\n",
    "                print(f\"예상치 못한 차원: {shap_values.ndim}\")\n",
    "                shap_values_use = shap_values\n",
    "        else:\n",
    "            print(f\"알 수 없는 타입: {type(shap_values)}\")\n",
    "            shap_values_use = shap_values\n",
    "        \n",
    "        # 최종 확인\n",
    "        if shap_values_use is None:\n",
    "            print(\"SHAP 값 처리 실패\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"최종 사용할 SHAP 값 shape: {shap_values_use.shape}\")\n",
    "        print(f\"X_processed shape: {X_processed.shape}\")\n",
    "        print(f\"feature_names 길이: {len(feature_names)}\")\n",
    "        \n",
    "        # 차원 일치성 확인\n",
    "        if hasattr(shap_values_use, 'shape') and len(shap_values_use.shape) >= 2:\n",
    "            if shap_values_use.shape[1] != len(feature_names):\n",
    "                print(f\"경고: SHAP 피처 수({shap_values_use.shape[1]}) != feature_names 수({len(feature_names)})\")\n",
    "                # 최소 길이로 맞춤\n",
    "                min_features = min(shap_values_use.shape[1], len(feature_names))\n",
    "                shap_values_use = shap_values_use[:, :min_features]\n",
    "                feature_names = feature_names[:min_features]\n",
    "                print(f\"조정 후 - SHAP: {shap_values_use.shape}, features: {len(feature_names)}\")\n",
    "        \n",
    "        # Bar plot 시도\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            shap.summary_plot(shap_values_use, X_processed[:len(shap_values_use)],\n",
    "                              feature_names=feature_names,\n",
    "                              plot_type=\"bar\", show=False)\n",
    "            plt.title(f'{model_name} - SHAP Feature Importance')\n",
    "            figs.append(plt.gcf())\n",
    "            print(\"    SHAP bar plot 성공\")\n",
    "        except Exception as e:\n",
    "            print(f\"    SHAP bar plot 실패: {e}\")\n",
    "        \n",
    "        # Beeswarm plot 시도 (더 까다로움)\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            shap.summary_plot(shap_values_use, X_processed[:len(shap_values_use)],\n",
    "                              feature_names=feature_names,\n",
    "                              show=False)\n",
    "            plt.title(f'{model_name} - SHAP Summary Plot')\n",
    "            figs.append(plt.gcf())\n",
    "            print(\"    SHAP beeswarm plot 성공\")\n",
    "        except Exception as e:\n",
    "            print(f\"    SHAP beeswarm plot 실패: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"SHAP 시각화 전체 실패 ({model_name}): {e}\")\n",
    "\n",
    "    plt.close('all')\n",
    "    return figs\n",
    "\n",
    "# ================================================================================================\n",
    "# 5. 유틸리티 함수들\n",
    "# ================================================================================================\n",
    "def load_original_data():\n",
    "    \"\"\"원본 데이터 로드\"\"\"\n",
    "    nanji_raw = pd.read_csv('../data/processed/center_season/nanji/난지_merged.csv', encoding='utf-8-sig')\n",
    "    jungnang_raw = pd.read_csv('../data/processed/center_season/jungnang/중랑_merged.csv', encoding='utf-8-sig')\n",
    "    seonam_raw = pd.read_csv('../data/processed/center_season/seonam/서남_merged.csv', encoding='utf-8-sig')\n",
    "    tancheon_raw = pd.read_csv('../data/processed/center_season/tancheon/탄천_merged.csv', encoding='utf-8-sig')\n",
    "    \n",
    "    return {\n",
    "        \"nanji\": nanji_raw,\n",
    "        \"jungnang\": jungnang_raw,\n",
    "        \"seonam\": seonam_raw,\n",
    "        \"tancheon\": tancheon_raw\n",
    "    }\n",
    "\n",
    "def prepare_prediction_features(future_data, expected_features):\n",
    "    \"\"\"예측용 피처 준비\"\"\"\n",
    "    not_use_col = [\n",
    "        '날짜', '1처리장','2처리장','정화조','중계펌프장','합계','시설현대화',\n",
    "        '3처리장','4처리장','합계', '합계_1일후','합계_2일후',\n",
    "        '등급','등급_1일후','등급_2일후'\n",
    "    ]\n",
    "    \n",
    "    available_cols = [col for col in future_data.columns if col not in not_use_col]\n",
    "    X_future = future_data[available_cols].copy()\n",
    "    \n",
    "    for c in X_future.columns:\n",
    "        X_future[c] = pd.to_numeric(X_future[c], errors=\"coerce\")\n",
    "    \n",
    "    missing_features = set(expected_features) - set(X_future.columns)\n",
    "    if missing_features:\n",
    "        for feature in missing_features:\n",
    "            X_future[feature] = 0\n",
    "    \n",
    "    X_future = X_future[expected_features].copy()\n",
    "    return X_future\n",
    "\n",
    "# ================================================================================================\n",
    "# 6. 모델 성능 비교 시각화 함수들\n",
    "# ================================================================================================\n",
    "def plot_data_characteristics_comparison(centers_data, results_dir, timestamp):\n",
    "    \"\"\"센터별 데이터 특성 비교\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('센터별 데이터 특성 비교', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    center_stats = []\n",
    "    \n",
    "    for center_name, df in centers_data.items():\n",
    "        df['날짜'] = pd.to_datetime(df['날짜'])\n",
    "        df = df.sort_values('날짜').reset_index(drop=True)\n",
    "        \n",
    "        stats = {\n",
    "            'center': center_name,\n",
    "            'total_records': len(df),\n",
    "            'date_range': (df['날짜'].max() - df['날짜'].min()).days,\n",
    "            'avg_flow': df['합계'].mean() if '합계' in df.columns else 0,\n",
    "            'std_flow': df['합계'].std() if '합계' in df.columns else 0,\n",
    "            'missing_ratio': df.isnull().sum().sum() / (len(df) * len(df.columns))\n",
    "        }\n",
    "        center_stats.append(stats)\n",
    "    \n",
    "    center_stats_df = pd.DataFrame(center_stats)\n",
    "    \n",
    "    # 1. 데이터 양 비교\n",
    "    axes[0,0].bar(center_stats_df['center'], center_stats_df['total_records'], color='skyblue')\n",
    "    axes[0,0].set_title('센터별 데이터 양')\n",
    "    axes[0,0].set_ylabel('레코드 수')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    for i, v in enumerate(center_stats_df['total_records']):\n",
    "        axes[0,0].text(i, v + 10, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    # 2. 평균 유량 비교\n",
    "    axes[0,1].bar(center_stats_df['center'], center_stats_df['avg_flow'], color='lightgreen')\n",
    "    axes[0,1].set_title('센터별 평균 유량')\n",
    "    axes[0,1].set_ylabel('평균 유량')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    for i, v in enumerate(center_stats_df['avg_flow']):\n",
    "        axes[0,1].text(i, v + max(center_stats_df['avg_flow'])*0.01, f'{v:.0f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. 유량 변동성 비교\n",
    "    axes[1,0].bar(center_stats_df['center'], center_stats_df['std_flow'], color='coral')\n",
    "    axes[1,0].set_title('센터별 유량 변동성 (표준편차)')\n",
    "    axes[1,0].set_ylabel('표준편차')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    for i, v in enumerate(center_stats_df['std_flow']):\n",
    "        axes[1,0].text(i, v + max(center_stats_df['std_flow'])*0.01, f'{v:.0f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. 데이터 완성도\n",
    "    missing_pct = center_stats_df['missing_ratio'] * 100\n",
    "    axes[1,1].bar(center_stats_df['center'], 100 - missing_pct, color='gold')\n",
    "    axes[1,1].set_title('센터별 데이터 완성도')\n",
    "    axes[1,1].set_ylabel('완성도 (%)')\n",
    "    axes[1,1].set_ylim(0, 100)\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    for i, v in enumerate(100 - missing_pct):\n",
    "        axes[1,1].text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"data_characteristics_comparison_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  데이터 특성 비교 저장: {os.path.basename(save_path)}\")\n",
    "\n",
    "def plot_model_complexity_vs_performance(training_summary, results_dir, timestamp):\n",
    "    \"\"\"모델 복잡도 vs 성능 관계\"\"\"\n",
    "    complexity_map = {\n",
    "        'LinearRegression': 1, 'LogisticRegression_Clf': 1,\n",
    "        'RandomForest_Reg': 3, 'RandomForest_Clf': 3,\n",
    "        'GradientBoosting_Reg': 4, 'GradientBoosting_Clf': 4,\n",
    "        'XGBoost_Reg': 4, 'XGBoost_Clf': 4,\n",
    "        'LightGBM_Reg': 4, 'LightGBM_Clf': 4,\n",
    "        'CatBoost_Reg': 5, 'CatBoost_Clf': 5\n",
    "    }\n",
    "    \n",
    "    if isinstance(training_summary, list):\n",
    "        summary_df = pd.DataFrame(training_summary)\n",
    "    else:\n",
    "        summary_df = training_summary.copy()\n",
    "    \n",
    "    summary_df['complexity'] = summary_df['model_name'].map(complexity_map)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle('모델 복잡도 vs 성능 관계', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 회귀 모델\n",
    "    reg_data = summary_df[summary_df['task_type'] == 'regression'].copy()\n",
    "    if len(reg_data) > 0:\n",
    "        reg_data['r2'] = reg_data['performance'].apply(lambda x: x.get('r2', 0))\n",
    "        \n",
    "        scatter = axes[0].scatter(reg_data['complexity'], reg_data['r2'], \n",
    "                                 c=reg_data.index, cmap='viridis', s=100, alpha=0.7)\n",
    "        \n",
    "        for i, row in reg_data.iterrows():\n",
    "            axes[0].annotate(f\"{row['center'][:3]}\", \n",
    "                           (row['complexity'], row['r2']),\n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        axes[0].set_xlabel('모델 복잡도 (1=단순, 5=복잡)')\n",
    "        axes[0].set_ylabel('R² Score')\n",
    "        axes[0].set_title('회귀: 복잡도 vs 성능')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].set_xticks(range(1, 6))\n",
    "    \n",
    "    # 분류 모델\n",
    "    clf_data = summary_df[summary_df['task_type'] == 'classification'].copy()\n",
    "    if len(clf_data) > 0:\n",
    "        clf_data['macro_f1'] = clf_data['performance'].apply(lambda x: x.get('macro_f1', 0))\n",
    "        \n",
    "        scatter = axes[1].scatter(clf_data['complexity'], clf_data['macro_f1'], \n",
    "                                 c=clf_data.index, cmap='plasma', s=100, alpha=0.7)\n",
    "        \n",
    "        for i, row in clf_data.iterrows():\n",
    "            axes[1].annotate(f\"{row['center'][:3]}\", \n",
    "                           (row['complexity'], row['macro_f1']),\n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        axes[1].set_xlabel('모델 복잡도 (1=단순, 5=복잡)')\n",
    "        axes[1].set_ylabel('Macro F1 Score')\n",
    "        axes[1].set_title('분류: 복잡도 vs 성능')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].set_xticks(range(1, 6))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"model_complexity_vs_performance_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  복잡도 vs 성능 저장: {os.path.basename(save_path)}\")\n",
    "\n",
    "def plot_prediction_difficulty_analysis(training_summary, results_dir, timestamp):\n",
    "    \"\"\"센터별 예측 난이도 분석\"\"\"\n",
    "    if isinstance(training_summary, list):\n",
    "        summary_df = pd.DataFrame(training_summary)\n",
    "    else:\n",
    "        summary_df = training_summary.copy()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('센터별 예측 난이도 분석', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    centers = summary_df['center'].unique()\n",
    "    \n",
    "    # 1. 센터별 최고 R² 성능\n",
    "    reg_best_scores = []\n",
    "    for center in centers:\n",
    "        center_reg = summary_df[(summary_df['center'] == center) & \n",
    "                               (summary_df['task_type'] == 'regression')]\n",
    "        if len(center_reg) > 0:\n",
    "            center_reg = center_reg.copy()\n",
    "            center_reg['r2'] = center_reg['performance'].apply(lambda x: x.get('r2', 0))\n",
    "            reg_best_scores.append(center_reg['r2'].max())\n",
    "        else:\n",
    "            reg_best_scores.append(0)\n",
    "    \n",
    "    bars1 = axes[0,0].bar(centers, reg_best_scores, color='skyblue')\n",
    "    axes[0,0].set_title('센터별 최고 R² 성능 (높을수록 예측 용이)')\n",
    "    axes[0,0].set_ylabel('최고 R² Score')\n",
    "    axes[0,0].set_ylim(0, 1)\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, score in zip(bars1, reg_best_scores):\n",
    "        axes[0,0].text(bar.get_x() + bar.get_width()/2, score + 0.01, \n",
    "                      f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. 센터별 성능 분산 (일관성)\n",
    "    reg_score_vars = []\n",
    "    for center in centers:\n",
    "        center_reg = summary_df[(summary_df['center'] == center) & \n",
    "                               (summary_df['task_type'] == 'regression')]\n",
    "        if len(center_reg) > 0:\n",
    "            center_reg = center_reg.copy()\n",
    "            center_reg['r2'] = center_reg['performance'].apply(lambda x: x.get('r2', 0))\n",
    "            reg_score_vars.append(center_reg['r2'].std())\n",
    "        else:\n",
    "            reg_score_vars.append(0)\n",
    "    \n",
    "    bars2 = axes[0,1].bar(centers, reg_score_vars, color='lightcoral')\n",
    "    axes[0,1].set_title('센터별 성능 분산 (낮을수록 일관성 높음)')\n",
    "    axes[0,1].set_ylabel('R² Score 표준편차')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, var in zip(bars2, reg_score_vars):\n",
    "        axes[0,1].text(bar.get_x() + bar.get_width()/2, var + max(reg_score_vars)*0.01, \n",
    "                      f'{var:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. 센터별 최고 F1 성능\n",
    "    clf_best_scores = []\n",
    "    for center in centers:\n",
    "        center_clf = summary_df[(summary_df['center'] == center) & \n",
    "                               (summary_df['task_type'] == 'classification')]\n",
    "        if len(center_clf) > 0:\n",
    "            center_clf = center_clf.copy()\n",
    "            center_clf['macro_f1'] = center_clf['performance'].apply(lambda x: x.get('macro_f1', 0))\n",
    "            clf_best_scores.append(center_clf['macro_f1'].max())\n",
    "        else:\n",
    "            clf_best_scores.append(0)\n",
    "    \n",
    "    bars3 = axes[1,0].bar(centers, clf_best_scores, color='lightgreen')\n",
    "    axes[1,0].set_title('센터별 최고 F1 성능 (높을수록 분류 용이)')\n",
    "    axes[1,0].set_ylabel('최고 Macro F1 Score')\n",
    "    axes[1,0].set_ylim(0, 1)\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, score in zip(bars3, clf_best_scores):\n",
    "        axes[1,0].text(bar.get_x() + bar.get_width()/2, score + 0.01, \n",
    "                      f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. 종합 예측 난이도 점수\n",
    "    difficulty_scores = [(r2 + f1) / 2 for r2, f1 in zip(reg_best_scores, clf_best_scores)]\n",
    "    colors = plt.cm.RdYlGn([score for score in difficulty_scores])\n",
    "    \n",
    "    bars4 = axes[1,1].bar(centers, difficulty_scores, color=colors)\n",
    "    axes[1,1].set_title('종합 예측 용이도 점수')\n",
    "    axes[1,1].set_ylabel('종합 점수 (R² + F1) / 2')\n",
    "    axes[1,1].set_ylim(0, 1)\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, score in zip(bars4, difficulty_scores):\n",
    "        axes[1,1].text(bar.get_x() + bar.get_width()/2, score + 0.01, \n",
    "                      f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(results_dir, f\"prediction_difficulty_analysis_{timestamp}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  예측 난이도 분석 저장: {os.path.basename(save_path)}\")\n",
    "\n",
    "    \"\"\"Split Method 상세 비교 분석 - 수정된 버전\"\"\"\n",
    "    if isinstance(all_training_results, list):\n",
    "        summary_df = pd.DataFrame(all_training_results)\n",
    "    else:\n",
    "        summary_df = all_training_results.copy()\n",
    "    \n",
    "    successful_results = summary_df[summary_df['success'] == True].copy()\n",
    "    \n",
    "    if len(successful_results) == 0 or 'split_method' not in successful_results.columns:\n",
    "        print(\"Split method 정보가 없거나 성공한 결과가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('Stratified vs Temporal Split 상세 비교', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 회귀 데이터 처리\n",
    "    reg_data = successful_results[successful_results['type'] == 'regression'].copy()\n",
    "    if len(reg_data) > 0:\n",
    "        reg_pivot = reg_data.pivot_table(index='center', columns='split_method', values='r2')\n",
    "        reg_pivot.plot(kind='bar', ax=axes[0,0])\n",
    "        axes[0,0].set_title('센터별 R² 성능: Split Method 비교')\n",
    "        axes[0,0].set_ylabel('R² Score')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        axes[0,0].legend(title='Split Method')\n",
    "        \n",
    "        split_methods = reg_data['split_method'].unique()\n",
    "        r2_by_split = [reg_data[reg_data['split_method'] == method]['r2'].values \n",
    "                       for method in split_methods]\n",
    "        \n",
    "        axes[0,1].boxplot(r2_by_split, labels=split_methods)\n",
    "        axes[0,1].set_title('Split Method별 R² 분포')\n",
    "        axes[0,1].set_ylabel('R² Score')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        if len(split_methods) == 2:\n",
    "            method1, method2 = split_methods\n",
    "            perf_diff = []\n",
    "            centers = reg_data['center'].unique()\n",
    "            \n",
    "            for center in centers:\n",
    "                center_data = reg_data[reg_data['center'] == center]\n",
    "                perf1 = center_data[center_data['split_method'] == method1]['r2']\n",
    "                perf2 = center_data[center_data['split\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6d8fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa31b319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_models, summary = train_and_save_best_models_with_plots_and_shap()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
